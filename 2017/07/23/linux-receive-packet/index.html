<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Linux 网络协议栈收消息过程 | A Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-80224793-1','auto');ga('send','pageview');</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Linux 网络协议栈收消息过程</h1><a id="logo" href="/.">A Blog</a><p class="description">by ylgrgyq</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Linux 网络协议栈收消息过程</h1><div class="post-meta">Jul 23, 2017<script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="post-content"><p>想看能不能完整梳理一下收消息过程。从 NIC 收数据开始，直到数据被交付用户进程。</p>
<h2 id="Ring-buffer"><a href="#Ring-buffer" class="headerlink" title="Ring buffer"></a>Ring buffer</h2><p>Ring Buffer 相关的收消息过程大致如下：</p>
<p>图片来自参考1，对 raise softirq 的函数名做了修改，改为了 napi_schedule</p>
<p>NIC (network interface card) 在系统启动过程中会向系统注册自己的各种信息，系统会分配 Ring Buffer 队列也会分配一块专门的区域给 NIC 用于存放传输上来的数据包。Ring Buffer 上的槽 (Packet Descriptor) 会指向 NIC 专有内存区域上分配出来的 struct sk_buff。Ring Buffer 上的槽分为 ready 和 used 两种状态，初始时槽是空的，指向一个空的 sk_buff，槽处在 ready 状态。当有数据时，<a href="https://en.wikipedia.org/wiki/DMA" target="_blank" rel="external">DMA </a> 负责从 NIC 取数据，并在 Ring Buffer 上按顺序找到下一个 ready 的槽，将数据存入该槽指向的 sk_buff 中，并标记槽为 used。因为是按顺序找 ready 的槽，所以 Ring Buffer 是个 FIFO 的队列。</p>
<p>当数据写完之后 NIC 会触发一个 IRQ 让 CPU 去处理收到的数据。因为每次触发 IRQ 后 CPU 都要花费时间去处理 Interrupt Handler，如果 NIC 每收到一个 Packet 都触发一个 IRQ 会导致 CPU 花费大量的时间在处理 Interrupt Handler，处理完后只能从 Ring Buffer 中拿出一个 Packet，虽然 Interrupt Handler 执行时间很短，但这么做也非常低效，并会给 CPU 带去很多负担。所以目前都是采用一个叫做 <a href="https://wiki.linuxfoundation.org/networking/napi" target="_blank" rel="external">New API(NAPI)</a> 的机制，去对 IRQ 做合并以减少 IRQ 次数。 </p>
<p>接下来介绍一下 NAPI 是怎么做到 IRQ 合并的。它主要是让 NIC 的 driver 能注册一个 <code>poll</code> 函数，之后 NAPI 的 subsystem 能通过 <code>poll</code> 函数去从 Ring Buffer 中批量拉取收到的数据。主要事件及其顺序如下：</p>
<ol>
<li>NIC driver 初始化时向 Kernel 注册 <code>poll</code> 函数，用于后续从 Ring Buffer 拉取收到的数据</li>
<li>driver 注册开启 NAPI，这个机制默认是关闭的，只有支持 NAPI 的 driver 才会去开启</li>
<li>收到数据后 NIC 通过 DMA 将数据存到内存</li>
<li>NIC 触发一个 IRQ，并触发 CPU 开始执行 driver 注册的 Interrupt Handler</li>
<li>driver 的 Interrupt Handler 通过 <a href="http://elixir.free-electrons.com/linux/v4.4/source/include/linux/netdevice.h#L421" target="_blank" rel="external">napi_schedule</a> 函数触发 softirq (<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3204" target="_blank" rel="external">NET_RX_SOFTIRQ</a>) 来唤醒 NAPI subsystem，NET_RX_SOFTIRQ 的 handler 是 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L4861" target="_blank" rel="external">net_rx_action 会在另一个线程中被执行，在其中会调用 driver 注册的 <code>poll</code> 函数获取收到的 Packet</a></li>
<li>driver 会禁用当前 NIC 的 IRQ，从而能在 <code>poll</code> 完所有数据之前不会再有新的 IRQ</li>
<li>当所有事情做完之后，NAPI subsystem 会被禁用，并且会重新启用 NIC 的 IRQ</li>
<li>回到第三步</li>
</ol>
<p>从上面的描述可以看出来还缺一些东西，Ring Buffer 上的数据被 <code>poll</code> 走之后是怎么交付上层网络栈继续处理的呢？以及被消耗掉的 sk_buff 是怎么被重新分配重新放入 Ring Buffer 的呢？</p>
<p>这两个工作都在 <code>poll</code> 中完成，上面说过 <code>poll</code> 是个 driver 实现的函数，所以每个 driver 实现可能都不相同。但 <code>poll</code> 的工作基本是一致的就是：</p>
<ol>
<li>从 Ring Buffer 中将收到的 sk_buff 读取出来</li>
<li>对 sk_buff 做一些基本检查，可能会涉及到将几个 sk_buff 合并因为可能同一个 Frame 被分散放在多个 sk_buff 中</li>
<li>将 sk_buff 交付上层网络栈处理</li>
<li>清理 sk_buff，清理 Ring Buffer 上的 Descriptor 将其指向新分配的 sk_buff 并将状态设置为 ready</li>
<li>更新一些统计数据，比如收到了多少 packet，一共多少字节等</li>
</ol>
<p>如果拿 intel igb 这个网卡的实现来看，其 <code>poll</code> 函数在这里：<a href="http://elixir.free-electrons.com/linux/v4.4/source/drivers/net/ethernet/intel/igb/igb_main.c#L6361" target="_blank" rel="external">linux/drivers/net/ethernet/intel/igb/igb_main.c - Elixir - Free Electrons</a><br>首先是看到有 tx.ring 和 rx.ring，说明收发消息都会走到这里。发消息先不管，先看收消息，收消息走的是 <a href="http://elixir.free-electrons.com/linux/v4.4/source/drivers/net/ethernet/intel/igb/igb_main.c#L6912" target="_blank" rel="external">igb_clean_rx_irq</a>。收完消息后执行 <code>napi_complete_done</code> 退出 polling 模式，并开启 NIC 的 IRQ。从而我们知道大部分工作是在 igb_clean_rx_irq 中完成的，其实现大致上还是比较清晰的，就是上面描述的几步。里面有个 while 循环通过 buget 控制，从而在 Packet 特别多的时候不要让 CPU 在这里无穷循环下去，要让别的事情也能够被执行。循环内做的事情如下：</p>
<ol>
<li>先批量清理已经读出来的 sk_buff 并分配新的 buffer 从而避免每次读一个 sk_buff 就清理一个，很低效</li>
<li>找到 Ring Buffer 上下一个需要被读取的 Descriptor ，并检查描述符状态是否正常</li>
<li>根据 Descriptor 找到 sk_buff 读出来</li>
<li>检查是否是 End of packet，是的话说明 sk_buff 内有 Frame 的全部内容，不是的话说明 Frame 数据比 sk_buff 大，需要再读一个 sk_buff，将两个 sk_buff 数据合并起来</li>
<li>通过 Frame 的 Header 检查 Frame 数据完整性，是否正确之类的</li>
<li>记录 sk_buff 的长度，读了多少数据</li>
<li>设置 Hash、checksum、timestamp、VLAN id 等信息，这些信息是硬件提供的。</li>
<li>通过 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L4354" target="_blank" rel="external">napi_gro_receive</a> 将 sk_buff 交付上层网络栈</li>
<li>更新一堆统计数据</li>
<li>回到 1，如果没数据或者 budget 不够就退出循环</li>
</ol>
<p>看到 budget 会影响到 CPU 执行 <code>poll</code> 的时间，budget 越大当数据包特别多的时候可以提高 CPU 利用率并减少数据包的延迟。但是 CPU 时间都花在这里会影响别的认为的执行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">budget 默认 300，可以调整</div><div class="line">sysctl -w net.core.netdev_budget=600</div></pre></td></tr></table></figure></p>
<p><code>napi_gro_receive</code>会涉及到 GRO，稍后再说，大致上就是会对多个数据包做聚合，<code>napi_gro_receive</code> 最终是将处理好的 sk_buff 通过调用 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L4026" target="_blank" rel="external">netif_receive_skb</a>，将数据包送至上层网络栈。执行完 GRO 之后，基本可以认为数据包正式离开 Ring Buffer，进入下一个阶段了。在记录下一阶段的处理之前，补充一下收消息阶段 Ring Buffer 相关的更多细节。</p>
<h3 id="Generic-Receive-Offloading-GRO"><a href="#Generic-Receive-Offloading-GRO" class="headerlink" title="Generic Receive Offloading(GRO)"></a>Generic Receive Offloading(GRO)</h3><p>GRO 是 <a href="https://en.wikipedia.org/wiki/Large_receive_offload" target="_blank" rel="external">Large receive offload</a> 的一个实现。网络上大部分 MTU 都是 1500 字节，开启 Jumbo Frame 后能到 9000 字节，如果发送的数据超过 MTU 就需要切割成多个数据包。LRO 就是在收到多个数据包的时候将同一个 Flow 的多个数据包按照一定的规则合并起来交给上层处理，这样就能减少上层需要处理的数据包数量。</p>
<p>GRO 就是 LRO 在软件上的实现，从而能让所有 NIC 都支持这个功能。</p>
<p><code>napi_gro_receive</code> 就是在收到数据包的时候合并多个数据包用的，如果收到的数据包需要被合并，<code>napi_gro_receive</code> 会很快返回。当合并完成后会调用 <code>napi_skb_finish</code> ，将因为数据包合并而不再用到的数据结构释放。最终会调用到 <code>netif_receive_skb</code> 将数据包交到上层网络栈继续处理。<code>netif_receive_skb</code> 上面说过，就是数据包从 Ring Buffer 出来后到上层网络栈的入口。</p>
<p>可以通过 ethtool 查看和设置 GRO：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">查看 GRO</div><div class="line">ethtool -k eth0 | grep generic-receive-offload</div><div class="line">generic-receive-offload: on</div><div class="line">设置开启 GRO</div><div class="line">ethtool -K eth0 gro on</div></pre></td></tr></table></figure></p>
<h3 id="多-CPU-下的-Ring-Buffer-处理-Receive-Side-Scaling"><a href="#多-CPU-下的-Ring-Buffer-处理-Receive-Side-Scaling" class="headerlink" title="多 CPU 下的 Ring Buffer 处理 (Receive Side Scaling)"></a>多 CPU 下的 Ring Buffer 处理 (Receive Side Scaling)</h3><p>NIC 收到数据的时候产生的 IRQ 只可能被一个 CPU 处理，从而只有一个 CPU 会执行 napi_schedule 来触发 softirq，触发的这个 softirq 的 handler 也会在同一个 CPU 上执行。所以 driver 的 <code>poll</code> 函数也是在最开始处理 NIC 发出 IRQ 的那个 CPU 上执行。于是一个 Ring Buffer 上同一个时刻只有一个 CPU 在拉取数据。</p>
<p>从上面描述能看出来分配给 Ring Buffer 的空间是有限的，当收到的数据包速率大于单个 CPU 处理速度的时候 Ring Buffer 可能被占满，占满之后再来的新数据包会被自动丢弃。而现在机器都是有多个 CPU，同时只有一个 CPU 去处理 Ring Buffer 数据会很低效，这个时候就产生了叫做 Receive Side Scaling(RSS) 或者叫做 multiqueue 的机制来处理这个问题。WIKI 对 RSS 的介绍挺好的，简洁干练可以看看: <a href="https://en.wikipedia.org/wiki/Network_interface_controller#RSS" target="_blank" rel="external">Network interface controller - Wikipedia</a></p>
<p>简单说就是现在支持 RSS 的网卡内部会有多个 Ring Buffer，NIC 收到 Frame 的时候能通过 Hash Function 来决定 Frame 该放在哪个 Ring Buffer 上，触发的 IRQ 也可以通过操作系统或者手动配置 IRQ affinity 将 IRQ 分配到多个 CPU 上。这样 IRQ 能被不同的 CPU 处理，从而做到 Ring Buffer 上的数据也能被不同的 CPU 处理，从而提高数据的并行处理能力。</p>
<p>RSS 除了会影响到 NIC 将 IRQ 发到哪个 CPU 之外，不会影响别的逻辑了。收消息过程跟之前描述的是一样的。</p>
<p>如果支持 RSS 的话，NIC 会为每个队列分配一个 IRQ，通过 <code>/proc/interrupts</code> 能进行查看。你可以指定 IRQ 由哪个 CPU 来处理中断。先通过 <code>/proc/interrupts</code> 找到 IRQ 号之后，将希望绑定的 CPU 号写入 <code>/proc/irq/IRQ_NUMBER/smp_affinity</code>，写入的是 16 进制的 bit mask。比如看到队列 rx_0 对应的中断号是 41 那就执行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">echo 6 &gt; /proc/irq/41/smp_affinity</div><div class="line">6 表示的是 CPU2 和 CPU1</div></pre></td></tr></table></figure></p>
<p>0 号 CPU 的掩码是 0x1 (0001)，1 号 CPU 掩码是 0x2 (0010)，2 号 CPU 掩码是 0x4 (0100)，3 号 CPU 掩码是 0x8 (1000) 依此类推。</p>
<p>另外需要注意的是设置 smp_affinity 的话不能开启 irqbalance 或者需要为 irqbalance 设置 –banirq 列表，将设置了 smp_affinity 的 IRQ 排除。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L99-L222" target="_blank" rel="external">Receive Packet Steering(RPS)</a> 是在 NIC 不支持 RSS 时候在软件中实现 RSS 类似功能的机制。其好处就是对 NIC 没有要求，任何 NIC 都能支持 RPS，但缺点是 NIC 收到数据后 DMA 将数据存入的还是一个 Ring Buffer，NIC 触发 IRQ 还是发到一个 CPU，还是由这一个 CPU 调用 driver 的 <code>poll</code> 来将 Ring Buffer 的数据取出来。RPS 是在单个 CPU 将数据从 Ring Buffer 取出来之后才开始起作用，它会为每个 Packet 计算 Hash 之后将 Packet 发到对应 CPU 的 backlog 中，并通过 Inter-processor Interrupt(IPI) 告知这个 CPU 来处理 backlog。后续 Packet 的处理流程就由这个 CPU 来完成。</p>
<p>RPS 默认是关闭的，当机器有多个 CPU 并且通过 softirqs 的统计 <code>/proc/softirqs</code> 发现 NET_RX 在 CPU 上分布不均匀或者发现网卡不支持 mutiqueue 时，就可以考虑开启  RPS。开启 RPS 需要调整 <code>/sys/class/net/DEVICE_NAME/queues/QUEUE/rps_cpus</code> 的值。比如执行:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo f &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus</div></pre></td></tr></table></figure></p>
<p>表示的含义是处理网卡 eth0 的 rx-0 队列的 CPU 数设置为 f 。即设置有 15 个 CPU 来处理 rx-0 这个队列的数据，如果你的 CPU 数没有这么多就会默认使用所有 CPU 。甚至有人为了方便都是直接将 <code>echo fff &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus</code> 写到脚本里，这样基本能覆盖所有类型的机器，不管机器 CPU 数有多少，都能覆盖到。</p>
<p><em>注意：</em>如果 NIC 不支持 mutiqueue，RPS 不是完全不用思考就能打开的，因为其开启之后会加重所有 CPU 的负担，在一些场景下比如 CPU 密集型应用上并不一定能带来好处。所以得测试一下。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L225" target="_blank" rel="external">Receive Flow Steering(RFS)</a> 一般和 RPS 配合一起工作。RPS 是将收到的 packet 发配到不同的 CPU 以实现负载均衡，但是可能同一个 Flow 的数据包一会被发到 CPU1 一会被发到 CPU2，会降低 CPU cache hit 比率。RFS 就是保证同一个 flow 的 packet 都会被路由到同一个 CPU，从而提高 CPU cache 比率。<a href="https://github.com/torvalds/linux/blob/master/Documentation/networking/scaling.txt" target="_blank" rel="external">这篇文章</a> 把 RFS 机制介绍的挺好的。基本上就是收到数据后根据数据的一些信息做个 Hash 在这个 table 的 entry 中找到当前正在处理这个 flow 的 CPU 信息，从而将数据发给这个正在处理该 Flow 数据的 CPU 上，从而做到提高 CPU cache hit 率。当然还有很多细节，请看上面链接。</p>
<p>RFS 默认是关闭的，必须主动配置才能生效。正常来说开启了 RPS 都要再开启 RFS，以获取更好的性能。<a href="https://github.com/torvalds/linux/blob/master/Documentation/networking/scaling.txt" target="_blank" rel="external">这篇文章</a>也有说该怎么去开启 RFS 以及推荐的配置值。一个是要配置 rps_sock_flow_entries<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sysctl -w net.core.rps_sock_flow_entries=32768</div></pre></td></tr></table></figure></p>
<p>这个值依赖于系统期望的活跃连接数，注意是同一时间活跃的连接数，这个连接数正常来说会大大小于系统能承载的最大连接数，因为大部分连接不会同时活跃。该值建议是 32768，能覆盖大多数情况，每个活跃连接会分配一个 entry。除了这个之外还要配置 rps_flow_cnt：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo 2048 &gt; /sys/class/net/eth0/queues/rx-0/rps_flow_cnt</div></pre></td></tr></table></figure></p>
<p>配置的是每个队列分配的 entry 数量。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L324" target="_blank" rel="external">Accelerated Receive Flow Steering (aRFS)</a> 类似 RFS 只是由硬件协助完成这个工作。aRFS 对于 RFS 就和 RSS 对于 RPS 一样，就是把 CPU 的工作挪到了硬件来做，从而不用浪费 CPU 时间，直接由 NIC 完成 Hash 值计算并将数据发到目标 CPU，所以快一点。NIC 必须暴露出来一个 <code>ndo_rx_flow_steer</code> 的函数用来实现 aRFS。</p>
<h3 id="adaptive-RX-TX-IRQ-coalescing"><a href="#adaptive-RX-TX-IRQ-coalescing" class="headerlink" title="adaptive RX/TX IRQ coalescing"></a>adaptive RX/TX IRQ coalescing</h3><p>有的 NIC 支持这个功能，用来动态的将 IRQ 进行合并，以做到在数据包少的时候减少数据包的延迟，在数据包多的时候提高吞吐量。查看方法:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ethtool -c eth1</div><div class="line">Coalesce parameters for eth1:</div><div class="line">Adaptive RX: off  TX: off</div><div class="line">stats-block-usecs: 0</div><div class="line">.....</div></pre></td></tr></table></figure></p>
<p>开启 RX 队列的 adaptive coalescing 执行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -C eth0 adaptive-rx on</div></pre></td></tr></table></figure></p>
<p>并且有四个值需要设置：rx-usecs、rx-frames、rx-usecs-irq、rx-frames-irq，具体含义等需要用到的时候查吧。</p>
<h3 id="Ring-Buffer-相关监控及配置"><a href="#Ring-Buffer-相关监控及配置" class="headerlink" title="Ring Buffer 相关监控及配置"></a>Ring Buffer 相关监控及配置</h3><h4 id="收到数据包统计"><a href="#收到数据包统计" class="headerlink" title="收到数据包统计"></a>收到数据包统计</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">ethtool -S eh0</div><div class="line">NIC statistics:</div><div class="line">     rx_packets: 792819304215</div><div class="line">     tx_packets: 778772164692</div><div class="line">     rx_bytes: 172322607593396</div><div class="line">     tx_bytes: 201132602650411</div><div class="line">     rx_broadcast: 15118616</div><div class="line">     tx_broadcast: 2755615</div><div class="line">     rx_multicast: 0</div><div class="line">     tx_multicast: 10</div></pre></td></tr></table></figure>
<p>RX 就是收到数据，TX 是发出数据。还会展示 NIC 每个队列收发消息情况。<em>其中比较关键的是带有 drop 字样的统计和 fifo_errors 的统计</em> :<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">tx_dropped: 0</div><div class="line">rx_queue_0_drops: 93</div><div class="line">rx_queue_1_drops: 874</div><div class="line">....</div><div class="line">rx_fifo_errors: 2142</div><div class="line">tx_fifo_errors: 0</div></pre></td></tr></table></figure></p>
<p>看到发送队列和接收队列 drop 的数据包数量显示在这里。并且所有 queue_drops 加起来等于 rx_fifo_errors。所以总体上能通过 rx_fifo_errors 看到 Ring Buffer 上是否有丢包。如果有的话一方面是看是否需要调整一下每个队列数据的分配，或者是否要加大 Ring Buffer 的大小。</p>
<p><code>/proc/net/dev</code>是另一个数据包相关统计：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cat /proc/net/dev</div><div class="line">Inter-|   Receive                                                |  Transmit</div><div class="line"> face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed</div><div class="line">    lo: 14472296365706 10519818839    0    0    0     0          0         0 14472296365706 10519818839    0    0    0     0       0          0</div><div class="line">  eth1: 164650683906345 785024598362    0    0 2142     0          0         0 183711288087530 704887351967    0    0    0     0       0          0</div></pre></td></tr></table></figure></p>
<h5 id="调整-Ring-Buffer-队列数量"><a href="#调整-Ring-Buffer-队列数量" class="headerlink" title="调整 Ring Buffer 队列数量"></a>调整 Ring Buffer 队列数量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ethtool -l eth0</div><div class="line">Channel parameters for eth0:</div><div class="line">Pre-set maximums:</div><div class="line">RX:             0</div><div class="line">TX:             0</div><div class="line">Other:          1</div><div class="line">Combined:       8</div><div class="line">Current hardware settings:</div><div class="line">RX:             0</div><div class="line">TX:             0</div><div class="line">Other:          1</div><div class="line">Combined:       8</div></pre></td></tr></table></figure>
<p>看的是 Combined 这一栏是队列数量。Combined 按说明写的是多功能队列，猜想是能用作 RX 队列也能当做 TX 队列，但数量一共是 8 个？</p>
<p>如果不支持 mutiqueue 的话上面执行下来会是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Channel parameters for eth0:</div><div class="line">Cannot get device channel parameters</div><div class="line">: Operation not supported</div></pre></td></tr></table></figure></p>
<p>能自己设置 Ring Buffer 数量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ethtool -L eth0 combined 8</div></pre></td></tr></table></figure></p>
<p>如果支持对特定类型 RX 或 TX 设置队列数量的话可以执行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ethtool -L eth0 rx 8</div></pre></td></tr></table></figure></p>
<p><em>需要注意的是</em>，ethtool 的设置操作可能都要重启一下才能生效。</p>
<h5 id="调整-Ring-Buffer-队列大小"><a href="#调整-Ring-Buffer-队列大小" class="headerlink" title="调整 Ring Buffer 队列大小"></a>调整 Ring Buffer 队列大小</h5><p>先查看当前 Ring Buffer 大小：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ethtool -g eth0</div><div class="line">Ring parameters for eth0:</div><div class="line">Pre-set maximums:</div><div class="line">RX:   4096</div><div class="line">RX Mini:  0</div><div class="line">RX Jumbo: 0</div><div class="line">TX:   4096</div><div class="line">Current hardware settings:</div><div class="line">RX:   512</div><div class="line">RX Mini:  0</div><div class="line">RX Jumbo: 0</div><div class="line">TX:   512</div></pre></td></tr></table></figure></p>
<p>看到 RX 和 TX 最大是 4096，当前值为 512。<em>队列越大丢包的可能越小，但数据延迟会增加</em></p>
<p>设置 RX 队列大小：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -G eth0 rx 4096</div></pre></td></tr></table></figure></p>
<h5 id="调整-Ring-Buffer-队列的权重"><a href="#调整-Ring-Buffer-队列的权重" class="headerlink" title="调整 Ring Buffer 队列的权重"></a>调整 Ring Buffer 队列的权重</h5><p>NIC 如果支持 mutiqueue 的话 NIC 会根据一个 Hash 函数对收到的数据包进行分发。能调整不同队列的权重，用于分配数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">ethtool -x eth0</div><div class="line">RX flow hash indirection table for eth0 with 8 RX ring(s):</div><div class="line">    0:      0     0     0     0     0     0     0     0</div><div class="line">    8:      0     0     0     0     0     0     0     0</div><div class="line">   16:      1     1     1     1     1     1     1     1</div><div class="line">   ......</div><div class="line">   64:      4     4     4     4     4     4     4     4</div><div class="line">   72:      4     4     4     4     4     4     4     4</div><div class="line">   80:      5     5     5     5     5     5     5     5</div><div class="line">   ......</div><div class="line">  120:      7     7     7     7     7     7     7     7</div></pre></td></tr></table></figure>
<p>我的 NIC 一共有 8 个队列，一个有 128 个不同的 Hash 值，上面就是列出了每个 Hash 值对应的队列是什么。最左侧 0 8 16 是为了能让你快速的找到某个具体的 Hash 值。比如 Hash 值是 76 的话我们能立即找到 72 那一行：”72:      4     4     4     4     4     4     4     4”，从左到右第一个是 72 数第 5 个就是 76 这个 Hash 值对应的队列是 4 。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -X eth0 weight 6 2 8 5 10 7 1 5</div></pre></td></tr></table></figure>
<p>设置 8 个队列的权重。加起来不能超过 128 。128 是 indirection table 大小，每个 NIC 可能不一样。</p>
<h5 id="更改-Ring-Buffer-Hash-Field"><a href="#更改-Ring-Buffer-Hash-Field" class="headerlink" title="更改 Ring Buffer Hash Field"></a>更改 Ring Buffer Hash Field</h5><p>分配数据包的时候是按照数据包内的某个字段来进行的，这个字段能进行调整。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ethtool -n eth0 rx-flow-hash tcp4</div><div class="line">TCP over IPV4 flows use these fields for computing Hash flow key:</div><div class="line">IP SA</div><div class="line">IP DA</div><div class="line">L4 bytes 0 &amp; 1 [TCP/UDP src port]</div><div class="line">L4 bytes 2 &amp; 3 [TCP/UDP dst port]</div></pre></td></tr></table></figure>
<p>查看 tcp4 的 Hash 字段。</p>
<p>也可以设置 Hash 字段：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -N eth0 rx-flow-hash udp4 sdfn</div></pre></td></tr></table></figure></p>
<p>sdfn 需要查看 ethtool 看其含义。</p>
<h5 id="softirq-数统计"><a href="#softirq-数统计" class="headerlink" title="softirq 数统计"></a>softirq 数统计</h5><p>通过 <code>/proc/softirqs</code> 能看到每个 CPU 上 softirq 数量统计：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">cat /proc/softirqs</div><div class="line">                    CPU0       CPU1       </div><div class="line">          HI:          1          0</div><div class="line">       TIMER: 1650579324 3521734270</div><div class="line">      NET_TX:   10282064   10655064</div><div class="line">      NET_RX: 3618725935       2446</div><div class="line">       BLOCK:          0          0</div><div class="line">BLOCK_IOPOLL:          0          0</div><div class="line">     TASKLET:      47013      41496</div><div class="line">       SCHED: 1706483540 1003457088</div><div class="line">     HRTIMER:    1698047   11604871</div><div class="line">         RCU: 4218377992 3049934909</div></pre></td></tr></table></figure></p>
<p>看到 NET_RX 就是收消息时候触发的 softirq，一般看这个统计是为了看看 softirq 在每个 CPU 上分布是否均匀，不均匀的话可能就需要做一些调整。比如上面看到 CPU0 和 CPU1 两个差距很大，原因是这个机器的 NIC 不支持 RSS，没有多个 Ring Buffer。开启 RPS 后就均匀多了。</p>
<h5 id="IRQ-统计"><a href="#IRQ-统计" class="headerlink" title="IRQ 统计"></a>IRQ 统计</h5><p><code>/proc/interrupts</code> 能看到每个 CPU 的 IRQ 统计。可能就是看看 NAPI 的 IRQ 合并机制是否生效。看看 IRQ 是不是增长的很快。</p>
<h2 id="Per-CPU-Backlog"><a href="#Per-CPU-Backlog" class="headerlink" title="Per CPU Backlog"></a>Per CPU Backlog</h2><p>前面说到数据是交给 <code>netif_receive_skb</code> 来做进一步的处理，而<code>netif_receive_skb</code> 基本没干什么事情，主要事情都在 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3983" target="_blank" rel="external">netif_receive_skb_internal</a> 中完成。此时数据处理都还在软中断的 Handler 中，<code>top</code> 的 <code>si</code> 能反应出 CPU 在这个阶段花费的时间。</p>
<p>如果没有开启 RPS 会调用 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3958" target="_blank" rel="external">__netif_receive_skb</a>，进而调用 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3806" target="_blank" rel="external">__netif_receive_skb_core</a> 这就基本上进入 Protocol Layer 了。</p>
<p>如果开启了 RPS 则还有一大段路要走，先调用<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3482" target="_blank" rel="external">enqueue_to_backlog</a>准备将数据包放入 CPU 的 Backlog 中。入队之前会检查队列长度，如果队列长度大于 <code>net.core.netdev_max_backlog</code> 设置的值，则会丢弃数据包。同时也会检查 flow limit，超过的话也会丢弃数据包。丢弃的话会记录在 <code>/proc/net/softnet_stat</code> 中。入队的时候还会检查目标 CPU 的 NAPI 处理 backlog 的逻辑是否运行，没运行的话会通过 __napi_schedule 设置目标 CPU 处理 backlog 逻辑。之后会将数据包方式 backlog 发送 Inter-process Interrupt 去唤醒目标 CPU。</p>
<p>CPU 处理 backlog 的方式和 CPU 去调用 driver 的 <code>poll</code> 函数拉取 Ring Buffer 数据方法类似，也是注册了一个 <code>poll</code> 函数，只是这个 “poll” 函数在这里是 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L4534" target="_blank" rel="external">process_backlog</a> 并且是操作系统 network 相关子系统启动时候注册的。<code>process_backlog</code> 内就是个循环，跟 driver 的 <code>poll</code> 一样不断的从 backlog 中取出数据来处理。调用 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3958" target="_blank" rel="external">__netif_receive_skb</a>，进而调用 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3806" target="_blank" rel="external">__netif_receive_skb_core</a> ，跟关闭 RPS 情况下逻辑一样。并且也会按照 budget 来判断要处理多久而退出循环。budget 跟之前控制 <code>netif_rx_action</code> 执行时间的 budget 配置一样，也是 <code>net.core.netdev_budget</code> 这个系统配置来控制。</p>
<h3 id="net-core-netdev-max-backlog"><a href="#net-core-netdev-max-backlog" class="headerlink" title="net.core.netdev_max_backlog"></a>net.core.netdev_max_backlog</h3><p>上面说了将数据包放入 CPU 的 backlog 的时候需要看队列内当前积压的数据包有多少，超过 <code>net.core.netdev_max_backlog</code> 后要丢弃数据。所以可以根据需要来调整这个值：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sysctl -w net.core.netdev_max_backlog=2000</div></pre></td></tr></table></figure></p>
<p>需要注意的是，好多地方介绍在做压测的时候建议把这个值调高一点，但从我们上面的分析能看出来，这个值基本上只有在 RPS 开启的情况下才有用，没开启 RPS 的话设置这个值并没意义。</p>
<h3 id="Flow-Limit"><a href="#Flow-Limit" class="headerlink" title="Flow Limit"></a>Flow Limit</h3><p>如果一个 Flow 或者说连接数据特别多，发送数据速度也快，可能会出现该 Flow 的数据包把所有 CPU 的 Backlog 都占满的情况，从而导致一些数据量少但延迟要求很高的数据包不能快速的被处理。所以就有了 Flow Limit 机制来限制每个 Flow 能排在 CPU Backlog 的总数，超过这个数值后，这个 Flow 就不能在放数据到 Backlog，必须丢弃。从而尽可能公平一些的处理所有 Flow 上的数据。</p>
<p>开启 Flow Limit 首先要设置 flow table 大小：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sysctl -w net.core.flow_limit_table_len=8192</div></pre></td></tr></table></figure></p>
<p>默认值是 4096。</p>
<p>之后需要为单个 CPU 开启 Flow Limit，这两个配置先后顺序不能搞错:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo f &gt; /proc/sys/net/core/flow_limit_cpu_bitmap</div></pre></td></tr></table></figure></p>
<p>这个跟开启 RPS 的配置类似，也是个 bitmap 来标识哪些 CPU 开启 Flow Limit。如果希望所有 CPU 都开启就设置个大一点的值，不管有多少 CPU 都能覆盖。</p>
<h3 id="丢弃数据包统计"><a href="#丢弃数据包统计" class="headerlink" title="丢弃数据包统计"></a>丢弃数据包统计</h3><p>如果因为 backlog 不够或者 flow limit 不够数据包被丢弃的话会将丢包信息计入 <code>/proc/net/softnet_stat</code>。我们也能在这里看到有没有丢包发生：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"> cat /proc/net/softnet_stat</div><div class="line">930c8a79 00000000 0000270b 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000</div><div class="line">280178c6 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 0cbbd3d4 00000000</div></pre></td></tr></table></figure></p>
<p>一个 CPU 一行数据。但比较麻烦的是每一列具体表示的是什么意思没有明确文档，可能不同版本的 kernel 打印的数据不同。需要看 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/net-procfs.c#L146" target="_blank" rel="external">softnet_seq_show</a> 这个函数是怎么打印的。一般来说第二列是丢包数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">seq_printf(seq,</div><div class="line">           &quot;%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n&quot;,</div><div class="line">           sd-&gt;processed, sd-&gt;dropped, sd-&gt;time_squeeze, 0,</div><div class="line">           0, 0, 0, 0, /* was fastroute */</div><div class="line">           sd-&gt;cpu_collision, sd-&gt;received_rps, flow_limit_count);</div></pre></td></tr></table></figure></p>
<p>time_squeeze 是 net_rx_action 执行的时候因为 budget 不够而停止的次数。这说明数据包多而 budget 小，增大 budget 有助于更快的处理数据包<br>cpu_collision 是发消息时候 CPU 去抢 driver 的锁没抢到的次数<br>received_rps 是 CPU 被通过 Inter-processor Interrupt 唤醒来处理 Backlog 数据的次数。上面例子中看到只有 CPU1 被唤醒过，因为这个 NIC 只有一个 Ring Buffer，IRQ 都是 CPU0 在处理，所以开启 RPS 后都是 CPU0 将数据发到 CPU1 的 Backlog 然后唤醒 CPU1；<br>flow_limit_count 标识触碰 flow limit 的次数</p>
<h2 id="Internet-Protocol-Layer"><a href="#Internet-Protocol-Layer" class="headerlink" title="Internet Protocol Layer"></a>Internet Protocol Layer</h2><p>前面介绍到不管开启还是关闭 RPS 都会通过 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3806" target="_blank" rel="external">__netif_receive_skb_core</a> 将数据包传入上层。传入前先会将数据包交到 pcap，tcpdump 就是基于 libcap 实现的，libcap 之所以能捕捉到所有的数据包就是在 <code>__netif_receive_skb_core</code> 实现的。具体位置在：<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3850" target="_blank" rel="external">http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3850</a></p>
<p>可以看到这个时候还是在 softirq 的 handler 中呢，所以 tcpdump 这种工具一定是会在一定程度上延长 softirq 的处理时间。</p>
<p>之后就是在 <code>__netif_receive_skb_core</code> 里会遍历 ptype_base 链表，找出 Protocol Layer 中能处理当前数据包的 packet_type 来接着处理数据。所有能处理链路层数据包的协议都会注册到 ptype_base 中。拿 ipv4 来说，在初始化的时候会执行 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/af_inet.c#L1791" target="_blank" rel="external">inet_init</a>，看到在这里会构造 ip_packet_type 并执行 <code>dev_add_pack</code>，进入<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L395" target="_blank" rel="external">dev_add_pack</a>  能看到是将 ip_packet_type 加入到 ptype_head 指向的链表中，这里 ptype_head 取到的就是 ptype_base。</p>
<p>回到 ip_packet_type 我们看到其定义为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">static struct packet_type ip_packet_type __read_mostly = &#123;</div><div class="line">    .type = cpu_to_be16(ETH_P_IP),</div><div class="line">    .func = ip_rcv,</div><div class="line">&#125;;</div></pre></td></tr></table></figure></p>
<p>在 <code>__netif_receive_skb_core</code> 中找到 sk_buff 对应的 protocol 是 ETH_P_IP 时就会执行 ip_packet_type 下的 func 函数，即 ip_rcv，从而将数据包交给 Protocol Layer 开始处理了。</p>
<h3 id="ip-rcv"><a href="#ip-rcv" class="headerlink" title="ip_rcv"></a>ip_rcv</h3><p><code>ip_rcv</code> 能看出来逻辑比较简单，基本就是在做各种检查以及为 transport 层做一些数据准备。最后如果各种检查都能过，就执行 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/ip_input.c#L455" target="_blank" rel="external">NF_HOOK</a>。如果有检查不过需要丢弃数据包就会返回 NET_RX_DROP 并在之后会对丢数据包这个事情进行计数。</p>
<p>NF_HOOK 比较神，它实际是 HOOK 到一个叫做 <a href="https://en.wikipedia.org/wiki/Netfilter" target="_blank" rel="external">Netfilter</a> 的东西，在这里你可以根据各种规则对数据包做过滤以及对数据包做一些修改。如果 HOOK 执行后返回 1 表示 Netfilter 允许继续处理该数据包，就会进入 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/ip_input.c#L314" target="_blank" rel="external">ip_rcv_finish</a>，HOOK 没有返回 1 则会返回 Netfilter 的结果，数据包不会继续被处理。</p>
<p><code>ip_rcv_finish</code> 负责为 sk_buff 从 IP Route System 中找到路由目标，如果是路由到本机则在下一个处理这个 sk_buff 的协议内还需要从 sk_buff 中找到对应的 socket。也就是说每个收到的数据包都会有两次 demux 工作。但是对于类似 TCP 这种协议当 socket 处在 ESTABLISHED 状态后，协议栈不会出现变化，后来的数据包的路由路径跟握手时数据包的路由路径完全相同，所以就有了 Early Demux 机制，用于在收到数据包的时候根据 IP Header 中的 protocol 字段找到上一层网络协议，用上一层网络协议来解析数据包的路由路径，以减少一次查询。拿 TCP 来说，<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/af_inet.c#L1506" target="_blank" rel="external">TCP 会将自己的处理函数</a>在 IP 层初始化的时候<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/af_inet.c#L1724" target="_blank" rel="external">注册</a>在 IP 层的 <a href="http://elixir.free-electrons.com/linux/v4.4/source/include/net/protocol.h#L95" target="_blank" rel="external">inet_protos</a> 中。看到 TCP 注册的这些处理函数中就有 early_demux 函数 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_ipv4.c#L1432" target="_blank" rel="external">tcp_v4_early_demux</a>。在 <code>tcp_v4_early_demux</code> 中我们看到主要是根据 sk_buff 的 source addr、dest addr 等信息从 ESTABLISHED 连接列表中找到当前数据包所属的 Socket，并获取 Socket 中的 sk_rx_dst 即 struct dst_entry 设置到 sk_buff 中。之后这个 sk_buff 就会被路由到 sk_rx_dst 所指的位置，并且会将找到的 Socket 的 struct sock 指针存入 sk_buff 避免再次查找 ESTABLISHED 连接表。后面会看到 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_ipv4.c#L1272" target="_blank" rel="external">TCP 新建立的 Socket 会从 sk_buff 中读取 dst_entry 设置到 struct sock 的 sk_rx_dst 中</a>。struct sock 中的 sk_rx_dst 在这里：<a href="http://elixir.free-electrons.com/linux/v4.4/source/include/net/sock.h#L393" target="_blank" rel="external">linux/include/net/sock.h - Elixir - Free Electrons</a>。</p>
<p>如果 IP Early Demux 没有起作用，比如当前 sk_buff 可能是 Flow 的第一个数据包，Socket 还未处在 ESTABLISHED 状态，所以还未找到这个 Socket 也就无法进行 Early Demux。则需要调用 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/route.c#L1914" target="_blank" rel="external">ip_route_input_noref</a>经过 IP Route System 去处理 sk_buff 查找这个 sk_buff 该由谁处理，是不是当前机器处理，还是要转发出去。这个路由机制看上去还挺复杂的，怪不得需要 Early Demux 机制来省略该步骤呢。如果 IP Route System 找了一圈之后发现这个 sk_buff 确实是需要当前机器处理，最终会设置 dst_entry 指向的函数为 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/ip_input.c#L245" target="_blank" rel="external">ip_local_deliver</a>。</p>
<p>需要补充一下 Early Demux 对 Socket 还未处在 ESTABLISHED 状态的 TCP 连接无效。这就导致这种数据包不但会查一次 IP Route System 还会到 TCP ESTABLISHED 连接表中查一次，总体开销就会比只查一次 IP Route System 还要大。所以 Early Demux 并不是无代价的，只是大多数场景可能开启后会对性能有提高，所以 Linux 默认是开启的。但在某些场景下，目前来看应该是大量短连接的场景，连接要不断建立断开，有大量的数据包都是在 TCP ESTABLISHED 表中查不到东西，这个机制开启后性能会有损耗，所以 Linux 提供了关闭该机制的办法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sysctl -w net.ipv4.ip_early_demux=0</div></pre></td></tr></table></figure></p>
<p>有人测试在特定场景下这个机制会带来最大 5% 的损耗：<a href="https://patchwork.ozlabs.org/patch/166441/" target="_blank" rel="external">https://patchwork.ozlabs.org/patch/166441/</a></p>
<p>Early Demux 和查询 IP Route System 都是为了设置 sk_buff 中的 dst_entry，通过 dst_entry 来跳到下一个负责处理该 sk_buff 的函数。这个跳转由 <code>ip_rcv_finish</code> 最后的 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/ip_input.c#L365" target="_blank" rel="external">dst_input</a> 来完成。<code>dst_input</code> 实现很简单：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">return skb_dst(skb)-&gt;input(skb);</div></pre></td></tr></table></figure></p>
<p>就是从 sk_buff 中读出来之前构造好的 <a href="http://elixir.free-electrons.com/linux/v4.4/source/include/net/dst.h#L33" target="_blank" rel="external">struct dst_entry</a>，执行里面的 input 指向的函数并将 sk_buff 交进去。</p>
<p>如果 sk_buff 就是发给当前机器的话，Early Demux 和查询 IP Route System 都会最终走到 <code>ip_local_deliver</code>。</p>
<h3 id="ip-local-deliver"><a href="#ip-local-deliver" class="headerlink" title="ip_local_deliver"></a>ip_local_deliver</h3><p>很简单，做三个事情：</p>
<ol>
<li>判断是否有 IP Fragment，有的话就先存下这个 sk_buff 直接返回，等后续数据包来了之后进行组装；</li>
<li>还是通过和 <code>ip_rcv</code> 里一样的 NET_HOOK 将数据包发到 Netfilter 做过滤</li>
<li>如果数据包被过滤掉了，就直接丢弃数据包返回，没过滤掉最终会执行 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/ip_input.c#L192" target="_blank" rel="external">ip_local_deliver_finish</a></li>
</ol>
<p><code>ip_local_deliver_finish</code> 内会取出 IP Header 中的 protocol 字段，根据该字段在上面提到过的 <a href="http://elixir.free-electrons.com/linux/v4.4/source/include/net/protocol.h#L95" target="_blank" rel="external">inet_protos</a>中找到 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/af_inet.c#L1724" target="_blank" rel="external">IP 层初始化时注册</a>过的上层协议处理函数。拿 TCP 来说，TCP 注册的信息在这里： <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/af_inet.c#L1506" target="_blank" rel="external">linux/net/ipv4/af_inet.c - Elixir - Free Electrons</a>。<code>ip_local_deliver_finish</code> 会调用注册的 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/ip_input.c#L216" target="_blank" rel="external">handler 函数</a>，对 TCP 来说就是 <code>tcp_v4_rcv</code>。</p>
<p>IP 层在处理数据过程中会更新很多计数，在 <a href="http://elixir.free-electrons.com/linux/v4.4/source/include/uapi/linux/snmp.h#L24" target="_blank" rel="external">snmp.h 这个文件中</a>可以看看。基本上 <code>proc/net/netstat</code> 中展示的带有 IP 字样的统计都是这个文件中定义的。</p>
<h2 id="TCP-Protocol-Layer"><a href="#TCP-Protocol-Layer" class="headerlink" title="TCP Protocol Layer"></a>TCP Protocol Layer</h2><p>漫漫长路，终于到了我们比较熟悉的 TCP 一层了，但很快就会发现上面这一大堆内容什么 NIC、中断、IP 路由之类的加起来可能都没有 TCP 一层的内容复杂，单是 goto 都比别的地方用的都多。因为 TCP 有状态，不同状态下收不到不同数据会有不同的行为，就导致了这个复杂度。为了不陷入 TCP 各种细节逻辑中，我们还是先只看最简单的连接处在 ESTABLISHED 状态的收消息过程。TCP 数据还分为 Normal 和 Urgent 两种，两种类型数据在处理过程中并不相同，为了简单起见，这里只大致介绍 Normal 的数据接收过程。</p>
<p>先推荐一本书叫做 《TCP/IP Architecture, Design and Implementation in Linux》 ，对 TCP 收消息这块逻辑讲的比较清楚，带着你理内核代码。如果有兴趣深究这块看这本书挺好的。</p>
<p>正常来说 TCP 收消息过程会涉及三个队列：</p>
<ol>
<li>Backlog Queue <a href="http://elixir.free-electrons.com/linux/v4.4/source/include/net/sock.h#L373" target="_blank" rel="external">sk-&gt;sk_backlog</a></li>
<li>Prequeue <a href="http://elixir.free-electrons.com/linux/v4.4/source/include/linux/tcp.h#L182" target="_blank" rel="external">tp-&gt;ucopy.prequeue</a></li>
<li>Receive Queue <a href="http://elixir.free-electrons.com/linux/v4.4/source/include/net/sock.h#L359" target="_blank" rel="external">sk-&gt;sk_receive_queue</a></li>
</ol>
<p>当然还有个 out of order queue <a href="http://elixir.free-electrons.com/linux/v4.4/source/include/linux/tcp.h#L275" target="_blank" rel="external">tp-&gt;out_of_order_queue</a>，先不管它，就先只看最简单的逻辑，不然会在复杂的 TCP 逻辑中迷失的。上述三个队列在处理数据的时候是序号大的队列优先级更高，先处理完序号大的队列之后才会处理序号小的队列。<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_ipv4.c#L1528" target="_blank" rel="external">tcp_v4_rcv</a>会负责将收到的数据包在上面三个队列之间做分配：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">bh_lock_sock_nested(sk);</div><div class="line">ret = 0;</div><div class="line">if (!sock_owned_by_user(sk)) &#123;</div><div class="line">    if (!tcp_prequeue(sk, skb))</div><div class="line">        ret = tcp_v4_do_rcv(sk, skb);</div><div class="line">&#125; else if (unlikely(sk_add_backlog(sk, skb,</div><div class="line">                   sk-&gt;sk_rcvbuf + sk-&gt;sk_sndbuf))) &#123;</div><div class="line">    bh_unlock_sock(sk);</div><div class="line">    NET_INC_STATS_BH(net, LINUX_MIB_TCPBACKLOGDROP);</div><div class="line">    goto discard_and_relse;</div><div class="line">&#125;</div><div class="line">bh_unlock_sock(sk);</div></pre></td></tr></table></figure></p>
<p>看到先是判断 socket 是否被 user 占用，如果被占用了，说明 User 正在读 socket 的数据，会操作 receivq queue 和 prequeue，所以此时将数据包放入 backlog 队列中，如果没有成功放入 backlog 队列比如 backlog 队列已经满了，则会丢弃数据包并更新 TcpBacklogDrop 计数。</p>
<p>如果数据包到的时候 User 没有占用 Socket 则先尝试将数据包放入 Prequeue，如果因为一些原因(稍后会说)放入失败的话就将数据包传入 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_ipv4.c#L1368" target="_blank" rel="external">tcp_v4_do_rcv</a>。在 <code>tcp_v4_do_rcv</code> 中如果连接已经处在 ESTABLISHED 状态，会走所谓的 Fast Path (过会在 <code>tcp_rcv_established</code> 内还会有个 Fast Path，都是一些优化)，将数据包经由调用 <code>tcp_rcv_established</code>放入 Receive Queue。如果是连接没有处在 ESTABLISHED，说明可能当前 sk_buff 内有 TCP 状态控制相关指令，又有数据。所以需要先到 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_input.c#L5745" target="_blank" rel="external">rcp_rcv_state_process</a> 经过一轮 TCP 状态转换，转换完之后再处理 sk_buff 内的数据。</p>
<p>在 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_input.c#L5228" target="_blank" rel="external">tcp_rcv_established</a> 内，从注释能看到 <code>tcp_rcv_established</code> 也有 Fast Path 和 Slow Path 之分，满足一大堆条件比如 Socket buffer 是否足够，TCP window 是否足够，是否是 Urgent Data 等之后，就能走 Fast Path，好处是更少的检查，更短的处理路径，从而能处理的更快。在 <code>tcp_rcv_established</code> 内会负责将 sk_buff 拷贝到 User Space 以及放入 receive queue。</p>
<p>Receive Queue 可以认为是 softIRQ 和 User Space 的分界线，softIRQ 负责将数据放入队列，用户调用读取数据的系统调用后会读取队列数据。</p>
<h3 id="Receive-Queue"><a href="#Receive-Queue" class="headerlink" title="Receive Queue"></a>Receive Queue</h3><p>与 Backlog 和 Prequeue 的不同点在于，放入 Receive Queue 的 sk_buff 都是已经被处理过的，抹去了所有 Protocol Header 信息，只有 sk_buff 中真正有用的会拷贝到 User space 的数据会放入 Receive Queue。并且，Receive Queue 中的数据一定是符合 TCP 序列的，所以才能被直接拷贝到 User Space。而其它两个队列入队的时候都还有 Header 信息，还未经过处理，而且可能包含乱序的数据。</p>
<p>后续在说处理 Receive Queue 部分的时候会看到 <code>tcp_v4_do_rcv</code> 和 <code>tcp_rcv_established</code>不光是在 softIRQ 内可能会执行，User Space 下也可能会执行。在 <code>tcp_rcv_established</code> 内如果发现用户进程正在读取 Socket，<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_input.c#L5315" target="_blank" rel="external">设置的 Receiver 刚好是当前进程(说明是 User Space 调用的 <code>tcp_rcv_established</code>，因为 Receiver 就是 Current 进程</a>，并且数据的 SEQ 表明其刚好是下一个需要的数据包，则会先尝试<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_input.c#L5321" target="_blank" rel="external">将该 sk_buff 直接拷贝到 User Space</a>。因为满足上面各种条件后，当前处理的数据包就一定是当前 TCP 连接上正在等待的下一个数据包，所以能直接将 sk_buff 拷贝到 User Space 不会产生乱序。如果不满足上面一堆条件，不能直接拷贝到 User Space，则会将 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_input.c#L4473" target="_blank" rel="external">sk_buff 内的 TCP Header 抹去</a>，并通过 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_input.c#L4478" target="_blank" rel="external">__skb_queue_tail</a> 将数据放入 sk_receive_queue。</p>
<h3 id="Prequeue"><a href="#Prequeue" class="headerlink" title="Prequeue"></a>Prequeue</h3><p><a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_ipv4.c#L1480" target="_blank" rel="external">如果没有开启 <code>net.ipv4.tcp_low_latency</code> 并且用户进程设置了 TCP Receiver Task 的话</a>，说明有个 User 进程正在读 Socket (因为只有正在读 Socket 时才会<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1719" target="_blank" rel="external">设置 tp.ucopy.task</a>)且还没有读够所需数据，<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1767" target="_blank" rel="external">正在 sleep 等待读入的数据</a>。此时会将 sk_buff 放入 Prequeue。</p>
<p>一般进入 Prequeue 的 sk_buff 是不进行处理的，还保留有 tcp header 等信息，sk_buff 直接放入 Prequeue，处理工作交给用户进程完成。但是当 sk_buff 放入 Prequeue 后<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_ipv4.c#L1500" target="_blank" rel="external">发现 socket 占用的内存超过了 sk_rcvbuf 的限制</a>，则需要立即将所有 Prequeue 内的 sk_buff 出队，并放入 <code>tcp_v4_do_rcv</code> 开始处理，最终 sk_buff 会被放入 Receive Queue。每个被处理的 sk_buff 都会更新 TCPPrequeueDropped 计数。因为会将 TCP Header 去除，所以占用的内存大小会少一些，如果还是超过 sk_rcvbuf，则会丢弃数据包。</p>
<p><a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_ipv4.c#L1512" target="_blank" rel="external">如果放入 Prequeue 的 sk_buff 是 Prequeue 内第一个元素</a>，则一方面会重置 ACK 回复时间，延迟 ACK 回复；另一方面会唤醒正在 sleep 等待数据到来的 User 进程。</p>
<h3 id="Backlog"><a href="#Backlog" class="headerlink" title="Backlog"></a>Backlog</h3><p>如果收到数据包时，Socket 正在被 User 占用，可能 User 正在读取数据，会操作 Receive Queue 和 Prequeue，所以新收到的数据就暂时不放入这些队列中，<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_ipv4.c#L1645" target="_blank" rel="external">而是放入 Backlog 里</a>。<a href="http://elixir.free-electrons.com/linux/v4.4/source/include/net/sock.h#L847" target="_blank" rel="external">如果 Backlog 满了</a> 会将数据包丢弃。</p>
<h3 id="出队处理"><a href="#出队处理" class="headerlink" title="出队处理"></a>出队处理</h3><p>上面都是数据包入队的过程，下面看看数据包出队的过程。</p>
<p>用户调用读取数据的系统调用后最终会走到 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1573" target="_blank" rel="external">tcp_recvmsg</a>，在读取数据前先<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c￼#L1595" target="_blank" rel="external">会将 Socket 上锁</a>，之后<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1628" target="_blank" rel="external">计算期望读取的数据量</a>。用户设置的期望读取数据量是 len，但是不一定非要读那么多数据才返回，系统有个 SO_RCVLOWAT 配置，表示如果当前没有 len 这么多数据时，就 block 住至少等待读到 SO_RCVLOWAT 这么多数据的时候才能返回。<a href="http://elixir.free-electrons.com/linux/v4.4/source/include/net/sock.h#L2096" target="_blank" rel="external">SO_RCVLOWAT 最少是 1 字节</a>。SO_RCVLOWAT 的结果会存入 target 变量。</p>
<p>处理数据是在一个大的 do while 循环内完成，用于从 Receive quque 或 Prequeue 上循环的读取数据。在开始处理之前，之后优先处理 Receive Queue，<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1646" target="_blank" rel="external">从 sk-&gt;sk_receive_queue 上每取下一个 sk_buff</a>，就<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1829" target="_blank" rel="external">拷贝到 User Space</a>。如果读到足够的数据超过 len 即用户期望的数据量，就退出循环直接返回，即使 Receive Queue 处理不完也会跳出循环。如果将 Receive Queue 处理完都一直读取不到 len 这么多数据，但至少读到了 target 这么多数据，也跳出循环<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1671" target="_blank" rel="external">去处理 Backlog 队列</a>，不 Block 等待数据。</p>
<p>如果 Receive Queue 处理完没有读到 len 这么多数据，也没读到 target 这么多数据，如果开启了 Prequeue 机制，则要<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1719" target="_blank" rel="external">配置一个 Receiver Task</a>后开始<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1785" target="_blank" rel="external">处理 Prequeue 上的数据</a>，将 Prequeue 的数据取出来之后调用 <code>tcp_v4_do_rcv</code> 进行处理。<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1453" target="_blank" rel="external">sk_backlog_rcv 指向的就是 <code>tcp_v4_do_rcv</code></a>。<code>tcp_prequeue_process</code> 内每处理一个 sk_buff 会更新 TCPPrequeued  计数。在 <code>tcp_prequeue_process</code> 内处理完 Prequeue 后会将从 Prequeue 内拷贝到 User Space 的数据量更新在 TCPDirectCopyFromPrequeue 计数内。Prequeue 处理完后 Receive Queue 内可能又有数据了，所以会经过 while 循环再次开始先处理 Receive Queue 再处理 Prequeue 的流程。</p>
<p>如果 Receive Queue、Prequeue 处理完获取到了足够的数据，则<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1764" target="_blank" rel="external">释放 Socket 锁去处理 Backlog</a>，但在 <code>release_lock</code> 内处理完 Backlog 后会立即再次加锁，再次开始 Receive Queue、Prequeue 的处理，尽力读取到用户期望的 len 这么多数据。跟处理完 Prequeue 时一样，处理完 Backlog 之后也要更新拷贝了多少数据到 User Space，更新到 TCPDirectCopyFromBacklog 计数里。</p>
<p>这里需要注意，如果开启了 Prequeue 机制，<code>release_lock</code>处理完 Backlog 后，因为设置的 Receiver Task 未被清理，所以 Backlog 内的数据一定都被挪到了 Prequeue。再一个可以看到 SO_RCVLOWAT 设置的最小读取数据量不是说读到这么多数据后就立即返回，而是指的在 Receive Queue、Prequeue、Backlog 三个队列全部读完的情况下，如果数据还没有达到用户需要的量时，只要能超过 SO_RCVLOWAT 指定的量后就能返回，而不会 Block。三个队列没处理完的时候即使读取的数据量超过了 SO_RCVLOWAT 但只要还未达到用户期望的 len 数据量则也不能返回。</p>
<p>如果处理完 Receive Queue、Prequeue、Backlog 数据量还是少于 SO_RCVLOWAT，则需要<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp.c#L1767" target="_blank" rel="external"> block 等待新数据到来</a>。在 <code>sk_wait_data</code> 内会释放 Socket 锁，从而新来的数据都会进入 Prequeue。在前面介绍 sk_buff 入队 Prequeue 时候介绍过在第一个 sk_buff 入队 Prequeue 时会唤醒这个 block 的进程。</p>
<p>虽然会去唤醒被 block 的进程，但如果当前机器负载过重，可能执行了唤醒但是目标进程很久都没被唤醒起来，此时延迟的 ACK 执行的时候会负责处理 Prequeue 内排队的数据。在 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_timer.c#L266" target="_blank" rel="external">tcp_delack_timer</a> 内如果 Socket 未被 User 进程占用，则会调用 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_timer.c#L219" target="_blank" rel="external">tcp_delack_timeer_handler</a>，能看到在这个 handler 内会从 Prequeue 取 sk_buff 下来，放入 <code>sk_backlog_rcv</code> 即 <code>tcp_v4_do_rcv</code> 内处理。这种情况下会更新：TCPSchedulerFailed 计数。正常情况下这个计数应该是 0，系统不该忙到都该回复 ACK 了还唤不醒目标进程。在 <code>netstat -s</code> 中能看到这个统计：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">4971 times receiver scheduled too late for direct processing</div></pre></td></tr></table></figure>
<p>如果在 <code>tcp_delack_timer</code> 内 Socket 被 User 进程占用，则会更新 TCPSchedulerFailed 计数并延迟 ACK 的回复。</p>
<h3 id="为什么要有-Prequeue"><a href="#为什么要有-Prequeue" class="headerlink" title="为什么要有 Prequeue"></a>为什么要有 Prequeue</h3><p>看到 Prequeue 是个可选项，默认是开启的但能通过 <code>net.ipv4.tcp_low_latency</code>来关闭。有这个选项存在就说明 Prequeue 存在的理由不像 Receive Queue 和 Backlog 一样那么明确可靠。所以我们需要看看 Prequeue 存在的原因。</p>
<p>有个关于 Prequeue 作用的讨论在这里： <a href="http://linux-kernel.2935.n7.nabble.com/TCP-prequeue-performance-td13885.html" target="_blank" rel="external">Linux Kernel - TCP prequeue performance</a>，可以参考一下。</p>
<p>如果关闭 Prequeue，我们知道如果 Socket 没有被 User 占用，收到的 sk_buff 会直接调用 <code>tcp_v4_do_rcv</code> 进行处理，放入 Receive Queue，这一切都会在 softIRQ 的 context 中执行，最关键的是在放入 Receive Queue 后会回复 ack，而实际此时用户进程并没有实际收到数据，离用户进程起来处理数据还有一段时间。这就导致对端收到 ack 后认为对方能很快处理数据从而会发的更快，直到对方 Receive Queue 满了之后突然不再回复 ack，开始丢包。而一般情况下 TCP 连接对性能影响最大的就是丢包，重传，所以需要尽可能避免上述情况的发生。这种情形下，ack 相当于是只送达了对方机器就被回复了，而没有送到目标进程。</p>
<p>有了 Prequeue 之后，ack 会有两种回复方式，一种是用户进程被唤醒将 Prequeue 数据读入 Receive Queue 后回复 ack，这种时候数据是确认送达用户进程了。另一种是用户进程迟迟无法被唤醒，延迟 ack 的定时器被触发而回复 ack，这样也能减慢 ack 回复速度让对端知道这边处理性能有点跟不上，要慢点发数据。两种方式都能减少或避免之前说的问题，这也是 Prequeue 存在的意义。</p>
<p>但是对于体量小延迟又要求高的数据包，Prequeue 的存在又会增加延迟。原因是如果关闭了 Prequeue 机制，每来一条数据都要经过 <code>tcp_v4_do_rcv</code> 的处理，上面我们只看了一下 Fast Path，但能走 Fast Path 的要求还是比较苛刻的，不能有乱序到达，数据只能是单向，要么单向收要么单向发，等等条件，只要有一条不满足就要走 Slow Path。Slow Path 内各种检查会更多，更麻烦一些。除了检查还一个耗时的是计算 checksum。如果没有 Prequeue 则这些逻辑全部要在 softIRQ context 内完成。在 User 进程被唤醒前可能只能放很少的数据到 Receive Queue 内。而有了 Prequeue 后，softIRQ 内只需要将数据包放入队列，不做任何检查和处理，接着就能处理下一个数据包，等到用户进程被唤醒后能从 Prequeue 批量处理数据。</p>
<p>不过 Prequeue 是 Linux 特有的机制，近些年因为 NIC 会自动计算 checksum，不需要在收到数据过程中再计算了，所以 Prequeue 存在的意义基本只是延迟 ack 回复到用户进程内这一个。开启它实际对延迟增加并不明显： <a href="http://hackingnasdaq.blogspot.jp/2010/01/myth-of-procsysnetipv4tcplowlatency.html" target="_blank" rel="external">the myth of /proc/sys/net/ipv4/tcp_low_latency</a> 、<a href="http://www.linuxvox.com/2009/11/what-is-the-linux-kernel-parameter-tcp_low_latency/" target="_blank" rel="external">What is the linux kernel parameter tcp_low_latency?</a>在 IPV6 内更是去掉了这个机制。</p>
<h3 id="Socket-Buffer-大小限制"><a href="#Socket-Buffer-大小限制" class="headerlink" title="Socket Buffer 大小限制"></a>Socket Buffer 大小限制</h3></div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://ylgrgyq.github.io/2017/07/23/linux-receive-packet/" data-id="cj5ggyo4i0007ynwxc4up1tmg" class="article-share-link">Share</a><div class="tags"><a href="/tags/Network/">Network</a></div><div class="post-nav"><a href="/2017/07/02/RTM-max-connections/" class="next">实时通信系统并发连接数测试时需要调整的各种参数</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Network/" style="font-size: 15px;">Network</a> <a href="/tags/TCP/" style="font-size: 15px;">TCP</a> <a href="/tags/Bug/" style="font-size: 15px;">Bug</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/Algorithm/" style="font-size: 15px;">Algorithm</a> <a href="/tags/Netty/" style="font-size: 15px;">Netty</a> <a href="/tags/Clojure/" style="font-size: 15px;">Clojure</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/07/23/linux-receive-packet/">Linux 网络协议栈收消息过程</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/02/RTM-max-connections/">实时通信系统并发连接数测试时需要调整的各种参数</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/30/tcp-time-wait/">TCP TIME-WAIT</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/18/tcp-backlog/">TCP Backlog</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/14/tls-sni/">一次 TLS SNI 问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/01/basic-concurrent-queue/">普通队列改造为并发队列</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/27/persistent-queue/">不可变队列的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/07/17/found-a-bug-in-netty/">追踪 Netty 异常占用堆外内存的经验分享</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/07/03/garbage-first-collector-understanding/">Garbage First Collector 理解</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2017 <a href="/." rel="nofollow">A Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>