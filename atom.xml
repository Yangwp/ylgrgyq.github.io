<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>A Blog</title>
  <subtitle>by ylgrgyq</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ylgrgyq.github.io/"/>
  <updated>2017-07-23T23:38:42.000Z</updated>
  <id>http://ylgrgyq.github.io/</id>
  
  <author>
    <name>ylgrgyq</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux 网络协议栈收消息过程-Ring Buffer</title>
    <link href="http://ylgrgyq.github.io/2017/07/23/linux-receive-packet-1/"/>
    <id>http://ylgrgyq.github.io/2017/07/23/linux-receive-packet-1/</id>
    <published>2017-07-23T08:32:40.000Z</published>
    <updated>2017-07-23T23:38:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>想看能不能完整梳理一下收消息过程。从 NIC 收数据开始，到触发软中断，交付数据包到 IP 层再经由路由机制到 TCP 层，最终交付用户进程。会尽力介绍收消息过程中的各种配置信息，以及各种监控数据。知道了收消息的完整过程，了解了各种配置，明白了各种监控数据后才有可能在今后的工作中做优化配置。</p>
<p>所有参考内容会列在这个系列最后一篇文章中。</p>
<p>Ring Buffer 相关的收消息过程大致如下：</p>
<img src="/2017/07/23/linux-receive-packet-1/ring-buffer.png" alt="图片来自参考1，对 raise softirq 的函数名做了修改，改为了 napi_schedule" title="图片来自参考1，对 raise softirq 的函数名做了修改，改为了 napi_schedule">
<p>NIC (network interface card) 在系统启动过程中会向系统注册自己的各种信息，系统会分配 Ring Buffer 队列也会分配一块专门的内核内存区域给 NIC 用于存放传输上来的数据包。struct sk_buff 是专门存放各种网络传输数据包的内存接口，在收到数据存放到 NIC 专用内核内存区域后，<a href="http://elixir.free-electrons.com/linux/v4.4/source/include/linux/skbuff.h#L706" target="_blank" rel="external">sk_buff 内有个 data 指针会指向这块内存</a>。Ring Buffer 队列内存放的是一个个 Packet Descriptor ，其有两种状态： ready 和 used 。初始时 Descriptor 是空的，指向一个空的 sk_buff，处在 ready 状态。当有数据时，<a href="https://en.wikipedia.org/wiki/DMA" target="_blank" rel="external">DMA </a> 负责从 NIC 取数据，并在 Ring Buffer 上按顺序找到下一个 ready 的 Descriptor，将数据存入该 Descriptor 指向的 sk_buff 中，并标记槽为 used。因为是按顺序找 ready 的槽，所以 Ring Buffer 是个 FIFO 的队列。</p>
<p>当 DMA 读完数据之后，NIC 会触发一个 IRQ 让 CPU 去处理收到的数据。因为每次触发 IRQ 后 CPU 都要花费时间去处理 Interrupt Handler，如果 NIC 每收到一个 Packet 都触发一个 IRQ 会导致 CPU 花费大量的时间在处理 Interrupt Handler，处理完后又只能从 Ring Buffer 中拿出一个 Packet，虽然 Interrupt Handler 执行时间很短，但这么做也非常低效，并会给 CPU 带去很多负担。所以目前都是采用一个叫做 <a href="https://wiki.linuxfoundation.org/networking/napi" target="_blank" rel="external">New API(NAPI)</a> 的机制，去对 IRQ 做合并以减少 IRQ 次数。 </p>
<p>接下来介绍一下 NAPI 是怎么做到 IRQ 合并的。它主要是让 NIC 的 driver 能注册一个 <code>poll</code> 函数，之后 NAPI 的 subsystem 能通过 <code>poll</code> 函数去从 Ring Buffer 中批量拉取收到的数据。主要事件及其顺序如下：</p>
<ol>
<li>NIC driver 初始化时向 Kernel 注册 <code>poll</code> 函数，用于后续从 Ring Buffer 拉取收到的数据</li>
<li>driver 注册开启 NAPI，这个机制默认是关闭的，只有支持 NAPI 的 driver 才会去开启</li>
<li>收到数据后 NIC 通过 DMA 将数据存到内存</li>
<li>NIC 触发一个 IRQ，并触发 CPU 开始执行 driver 注册的 Interrupt Handler</li>
<li>driver 的 Interrupt Handler 通过 <a href="http://elixir.free-electrons.com/linux/v4.4/source/include/linux/netdevice.h#L421" target="_blank" rel="external">napi_schedule</a> 函数触发 softirq (<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3204" target="_blank" rel="external">NET_RX_SOFTIRQ</a>) 来唤醒 NAPI subsystem，NET_RX_SOFTIRQ 的 handler 是 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L4861" target="_blank" rel="external">net_rx_action 会在另一个线程中被执行，在其中会调用 driver 注册的 <code>poll</code> 函数获取收到的 Packet</a></li>
<li>driver 会禁用当前 NIC 的 IRQ，从而能在 <code>poll</code> 完所有数据之前不会再有新的 IRQ</li>
<li>当所有事情做完之后，NAPI subsystem 会被禁用，并且会重新启用 NIC 的 IRQ</li>
<li>回到第三步</li>
</ol>
<p>从上面的描述可以看出来还缺一些东西，Ring Buffer 上的数据被 <code>poll</code> 走之后是怎么交付上层网络栈继续处理的呢？以及被消耗掉的 sk_buff 是怎么被重新分配重新放入 Ring Buffer 的呢？</p>
<p>这两个工作都在 <code>poll</code> 中完成，上面说过 <code>poll</code> 是个 driver 实现的函数，所以每个 driver 实现可能都不相同。但 <code>poll</code> 的工作基本是一致的就是：</p>
<ol>
<li>从 Ring Buffer 中将收到的 sk_buff 读取出来</li>
<li>对 sk_buff 做一些基本检查，可能会涉及到将几个 sk_buff 合并因为可能同一个 Frame 被分散放在多个 sk_buff 中</li>
<li>将 sk_buff 交付上层网络栈处理</li>
<li>清理 sk_buff，清理 Ring Buffer 上的 Descriptor 将其指向新分配的 sk_buff 并将状态设置为 ready</li>
<li>更新一些统计数据，比如收到了多少 packet，一共多少字节等</li>
</ol>
<p>如果拿 intel igb 这个网卡的实现来看，其 <code>poll</code> 函数在这里：<a href="http://elixir.free-electrons.com/linux/v4.4/source/drivers/net/ethernet/intel/igb/igb_main.c#L6361" target="_blank" rel="external">linux/drivers/net/ethernet/intel/igb/igb_main.c - Elixir - Free Electrons</a></p>
<p>首先是看到有 tx.ring 和 rx.ring，说明收发消息都会走到这里。发消息先不管，先看收消息，收消息走的是 <a href="http://elixir.free-electrons.com/linux/v4.4/source/drivers/net/ethernet/intel/igb/igb_main.c#L6912" target="_blank" rel="external">igb_clean_rx_irq</a>。收完消息后执行 <code>napi_complete_done</code> 退出 polling 模式，并开启 NIC 的 IRQ。从而我们知道大部分工作是在 igb_clean_rx_irq 中完成的，其实现大致上还是比较清晰的，就是上面描述的几步。里面有个 while 循环通过 buget 控制，从而在 Packet 特别多的时候不要让 CPU 在这里无穷循环下去，要让别的事情也能够被执行。循环内做的事情如下：</p>
<ol>
<li>先批量清理已经读出来的 sk_buff 并分配新的 buffer 从而避免每次读一个 sk_buff 就清理一个，很低效</li>
<li>找到 Ring Buffer 上下一个需要被读取的 Descriptor ，并检查描述符状态是否正常</li>
<li>根据 Descriptor 找到 sk_buff 读出来</li>
<li>检查是否是 End of packet，是的话说明 sk_buff 内有 Frame 的全部内容，不是的话说明 Frame 数据比 sk_buff 大，需要再读一个 sk_buff，将两个 sk_buff 数据合并起来</li>
<li>通过 Frame 的 Header 检查 Frame 数据完整性，是否正确之类的</li>
<li>记录 sk_buff 的长度，读了多少数据</li>
<li>设置 Hash、checksum、timestamp、VLAN id 等信息，这些信息是硬件提供的。</li>
<li>通过 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L4354" target="_blank" rel="external">napi_gro_receive</a> 将 sk_buff 交付上层网络栈</li>
<li>更新一堆统计数据</li>
<li>回到 1，如果没数据或者 budget 不够就退出循环</li>
</ol>
<p>看到 budget 会影响到 CPU 执行 <code>poll</code> 的时间，budget 越大当数据包特别多的时候可以提高 CPU 利用率并减少数据包的延迟。但是 CPU 时间都花在这里会影响别的任务的执行。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">budget 默认 300，可以调整</div><div class="line">sysctl -w net.core.netdev_budget=600</div></pre></td></tr></table></figure>
<p><code>napi_gro_receive</code>会涉及到 GRO 机制，稍后再说，大致上就是会对多个数据包做聚合，<code>napi_gro_receive</code> 最终是将处理好的 sk_buff 通过调用 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L4026" target="_blank" rel="external">netif_receive_skb</a>，将数据包送至上层网络栈。执行完 GRO 之后，基本可以认为数据包正式离开 Ring Buffer，进入下一个阶段了。在记录下一阶段的处理之前，补充一下收消息阶段 Ring Buffer 相关的更多细节。</p>
<h2 id="Generic-Receive-Offloading-GRO"><a href="#Generic-Receive-Offloading-GRO" class="headerlink" title="Generic Receive Offloading(GRO)"></a>Generic Receive Offloading(GRO)</h2><p>GRO 是 <a href="https://en.wikipedia.org/wiki/Large_receive_offload" target="_blank" rel="external">Large receive offload</a> 的一个实现。网络上大部分 MTU 都是 1500 字节，开启 Jumbo Frame 后能到 9000 字节，如果发送的数据超过 MTU 就需要切割成多个数据包。LRO 就是在收到多个数据包的时候将同一个 Flow 的多个数据包按照一定的规则合并起来交给上层处理，这样就能减少上层需要处理的数据包数量。</p>
<p>很多 LRO 机制是在 NIC 上实现的，没有实现 LRO 的 NIC 就少了上述合并数据包的能力。而 GRO 是 LRO 在软件上的实现，从而能让所有 NIC 都支持这个功能。</p>
<p><code>napi_gro_receive</code> 就是在收到数据包的时候合并多个数据包用的，如果收到的数据包需要被合并，<code>napi_gro_receive</code> 会很快返回。当合并完成后会调用 <code>napi_skb_finish</code> ，将因为数据包合并而不再用到的数据结构释放。最终会调用到 <code>netif_receive_skb</code> 将数据包交到上层网络栈继续处理。<code>netif_receive_skb</code> 上面说过，就是数据包从 Ring Buffer 出来后到上层网络栈的入口。</p>
<p>可以通过 ethtool 查看和设置 GRO：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">查看 GRO</div><div class="line">ethtool -k eth0 | grep generic-receive-offload</div><div class="line">generic-receive-offload: on</div><div class="line">设置开启 GRO</div><div class="line">ethtool -K eth0 gro on</div></pre></td></tr></table></figure>
<h2 id="多-CPU-下的-Ring-Buffer-处理-Receive-Side-Scaling"><a href="#多-CPU-下的-Ring-Buffer-处理-Receive-Side-Scaling" class="headerlink" title="多 CPU 下的 Ring Buffer 处理 (Receive Side Scaling)"></a>多 CPU 下的 Ring Buffer 处理 (Receive Side Scaling)</h2><p>NIC 收到数据的时候产生的 IRQ 只可能被一个 CPU 处理，从而只有一个 CPU 会执行 napi_schedule 来触发 softirq，触发的这个 softirq 的 handler 也还是会在这个产生 softIRQ 的 CPU 上执行。所以 driver 的 <code>poll</code> 函数也是在最开始处理 NIC 发出 IRQ 的那个 CPU 上执行。于是一个 Ring Buffer 上同一个时刻只有一个 CPU 在拉取数据。</p>
<p>从上面描述能看出来分配给 Ring Buffer 的空间是有限的，当收到的数据包速率大于单个 CPU 处理速度的时候 Ring Buffer 可能被占满，占满之后再来的新数据包会被自动丢弃。而现在机器都是有多个 CPU，同时只有一个 CPU 去处理 Ring Buffer 数据会很低效，这个时候就产生了叫做 Receive Side Scaling(RSS) 或者叫做 multiqueue 的机制来处理这个问题。WIKI 对 RSS 的介绍挺好的，简洁干练可以看看: <a href="https://en.wikipedia.org/wiki/Network_interface_controller#RSS" target="_blank" rel="external">Network interface controller - Wikipedia</a></p>
<p>简单说就是现在支持 RSS 的网卡内部会有多个 Ring Buffer，NIC 收到 Frame 的时候能通过 Hash Function 来决定 Frame 该放在哪个 Ring Buffer 上，触发的 IRQ 也可以通过操作系统或者手动配置 IRQ affinity 将 IRQ 分配到多个 CPU 上。这样 IRQ 能被不同的 CPU 处理，从而做到 Ring Buffer 上的数据也能被不同的 CPU 处理，从而提高数据的并行处理能力。</p>
<p>RSS 除了会影响到 NIC 将 IRQ 发到哪个 CPU 之外，不会影响别的逻辑了。收消息过程跟之前描述的是一样的。</p>
<p>如果支持 RSS 的话，NIC 会为每个队列分配一个 IRQ，通过 <code>/proc/interrupts</code> 能进行查看。你可以通过配置 IRQ affinity 指定 IRQ 由哪个 CPU 来处理中断。先通过 <code>/proc/interrupts</code> 找到 IRQ 号之后，将希望绑定的 CPU 号写入 <code>/proc/irq/IRQ_NUMBER/smp_affinity</code>，写入的是 16 进制的 bit mask。比如看到队列 rx_0 对应的中断号是 41 那就执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">echo 6 &gt; /proc/irq/41/smp_affinity</div><div class="line">6 表示的是 CPU2 和 CPU1</div></pre></td></tr></table></figure>
<p>0 号 CPU 的掩码是 0x1 (0001)，1 号 CPU 掩码是 0x2 (0010)，2 号 CPU 掩码是 0x4 (0100)，3 号 CPU 掩码是 0x8 (1000) 依此类推。</p>
<p>另外需要注意的是设置 smp_affinity 的话不能开启 irqbalance 或者需要为 irqbalance 设置 –banirq 列表，将设置了 smp_affinity 的 IRQ 排除。不然 irqbalance 机制运作时会忽略你设置的 IRQ affinity 配置。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L99-L222" target="_blank" rel="external">Receive Packet Steering(RPS)</a> 是在 NIC 不支持 RSS 时候在软件中实现 RSS 类似功能的机制。其好处就是对 NIC 没有要求，任何 NIC 都能支持 RPS，但缺点是 NIC 收到数据后 DMA 将数据存入的还是一个 Ring Buffer，NIC 触发 IRQ 还是发到一个 CPU，还是由这一个 CPU 调用 driver 的 <code>poll</code> 来将 Ring Buffer 的数据取出来。RPS 是在单个 CPU 将数据从 Ring Buffer 取出来之后才开始起作用，它会为每个 Packet 计算 Hash 之后将 Packet 发到对应 CPU 的 backlog 中，并通过 Inter-processor Interrupt(IPI) 告知目标 CPU 来处理 backlog。后续 Packet 的处理流程就由这个目标 CPU 来完成。从而实现将负载分到多个 CPU 的目的。</p>
<p>RPS 默认是关闭的，当机器有多个 CPU 并且通过 softirqs 的统计 <code>/proc/softirqs</code> 发现 NET_RX 在 CPU 上分布不均匀或者发现网卡不支持 mutiqueue 时，就可以考虑开启  RPS。开启 RPS 需要调整 <code>/sys/class/net/DEVICE_NAME/queues/QUEUE/rps_cpus</code> 的值。比如执行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo f &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus</div></pre></td></tr></table></figure>
<p>表示的含义是处理网卡 eth0 的 rx-0 队列的 CPU 数设置为 f 。即设置有 15 个 CPU 来处理 rx-0 这个队列的数据，如果你的 CPU 数没有这么多就会默认使用所有 CPU 。甚至有人为了方便都是直接将 <code>echo fff &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus</code> 写到脚本里，这样基本能覆盖所有类型的机器，不管机器 CPU 数有多少，都能覆盖到。从而就能让这个脚本在任意机器都能执行。</p>
<p><strong>注意：</strong>如果 NIC 不支持 mutiqueue，RPS 不是完全不用思考就能打开的，因为其开启之后会加重所有 CPU 的负担，在一些场景下比如 CPU 密集型应用上并不一定能带来好处。所以得测试一下。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L225" target="_blank" rel="external">Receive Flow Steering(RFS)</a> 一般和 RPS 配合一起工作。RPS 是将收到的 packet 发配到不同的 CPU 以实现负载均衡，但是可能同一个 Flow 的数据包正在被 CPU1 处理，但下一个数据包被发到 CPU2，会降低 CPU cache hit 比率并且会让数据包要从 CPU1 发到 CPU2 上。RFS 就是保证同一个 flow 的 packet 都会被路由到正在处理当前 Flow 数据的 CPU，从而提高 CPU cache 比率。<a href="https://github.com/torvalds/linux/blob/master/Documentation/networking/scaling.txt" target="_blank" rel="external">这篇文章</a> 把 RFS 机制介绍的挺好的。基本上就是收到数据后根据数据的一些信息做个 Hash 在这个 table 的 entry 中找到当前正在处理这个 flow 的 CPU 信息，从而将数据发给这个正在处理该 Flow 数据的 CPU 上，从而做到提高 CPU cache hit 率，避免数据在不同 CPU 之间拷贝。当然还有很多细节，请看上面链接。</p>
<p>RFS 默认是关闭的，必须主动配置才能生效。正常来说开启了 RPS 都要再开启 RFS，以获取更好的性能。<a href="https://github.com/torvalds/linux/blob/master/Documentation/networking/scaling.txt" target="_blank" rel="external">这篇文章</a>也有说该怎么去开启 RFS 以及推荐的配置值。一个是要配置 rps_sock_flow_entries</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sysctl -w net.core.rps_sock_flow_entries=32768</div></pre></td></tr></table></figure>
<p>这个值依赖于系统期望的活跃连接数，注意是同一时间活跃的连接数，这个连接数正常来说会大大小于系统能承载的最大连接数，因为大部分连接不会同时活跃。该值建议是 32768，能覆盖大多数情况，每个活跃连接会分配一个 entry。除了这个之外还要配置 rps_flow_cnt：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo 2048 &gt; /sys/class/net/eth0/queues/rx-0/rps_flow_cnt</div></pre></td></tr></table></figure>
<p>配置的是每个队列分配的 entry 数量。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L324" target="_blank" rel="external">Accelerated Receive Flow Steering (aRFS)</a> 类似 RFS 只是由硬件协助完成这个工作。aRFS 对于 RFS 就和 RSS 对于 RPS 一样，就是把 CPU 的工作挪到了硬件来做，从而不用浪费 CPU 时间，直接由 NIC 完成 Hash 值计算并将数据发到目标 CPU，所以快一点。NIC 必须暴露出来一个 <code>ndo_rx_flow_steer</code> 的函数用来实现 aRFS。</p>
<h2 id="adaptive-RX-TX-IRQ-coalescing"><a href="#adaptive-RX-TX-IRQ-coalescing" class="headerlink" title="adaptive RX/TX IRQ coalescing"></a>adaptive RX/TX IRQ coalescing</h2><p>有的 NIC 支持这个功能，用来动态的将 IRQ 进行合并，以做到在数据包少的时候减少数据包的延迟，在数据包多的时候提高吞吐量。查看方法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ethtool -c eth1</div><div class="line">Coalesce parameters for eth1:</div><div class="line">Adaptive RX: off  TX: off</div><div class="line">stats-block-usecs: 0</div><div class="line">.....</div></pre></td></tr></table></figure>
<p>开启 RX 队列的 adaptive coalescing 执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -C eth0 adaptive-rx on</div></pre></td></tr></table></figure>
<p>并且有四个值需要设置：rx-usecs、rx-frames、rx-usecs-irq、rx-frames-irq，具体含义等需要用到的时候查吧。</p>
<h2 id="Ring-Buffer-相关监控及配置"><a href="#Ring-Buffer-相关监控及配置" class="headerlink" title="Ring Buffer 相关监控及配置"></a>Ring Buffer 相关监控及配置</h2><h3 id="收到数据包统计"><a href="#收到数据包统计" class="headerlink" title="收到数据包统计"></a>收到数据包统计</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">ethtool -S eh0</div><div class="line">NIC statistics:</div><div class="line">     rx_packets: 792819304215</div><div class="line">     tx_packets: 778772164692</div><div class="line">     rx_bytes: 172322607593396</div><div class="line">     tx_bytes: 201132602650411</div><div class="line">     rx_broadcast: 15118616</div><div class="line">     tx_broadcast: 2755615</div><div class="line">     rx_multicast: 0</div><div class="line">     tx_multicast: 10</div></pre></td></tr></table></figure>
<p>RX 就是收到数据，TX 是发出数据。还会展示 NIC 每个队列收发消息情况。<em>其中比较关键的是带有 drop 字样的统计和 fifo_errors 的统计</em> :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">tx_dropped: 0</div><div class="line">rx_queue_0_drops: 93</div><div class="line">rx_queue_1_drops: 874</div><div class="line">....</div><div class="line">rx_fifo_errors: 2142</div><div class="line">tx_fifo_errors: 0</div></pre></td></tr></table></figure>
<p>看到发送队列和接收队列 drop 的数据包数量显示在这里。并且所有 queue_drops 加起来等于 rx_fifo_errors。所以总体上能通过 rx_fifo_errors 看到 Ring Buffer 上是否有丢包。如果有的话一方面是看是否需要调整一下每个队列数据的分配，或者是否要加大 Ring Buffer 的大小。</p>
<p><code>/proc/net/dev</code>是另一个数据包相关统计：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cat /proc/net/dev</div><div class="line">Inter-|   Receive                                                |  Transmit</div><div class="line"> face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed</div><div class="line">    lo: 14472296365706 10519818839    0    0    0     0          0         0 14472296365706 10519818839    0    0    0     0       0          0</div><div class="line">  eth1: 164650683906345 785024598362    0    0 2142     0          0         0 183711288087530 704887351967    0    0    0     0       0          0</div></pre></td></tr></table></figure>
<h4 id="调整-Ring-Buffer-队列数量"><a href="#调整-Ring-Buffer-队列数量" class="headerlink" title="调整 Ring Buffer 队列数量"></a>调整 Ring Buffer 队列数量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ethtool -l eth0</div><div class="line">Channel parameters for eth0:</div><div class="line">Pre-set maximums:</div><div class="line">RX:             0</div><div class="line">TX:             0</div><div class="line">Other:          1</div><div class="line">Combined:       8</div><div class="line">Current hardware settings:</div><div class="line">RX:             0</div><div class="line">TX:             0</div><div class="line">Other:          1</div><div class="line">Combined:       8</div></pre></td></tr></table></figure>
<p>看的是 Combined 这一栏是队列数量。Combined 按说明写的是多功能队列，猜想是能用作 RX 队列也能当做 TX 队列，但数量一共是 8 个？</p>
<p>如果不支持 mutiqueue 的话上面执行下来会是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Channel parameters for eth0:</div><div class="line">Cannot get device channel parameters</div><div class="line">: Operation not supported</div></pre></td></tr></table></figure>
<p>能自己设置 Ring Buffer 数量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ethtool -L eth0 combined 8</div></pre></td></tr></table></figure>
<p>如果支持对特定类型 RX 或 TX 设置队列数量的话可以执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ethtool -L eth0 rx 8</div></pre></td></tr></table></figure>
<p><strong>需要注意的是</strong>，ethtool 的设置操作可能都要重启一下才能生效。</p>
<h4 id="调整-Ring-Buffer-队列大小"><a href="#调整-Ring-Buffer-队列大小" class="headerlink" title="调整 Ring Buffer 队列大小"></a>调整 Ring Buffer 队列大小</h4><p>先查看当前 Ring Buffer 大小：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ethtool -g eth0</div><div class="line">Ring parameters for eth0:</div><div class="line">Pre-set maximums:</div><div class="line">RX:   4096</div><div class="line">RX Mini:  0</div><div class="line">RX Jumbo: 0</div><div class="line">TX:   4096</div><div class="line">Current hardware settings:</div><div class="line">RX:   512</div><div class="line">RX Mini:  0</div><div class="line">RX Jumbo: 0</div><div class="line">TX:   512</div></pre></td></tr></table></figure>
<p>看到 RX 和 TX 最大是 4096，当前值为 512。<em>队列越大丢包的可能越小，但数据延迟会增加</em></p>
<p>设置 RX 队列大小：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -G eth0 rx 4096</div></pre></td></tr></table></figure>
<h4 id="调整-Ring-Buffer-队列的权重"><a href="#调整-Ring-Buffer-队列的权重" class="headerlink" title="调整 Ring Buffer 队列的权重"></a>调整 Ring Buffer 队列的权重</h4><p>NIC 如果支持 mutiqueue 的话 NIC 会根据一个 Hash 函数对收到的数据包进行分发。能调整不同队列的权重，用于分配数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">ethtool -x eth0</div><div class="line">RX flow hash indirection table for eth0 with 8 RX ring(s):</div><div class="line">    0:      0     0     0     0     0     0     0     0</div><div class="line">    8:      0     0     0     0     0     0     0     0</div><div class="line">   16:      1     1     1     1     1     1     1     1</div><div class="line">   ......</div><div class="line">   64:      4     4     4     4     4     4     4     4</div><div class="line">   72:      4     4     4     4     4     4     4     4</div><div class="line">   80:      5     5     5     5     5     5     5     5</div><div class="line">   ......</div><div class="line">  120:      7     7     7     7     7     7     7     7</div></pre></td></tr></table></figure>
<p>我的 NIC 一共有 8 个队列，一个有 128 个不同的 Hash 值，上面就是列出了每个 Hash 值对应的队列是什么。最左侧 0 8 16 是为了能让你快速的找到某个具体的 Hash 值。比如 Hash 值是 76 的话我们能立即找到 72 那一行：”72:      4     4     4     4     4     4     4     4”，从左到右第一个是 72 数第 5 个就是 76 这个 Hash 值对应的队列是 4 。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -X eth0 weight 6 2 8 5 10 7 1 5</div></pre></td></tr></table></figure>
<p>设置 8 个队列的权重。加起来不能超过 128 。128 是 indirection table 大小，每个 NIC 可能不一样。</p>
<h4 id="更改-Ring-Buffer-Hash-Field"><a href="#更改-Ring-Buffer-Hash-Field" class="headerlink" title="更改 Ring Buffer Hash Field"></a>更改 Ring Buffer Hash Field</h4><p>分配数据包的时候是按照数据包内的某个字段来进行的，这个字段能进行调整。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ethtool -n eth0 rx-flow-hash tcp4</div><div class="line">TCP over IPV4 flows use these fields for computing Hash flow key:</div><div class="line">IP SA</div><div class="line">IP DA</div><div class="line">L4 bytes 0 &amp; 1 [TCP/UDP src port]</div><div class="line">L4 bytes 2 &amp; 3 [TCP/UDP dst port]</div></pre></td></tr></table></figure>
<p>查看 tcp4 的 Hash 字段。</p>
<p>也可以设置 Hash 字段：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -N eth0 rx-flow-hash udp4 sdfn</div></pre></td></tr></table></figure>
<p>sdfn 需要查看 ethtool 看其含义。</p>
<h4 id="softirq-数统计"><a href="#softirq-数统计" class="headerlink" title="softirq 数统计"></a>softirq 数统计</h4><p>通过 <code>/proc/softirqs</code> 能看到每个 CPU 上 softirq 数量统计：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">cat /proc/softirqs</div><div class="line">                    CPU0       CPU1       </div><div class="line">          HI:          1          0</div><div class="line">       TIMER: 1650579324 3521734270</div><div class="line">      NET_TX:   10282064   10655064</div><div class="line">      NET_RX: 3618725935       2446</div><div class="line">       BLOCK:          0          0</div><div class="line">BLOCK_IOPOLL:          0          0</div><div class="line">     TASKLET:      47013      41496</div><div class="line">       SCHED: 1706483540 1003457088</div><div class="line">     HRTIMER:    1698047   11604871</div><div class="line">         RCU: 4218377992 3049934909</div></pre></td></tr></table></figure>
<p>看到 NET_RX 就是收消息时候触发的 softirq，一般看这个统计是为了看看 softirq 在每个 CPU 上分布是否均匀，不均匀的话可能就需要做一些调整。比如上面看到 CPU0 和 CPU1 两个差距很大，原因是这个机器的 NIC 不支持 RSS，没有多个 Ring Buffer。开启 RPS 后就均匀多了。</p>
<h4 id="IRQ-统计"><a href="#IRQ-统计" class="headerlink" title="IRQ 统计"></a>IRQ 统计</h4><p><code>/proc/interrupts</code> 能看到每个 CPU 的 IRQ 统计。可能就是看看 NAPI 的 IRQ 合并机制是否生效。看看 IRQ 是不是增长的很快。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;想看能不能完整梳理一下收消息过程。从 NIC 收数据开始，到触发软中断，交付数据包到 IP 层再经由路由机制到 TCP 层，最终交付用户进程。会尽力介绍收消息过程中的各种配置信息，以及各种监控数据。知道了收消息的完整过程，了解了各种配置，明白了各种监控数据后才有可能在今后的
    
    </summary>
    
    
      <category term="Network" scheme="http://ylgrgyq.github.io/tags/Network/"/>
    
  </entry>
  
  <entry>
    <title>实时通信系统并发连接数测试时需要调整的各种参数</title>
    <link href="http://ylgrgyq.github.io/2017/07/02/RTM-max-connections/"/>
    <id>http://ylgrgyq.github.io/2017/07/02/RTM-max-connections/</id>
    <published>2017-07-01T23:26:59.000Z</published>
    <updated>2017-07-02T01:36:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>维持大量并发连接是实时通信系统的关键能力之一，而要想测出一台服务器到底能支撑多少连接有时候会比较麻烦，需要涉及到好几个系统参数的调整，在这里希望能将遇到过的各种参数调整记录一下，以备后用。</p>
<p>以下所说连接均指 TCP 连接。</p>
<h2 id="客户端连接数限制"><a href="#客户端连接数限制" class="headerlink" title="客户端连接数限制"></a>客户端连接数限制</h2><p>首先需要明确一点是单个压测客户端（单个 IP）能承载的并发连接数是有限制的，这个上限是 65535。也就是说无论压测客户端所在机器性能有多强大，单个 IP 能和服务端建立的并发连接数就只有 65535 个，不可能更多。而从服务端角度来看，服务端能承受的并发连接数又远远不止 65535 个，只要服务器内存足够，CPU 足够强大，单机承载几十上百万的并发连接完全不是问题，所以我们经常能听到评价某些实时通信服务时候说单机能承担百万并发连接等。为什么从客户端和从服务端两个角度会得到不同的限制呢？</p>
<p>从网络协议层面看，需要有四个信息能唯一确定一条连接：Source IP + Source Port + Destination IP + Destination Port，客户端在创建 Socket 连接服务端的时候服务端的 Destination IP + Destination Port 是固定不变的，对客户端来说如果只有一个 IP 那 Source IP 也固定不变，能唯一确定一条连接的信息中只有 Source Port 可变，所以对客户端来说单个 IP 能创建的连接数完全取决于 Source Port 数量。而反观服务端，Destination IP 和 Destination Port 是固定的，但能连上服务端的客户端 Source IP 和 Source Port 都是可变的，所以服务端连接数量最多是 Source IP 乘以 Source Port 数量。</p>
<p>为了得到 Source Port 的限制数量我们可以看看 TCP Header 格式：</p>
<img src="/2017/07/02/RTM-max-connections/tcp-header.png" alt="图片来自：https://tools.ietf.org/html/rfc793" title="图片来自：https://tools.ietf.org/html/rfc793">
<p>从上图能看到 Source Port 和  Destination Port 都只有 16 个 bit，也就是说上限都是 65535 个。</p>
<p>如果使用的是 IPv4，IP 协议报文的 Header 如下：</p>
<img src="/2017/07/02/RTM-max-connections/ipv4-header.png" alt="图片来自：https://tools.ietf.org/html/rfc791" title="图片来自：https://tools.ietf.org/html/rfc791">
<p>看到 Source Address 和 Destination Address 都是 32 bit，这支持的 IP 数量就多了去了在几十亿的级别。</p>
<p>所以，客户端单个 IP 能创建的连接上限是 65535 个，服务端单个 IP 能创建的连接数量按目前一般机器性能来看可以认为是无限。如果压测客户端性能强大，想单个客户端上与服务端创建超过 65535 个连接，唯一的方法就是扩展客户端 IP 数量，比如使用<a href="https://en.wikipedia.org/wiki/Virtual_IP_address" target="_blank" rel="external">虚拟 IP</a>。</p>
<h3 id="报错信息"><a href="#报错信息" class="headerlink" title="报错信息"></a>报错信息</h3><p>当因为 IP 端口不足而无法建立连接时，Java 的话会报如下错误：<br>Cannot assign requested address (Address not available)</p>
<h2 id="Open-File-Descriptors-限制"><a href="#Open-File-Descriptors-限制" class="headerlink" title="Open File Descriptors 限制"></a>Open File Descriptors 限制</h2><p>ulimit 命令用于展示和设置用户进程对一些系统关键资源使用的限制。因为每一个建立的连接在 Linux 上都可视为一个打开的文件，会占用一个 File Descriptor，所以 ulimit 内各种限制中跟并发连接数最相关的是进程最大能打开的 File Descriptor 数量。这个限制经常有人和单个 IP 端口数限制搞混，认为客户端单个 IP 能建立的连接数上限为 65535 是因为 File Descriptor 限制，只要机器内存够大，将 File Descriptor 改的够大，客户端单个 IP 能建立的连接数上限就能提升，这个是不正确的。</p>
<h3 id="报错信息-1"><a href="#报错信息-1" class="headerlink" title="报错信息"></a>报错信息</h3><p>当因为 Open File Descriptors 数量不足受限时会报错：<br><code>Too many open files</code></p>
<h3 id="修改方法"><a href="#修改方法" class="headerlink" title="修改方法"></a>修改方法</h3><p>Linux 下，临时修改的话直接执行 ulimit -n XXXXX 即可，但这个执行完后只对当前 Shell session 有效，再次登录后会恢复原状，如果想永久有效需要修改 /etc/security/limits.conf 文件，打开该文件找到 nofile 就是 File Descriptor 限制，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soft  nofile  1000000</div><div class="line">hard  nofile  1000000</div></pre></td></tr></table></figure>
<p>普通用户默认使用的是 soft 限制，并且能够通过 ulimit -n 修改 soft 限制到最大跟 hard 一样，超过 hard 的话会报错：<br><code>ulimit: open files: cannot modify limit: Operation not permitted</code></p>
<p>但普通用户只能<em>不可逆的</em>降低 hard 值，不能将 hard 值提高，只有 root 用户能将 hard 限制提高。目前没有看到修改 hard 值的命令，修改方法应该是只有通过系统调用 setrlimit，具体使用可以用 man 3 看看。</p>
<p>Mac 下修改看上去好麻烦，我没有用过本地机器做过量特别大的压测，所以没有实际做过调整，留下一些线索供以后查阅：<br><a href="https://superuser.com/questions/302754/increase-the-maximum-number-of-open-file-descriptors-in-snow-leopard" target="_blank" rel="external">mac - Increase the maximum number of open file descriptors in Snow Leopard? - Super User</a><br><a href="https://www.chrissearle.org/2016/10/01/too-many-open-files-on-osx-macos/" target="_blank" rel="external">https://www.chrissearle.org/2016/10/01/too-many-open-files-on-osx-macos/</a></p>
<h2 id="自动分配本地端口范围"><a href="#自动分配本地端口范围" class="headerlink" title="自动分配本地端口范围"></a>自动分配本地端口范围</h2><p>确定一个连接需要四个元素 Source IP + Source Port + Destination IP + Destination Port，一般客户端在连服务端的时候只要获取到服务端的 Destination IP 和 Destination Port 即可，Source IP 是客户端自己的 IP，客户端系统会自动分配一个 Source Port 来建立连接。而这个 Source Port 的选择范围是可通过 sysctl net.ipv4.ip_local_port_range 参数来定制的。可以执行一下这个命令来获取当前系统的设定，例如：</p>
<p><code>net.ipv4.ip_local_port_range = 49152    65535</code></p>
<p>即表示在与 remote 服务建立连接时，系统只能自动从 49152 至 65535 中选择一个作为 Local Port，也就是 Source Port。</p>
<p>如果希望压测客户端和服务器建立大量的连接，则需要将该范围设置的大一些，给客户端留足端口数。如果留的端口不足的话会报错。</p>
<p>为什么这个范围系统不能默认就设置的非常宽呢？比如默认就是 0 ~ 65535 ？</p>
<p>一个是因为 0 ~ 1023 是系统预留的 Well-Known Ports，普通进程就不可使用。另一个是因为当系统作为服务器使用的时候，经常需要开启一些端口比如 MySQL 会默认开启 3306 端口。如果将 net.ipv4.ip_local_port_range 范围设置的很广，比如 2000 ~ 65535 那就可能会出现在 MySQL 启动的时候 3306 端口已经被系统自动分配给了一个进程作为 Source Port 去连接另一个服务，从而导致 MySQL 因为无法绑定 3306 端口而无法启动。所以这个 net.ipv4.ip_local_port_range 需要根据你系统提供的服务情况来酌情进行调整，在写服务端程序时候开启的端口也不是能随便定的，一定要小于系统的 net.ipv4.ip_local_port_range 才行，不然就可能出现系统启动的时候待绑定端口已经被别的连接占用。</p>
<h3 id="报错信息-2"><a href="#报错信息-2" class="headerlink" title="报错信息"></a>报错信息</h3><p>当因为本地端口范围限制，无法分配出空闲端口时会报错：<br><code>Cannot assign requested address</code><br>这个错误信息跟端口不足时报的一样。</p>
<h3 id="修改方法-1"><a href="#修改方法-1" class="headerlink" title="修改方法"></a>修改方法</h3><p>Linux 系统下，执行命令：</p>
<p><code>sysctl -w net.ipv4.ip_local_port_range=&quot;15000 61000&quot;</code></p>
<h2 id="端口复用"><a href="#端口复用" class="headerlink" title="端口复用"></a>端口复用</h2><p>TCP 连接断开之后主动发起 FIN 的一方最终会进入 TIME_WAIT 状态，处在这个状态时连接之前所占用的端口不能被下一个新的连接使用，必须等待一段时间之后才能使用。如果是单独测试并发连接峰值，减少 TIME_WAIT 连接数可能用处不大，但如果是连续的测试，每次关闭客户端准备再来下一轮测试时必须等足 TIME_WAIT 时间，如果 TIME_WAIT 时间比较长就比较烦，所以减少 TIME WAIT 对测试有一定好处。因为一般压测都是内网，所以 TIME WAIT 清理方面能稍微激进一些。TIME WAIT 相关内容可以参看：<a href="https://ylgrgyq.github.io/2017/06/30/tcp-time-wait/">TCP TIME-WAIT</a>。可以考虑：</p>
<ol>
<li>Client 开启 TCP Timestamps 后开启 net.ipv4.tcp_tw_reuse 或 net.ipv4.tcp_tw_recycle；</li>
<li>将 net.ipv4.tcp_max_tw_buckets 设置的很小，TIME WAIT 连接超过该值后直接清理。因为一般测试都在内网，没有 NAP 的情况下 Per-Host 的 Timestamp 配合 PAWS 一般能消除跨连接数据包错误到达问题;</li>
<li>考虑压测结束的时候由 Client 主动断开连接，并且设置 SO_LINGER 为 0，断开连接时候直接发 RST；</li>
</ol>
<h3 id="修改方法-2"><a href="#修改方法-2" class="headerlink" title="修改方法"></a>修改方法</h3><p>sysctl -w net.ipv4.tcp_timestamps=1<br>sysctl -w net.ipv4.tcp_tw_reuse=1<br>sysctl -w net.ipv4.tcp_tw_recycle=1<br>sysctl -w net.ipv4.tcp_max_tw_buckets=10000</p>
<h2 id="TCP-Backlog"><a href="#TCP-Backlog" class="headerlink" title="TCP Backlog"></a>TCP Backlog</h2><p>TCP Backlog 的相关信息可以参考，<a href="https://ylgrgyq.github.io/2017/05/18/tcp-backlog/">TCP Backlog</a>。实时通信服务也属于高并发服务，在搞活动、服务重启等时候可能出现大量用户同时和服务器创建连接的情况，所以也需要酌情调整 TCP Backlog 的大小。</p>
<h3 id="修改方法-3"><a href="#修改方法-3" class="headerlink" title="修改方法"></a>修改方法</h3><p>临时修改执行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sysctl -w net.core.somaxconn=2048</div><div class="line">sysctl -w net.ipv4.tcp_syncookies=1</div><div class="line">sysctl -w net.ipv4.tcp_max_syn_backlog=4096</div></pre></td></tr></table></figure>
<p>永久性修改需要修改 /etc/sysctl.conf 文件，将上面修改值写在文件中。</p>
<p>以后遇到更多需要调整的参数后再继续记录吧。 To be continue…</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;维持大量并发连接是实时通信系统的关键能力之一，而要想测出一台服务器到底能支撑多少连接有时候会比较麻烦，需要涉及到好几个系统参数的调整，在这里希望能将遇到过的各种参数调整记录一下，以备后用。&lt;/p&gt;
&lt;p&gt;以下所说连接均指 TCP 连接。&lt;/p&gt;
&lt;h2 id=&quot;客户端连接数
    
    </summary>
    
    
      <category term="TCP" scheme="http://ylgrgyq.github.io/tags/TCP/"/>
    
  </entry>
  
  <entry>
    <title>TCP TIME-WAIT</title>
    <link href="http://ylgrgyq.github.io/2017/06/30/tcp-time-wait/"/>
    <id>http://ylgrgyq.github.io/2017/06/30/tcp-time-wait/</id>
    <published>2017-06-29T22:56:06.000Z</published>
    <updated>2017-07-04T02:01:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>TIME-WAIT 是 TCP 挥手过程的一个状态。很多地方都对它有说明，这里只贴两个图唤起记忆。下面是 TCP 完整的状态图：</p>
<img src="/2017/06/30/tcp-time-wait/TCP-State.png" alt="来自：http://www.tcpipguide.com/free/t_TCPOperationalOverviewandtheTCPFiniteStateMachineF-2.htm" title="来自：http://www.tcpipguide.com/free/t_TCPOperationalOverviewandtheTCPFiniteStateMachineF-2.htm">
<p>看到最下面有个 TIME-WAIT 状态。状态图可能看着不那么直观，可以看这个：</p>
<img src="/2017/06/30/tcp-time-wait/TCP-Fin.png" alt="来自：http://www.tcpipguide.com/free/t_TCPConnectionTermination-2.htm" title="来自：http://www.tcpipguide.com/free/t_TCPConnectionTermination-2.htm">
<p>上面实际不一定是只有 Client 才能进入 TIME-WAIT 状态，而是谁发起 TCP 连接断开先发的 FIN，谁最终就进入 TIME-WAIT 状态。</p>
<h2 id="TIME-WAIT-的作用"><a href="#TIME-WAIT-的作用" class="headerlink" title="TIME-WAIT 的作用"></a>TIME-WAIT 的作用</h2><p>第一个作用是避免上一个连接延迟到达的数据包被下一个连接错误接收。如下图所示：</p>
<img src="/2017/06/30/tcp-time-wait/TIME-WAIT-Function.png" alt="来自：https://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux" title="来自：https://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux">
<p>虚线将两次连接分开，两次连接都使用的同一组 TCP Tuple，即 Source IP, Source Port, Destination IP, Destination Port 组合。第一次连接中 SEQ 为 3 的数据包出现了重发，第二次连接中刚好再次使用 SEQ 为 3 这个序号的时候，第一次连接中本来发丢(延迟)的 SEQ 为 3 的数据包在此时到达，就导致这个延迟了的 SEQ 为 3 的数据包被当做正确的数据而接收，之后如果还有 SEQ 为 3 的正常数据包到达会被接收方认为是重复数据包而直接丢弃，导致 TCP 连接接收的数据错误。</p>
<p>这种错误可能看上去很难发生，因为必须 TCP Tuple 一致，并且 SEQ 号<em>必须 valid </em>时才会发生，但在高速网络下发生的可能性会增大(因为 SEQ 号会很快被耗尽而出现折叠重用)，所以需要有个 TIME-WAIT 的存在减少上面这种情况的发生，并且 TIME-WAIT 的长度还要长一些， <a href="https://tools.ietf.org/html/rfc793" target="_blank" rel="external">RFC 793 - Transmission Control Protocol</a> 要求 TIME-WAIT 长度要大于两个 MSL (<a href="https://en.wikipedia.org/wiki/Maximum_segment_lifetime" target="_blank" rel="external">Maximum segment lifetime - Wikipedia</a>) 。MSL 是人为定下的值，就认为数据包在网络路由的时间不会超过这么长。等足两个 MSL 以保证上图第二次连接建立的时候之前发丢的 SEQ 为 3 的数据包已经在网络中丢失，不可能再出现在第二次连接中。</p>
<p>另一个作用是被动断开连接的一方，发出最后一个 FIN 之后进入 LAST ACK 状态。主动断开连接的一方在收到 FIN 之后回复 ACK，如果该 ACK 发丢了，被动断开连接的一方会一直处在 LAST ACK 状态，并在超时之后重发 FIN。主动断开连接的一方如果处在 TIME WAIT 状态，重复收到 FIN 后每次会重发 ACK。但如果 TIME WAIT 时间短，会进入 CLOSED 状态，此时再收到 FIN 就直接回复 RST 了。这个 RST 会导致被动断开连接的一方有个错误提示，虽然所有数据实际已经成功发到对端。</p>
<p>这里有个疑问，如果连接断开后 TIME WAIT 时间很短，TIME WAIT 结束之后主动断开连接一方直接发出 SYN 而被动断开连接一方还处在 LAST ACK 状态，因为 SYN 是其不期待的数据会不会触发其回复 RST 导致主动断开连接一方 connect 失败而放弃建立连接？<a href="https://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux" target="_blank" rel="external">这篇文章</a> 说如果开启 TCP Timestamp 后处在 LAST ACK 一方会丢弃 SYN 不会回复 RST 而是等到 FIN 超时后重发 FIN，从而让主动断开连接一方回复 RST 后再次发起 SYN 最终能保证连接正常建立。但依据是哪里？在下面介绍 tcp_tw_recycle 的地方会对 Linux 内收到数据包的过程做一下梳理，但从目前来看如果连接处在 LAST ACK，收到 SYN 后如果数据包没有损坏，SEQ 号也符合要求等等各种检查都能通过，则连接什么都不会做，相当于忽略了这个 SYN，也就是说不管 TCP TImestamp 是否开启，处在 LAST ACK 时收到 SYN 都不做任何事情。</p>
<h2 id="TIME-WAIT-带来的问题"><a href="#TIME-WAIT-带来的问题" class="headerlink" title="TIME WAIT 带来的问题"></a>TIME WAIT 带来的问题</h2><p>先引用一个名言：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">The TIME_WAIT state is our friend and is there to help us (i.e., to let old duplicate segments expire in the network). Instead of trying to avoid the state, we should understand it.</div></pre></td></tr></table></figure></p>
<p>据说这话的是 W. Richard Stevens 说的。也就是说 TIME WAIT 可能会给我们带来一些问题，但我们还是不要把它当成敌人，当成一个异常状态想方设法的去破坏它的正常工作，而是去利用它，理解它，让它为我们所用。这里要说 TIME WAIT 的问题只是需要我们去理解它的副作用，不是说 TIME WAIT 真的就很邪恶很讨厌。</p>
<p>TIME WAIT 带来的问题主要是三个：</p>
<ol>
<li>端口占用，导致新的连接可能没有可用的端口;</li>
<li>TIME WAIT 状态的连接依然会占用系统内存;</li>
<li>会带来一些 CPU 开销</li>
</ol>
<p>对于第一个问题可以考虑开启下面说的 tcp_tw_reuse 甚至 tcp_tw_recycle，也能调大 net.ipv4.ip_local_port_range 以获取更多的可用端口，还可以使用多个 IP 或 Server 开启多个 port 的方法来避免没有端口可用的情况。</p>
<p>对于内存上的开销，进入 TIME WAIT 后应用层实际已经将连接相关信息销毁了，只是在 kernel 还维护有连接相关信息，所以内存占用只发生在 kernel 内。正常状态下的 Socket 结构比较复杂，<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/linux/tcp.h#L144" target="_blank" rel="external">可以看看这里 struct tcp_sock</a>，它里面使用的是<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/net/inet_connection_sock.h#L88" target="_blank" rel="external">struct inet_connection_sock</a>。Linux 为了减少 TIME WAIT 连接的开销，专门构造了更精简的 Socket 数据结构给进入 TIME WAIT 状态的连接用，参看<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/linux/tcp.h#L397" target="_blank" rel="external">这里 struct tcp_timewait_sock</a> ，它里面用的是<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/net/inet_timewait_sock.h#L39" target="_blank" rel="external">inet_timewait_sock</a>。可以看到 TIME WAIT 状态下连接的结构要比正常连接数据结构简单不少，在内核的数据结构最多百来字节，即使有 65535 个 TIME WAIT 的连接存在也占不了多少内存，几十 M 最多了。</p>
<p>对于 CPU 的开销一般也不大，主要是在建立连接时<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/inet_connection_sock.c#L282" target="_blank" rel="external">在 inet_csk_get_port 函数内查找一个可用端口上的开销</a>。</p>
<p>所以 TIME WAIT 带来的最主要的副作用就是会占用端口，而端口数量有限，可能导致无法创建新连接的情况。</p>
<h2 id="减少-TIME-WAIT-占用端口的方法"><a href="#减少-TIME-WAIT-占用端口的方法" class="headerlink" title="减少 TIME WAIT 占用端口的方法"></a>减少 TIME WAIT 占用端口的方法</h2><h3 id="SO-LINGER"><a href="#SO-LINGER" class="headerlink" title="SO_LINGER"></a>SO_LINGER</h3><p>对于应用层来说调用 <code>send()</code> 后数据并没有实际写入网络，而是先放到一个 buffer 当中，之后慢慢的往网络上写。所以会出现应用层想要关闭一个连接时，连接的 buffer 内还有数据没写出去，需要等待这部分数据写出。调用 <code>close()</code> 后等待 buffer 内数据全部写出去的时间叫做 Linger Time。Linger Time 结束后开始正常的 TCP 挥手过程。</p>
<p>从前面介绍能看到，正常的主动断开连接一定会进入 TIME WAIT 状态，但除了正常的连接关闭之外还有非正常的断开连接的方法，可以不让连接进入 TIME WAIT 状态。方法就是给 Socket 配置 SO_LIGNER，这样在调用 <code>close()</code> 关闭连接的时候主动断开连接一方不是等待 buffer 内数据发完之后再发送 FIN 而是根据 SO_LINGER 参数配置的超时时间，等到最多这个超时时间这么长后，如果连接 buffer 内还有数据就直接发送 RST 强制重置连接，对方会收到 Connection Reset by peer 的错误，同时会导致主动断开连接一方所有还未来得及发送的数据全部丢弃。如果还未到 SO_LINGER 配置的超时时间连接 buffer 内的数据就全部发完了，就还是发 FIN 走正常挥手逻辑，但这样主动断开连接一方还是会进入 TIME WAIT。所以如果主动断开连接时完全不想让连接进入 TIME WAIT 状态，可以直接将 SO_LINGER 设置为 0 ，这样调用 <code>close()</code> 后会直接发 RST，丢弃 buffer 内所有数据，并让连接直接进入 CLOSED 状态。</p>
<p>从上面描述也能看出来其应用场景可能会比较狭窄。看到<a href="http://www.serverframework.com/asynchronousevents/2011/01/time-wait-and-its-design-implications-for-protocols-and-scalable-servers.html" target="_blank" rel="external">有地方</a>建议是说收到错误数据，或者连接超时的时候通过这种方式直接重置连接，避免有问题的连接还进入 TIME WAIT 状态。</p>
<h3 id="tcp-tw-reuse、-tcp-tw-recycle-配置和-TCP-Timestamp"><a href="#tcp-tw-reuse、-tcp-tw-recycle-配置和-TCP-Timestamp" class="headerlink" title="tcp_tw_reuse、 tcp_tw_recycle 配置和 TCP Timestamp"></a>tcp_tw_reuse、 tcp_tw_recycle 配置和 TCP Timestamp</h3><p>为了介绍这两个配置，首先需要介绍一下 TCP Timestamp 机制。</p>
<p><a href="https://tools.ietf.org/html/rfc1323" target="_blank" rel="external">RFC 1323</a> 和 <a href="https://tools.ietf.org/html/rfc7323" target="_blank" rel="external">RFC 7323</a> 提出过一个优化，在 TCP option 中带上两个 timestamp 值：<br>TCP Timestamps option (TSopt):<br>Kind: 8<br>Length: 10 bytes<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">+-------+-------+---------------------+---------------------+</div><div class="line">|Kind=8 |  10   |   TS Value (TSval)  |TS Echo Reply (TSecr)|</div><div class="line">+-------+-------+---------------------+---------------------+</div><div class="line">    1       1              4                     4</div></pre></td></tr></table></figure></p>
<p>TCP 握手时，通信双方如果都带有 TCP Timestamp 则表示双方都支持 TCP Timestamp 机制，之后每个 TCP 包都需要将自己当前机器时间带在 TSval 中，并且在每次收到对方 TCP 包做 ACK 回复的时候将对方的 TSval 作为 ACK 中 TSecr 字段返回给对方。这样通信双方就能在收到 ACK 的时候读取 TSecr 值并根据当前自己机器时间计算 TCP Round Trip Time，从而根据网络状况动态调整 TCP 超时时间，以提高 TCP 性能。<strong>请注意这个 option 虽然叫做 Timestamp 但不是真实日期时间，而是一般跟操作系统运行时间相关的一个持续递增的值</strong>。更进一步信息请看 RFC 的链接。</p>
<p>除了对 TCP Round Trip 时间做测量外，这个 timestamp 还有个功能就是避免重复收到数据包影响正常的 TCP 连接，这个功能叫做 PAWS，在上面 RFC 中也有介绍。</p>
<h4 id="PAWS-Protection-Against-Wrapped-Sequences"><a href="#PAWS-Protection-Against-Wrapped-Sequences" class="headerlink" title="PAWS (Protection Against Wrapped Sequences)"></a>PAWS (Protection Against Wrapped Sequences)</h4><p>从 PAWS 的全名上大概能猜想出来它是干什么的。正常来说每个 TCP 包都会有自己唯一的 SEQ，出现 TCP 数据包重传的时候会复用 SEQ 号，这样接收方能通过 SEQ 号来判断数据包的唯一性，也能在重复收到某个数据包的时候判断数据是不是重传的。但是 TCP 这个 SEQ 号是有限的，一共 32 bit，SEQ 开始是递增，溢出之后从 0 开始再次依次递增。所以当 SEQ 号出现溢出后单纯通过 SEQ 号无法标识数据包的唯一性，某个数据包延迟或因重发而延迟时可能导致连接传递的数据被破坏，比如：</p>
<img src="/2017/06/30/tcp-time-wait/TCP-Delay.png" alt="来自:http://www.sdnlab.com/17530.html" title="来自:http://www.sdnlab.com/17530.html">
<p>上图 A 数据包出现了重传，并在 SEQ 号耗尽再次从 A 递增时，第一次发的 A 数据包延迟到达了 Server，这种情况下如果没有别的机制来保证，Server 会认为延迟到达的 A 数据包是正确的而接收，反而是将正常的第三次发的 SEQ 为 A 的数据包丢弃，造成数据传输错误。PAWS 就是为了避免这个问题而产生的。在开启 Timestamp 机制情况下，一台机器发的所有 TCP 包的 TSval 都是单调递增的，PAWS 要求连接双方维护最近一次收到的数据包的 TSval 值，每收到一个新数据包都会读取数据包中的 TSval 值跟 Recent TSval 值做比较，如果发现收到的数据包 TSval 没有递增，则直接丢弃这个数据包。对于上面图中的例子有了 PAWS 机制就能做到在收到 Delay 到达的 A 号数据包时，识别出它是个过期的数据包而将其丢掉。<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_metrics.c#L576" target="_blank" rel="external">tcp_peer_is_proven 是 Linux 一个做 PAWS 检查的函数</a>。</p>
<p>TCP Timestamp 就像是 SEQ 号的扩展一样，用以在 SEQ 号相同时候判断两个数据包是否相同以及他们的先后关系。TCP Timestamp 时间跟系统运行时间相关，但并不完全对应，也不需要 TCP 通信双方时间一致。Timestamp 的起跳粒度可以由系统实现决定，粒度不能太粗也不能太细。粒度最粗至少要保证 SEQ 耗尽的时候 Timestamp 跳了一次，从而在 SEQ 号重复的时候能通过 Timestamp 将数据包区分开。Timestamp 跳的粒度越细，能支持的最大发送速度越高。TCP SEQ 是 32 bit，全部耗尽需要发送 2^32 字节的数据（<a href="https://tools.ietf.org/html/rfc7323#section-5.4" target="_blank" rel="external">RFC 7323 说是 2^31 字节数据</a>，我还没弄明白为什么不是 2^32)，如果 Timestamp 一分钟跳一次，那支持的最高发送速度是一分钟发完 2^32 字节数据；如果 Timestamp 一秒钟跳一次，那支持的最高发送速度是一秒钟发完 2^32 字节数据。另外，Timestamp 因为担负着测量 RTT 的职责，过粗的粒度也会降低探测精度，不能达到效果。</p>
<p>但是 Timestamp 本身也是有限的，一共 32 bit，Timestamp 跳的粒度越细，跳的越快，被耗尽的速度也越快。越短时间被耗尽越会出现和只靠 SEQ 来判断数据包唯一性相同的问题场景，即收到一个延迟到达的数据包后无法确认它是正常数据包还是延迟数据包。所以一般推荐 Timestamp 是 1ms 或 1s 一跳。假若是 1ms 一跳的话能支持最高 8 Tbps 的传输速度，也能在长达 24.8 天才会被耗尽。只要 MSL (<a href="https://en.wikipedia.org/wiki/Maximum_segment_lifetime" target="_blank" rel="external">Maximum segment lifetime - Wikipedia</a>) 小于 24.8 天，通过 TCP Timestamp 机制就能拒绝同一个连接上 SEQ 相同的重复数据包。MSL 大于 24.8 天几乎不可能，一个延迟的数据包在 24.8 天后到达接收方，该数据包的 SEQ 、Timestamp 又恰好和一个正常数据包相同，这个概率非常的小。MSL 相关可以参看<a href="http://www.rfc-editor.org/rfc/rfc793.txt" target="_blank" rel="external">RFC 793</a> 。</p>
<p>TCP Timestamp 机制开启之后 PAWS 会自动开启，控制 TCP Timestamp 的配置为 <code>net.ipv4.tcp_timestamps</code> 一般现在 Linux 系统都是开启的。因为能提升 TCP 连接性能，付出的代价相对又少。该配置看着叫 ipv4 但对 ipv6 一样有效。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Linux 上有几个跟 PAWS 相关的统计信息：</div><div class="line">    LINUX_MIB_PAWSPASSIVEREJECTED,      /* PAWSPassiveRejected */</div><div class="line">    LINUX_MIB_PAWSACTIVEREJECTED,       /* PAWSActiveRejected */</div><div class="line">    LINUX_MIB_PAWSESTABREJECTED,        /* PAWSEstabRejected */</div></pre></td></tr></table></figure>
<p>PAWS passive rejected 是 tcp_tw_recycle 开启后，收到 Client 的 SYN 时，因为 SYN 内的 Timestamp 没有通过 PAWS 检测而被拒绝的 SYN 数据包数量。这个稍后再说。<br>PAWS active rejected 是 Client 发出 SYN 后，收到 Server 的 SYN/ACK 但 SYN/ACK 中的 Timestamp 没有通过 PAWS 检测而被拒绝的 SYN/ACK 数据包数量。<br>PAWS established rejected 是正常建立连接后，因为数据包没有通过 PAWS 检测而被拒绝的数据包数量。</p>
<p>这三个定义在一起的，在<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/uapi/linux/snmp.h#L180" target="_blank" rel="external">uapi/linux/snmp.h 下</a>，下面会再次介绍这些计数是什么场景下被记录的。</p>
<p>目前来看 netstat -s 中只展示了 PAWS established rejected 的值，另外两个没展示，需要到 <code>/proc/net/netstat</code> 中看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">358306 packets rejects in established connections because of timestamp</div></pre></td></tr></table></figure>
<h4 id="net-ipv4-tcp-tw-reuse"><a href="#net-ipv4-tcp-tw-reuse" class="headerlink" title="net.ipv4.tcp_tw_reuse"></a>net.ipv4.tcp_tw_reuse</h4><p>需要注意的是这里说的 net.ipv4.tcp_tw_reuse 和下面说的 net.ipv4.tcp_tw_recycle 都是虽然名字里有 ipv4 但对 ipv6 同样生效。</p>
<p>只有开启了 PAWS 机制之后开启 net.ipv4.tcp_tw_reuse 才有用，并且仅对 outgoing 的连接有效，即仅对 Client 有效。Linux 中使用到 tcp_tw_reuse 的地方<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_ipv4.c#L111" target="_blank" rel="external">是 tcp_twsk_unique 函数</a>。<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/inet_hashtables.c#L315" target="_blank" rel="external">它是在 __inet_check_established 内被使用</a>，其作用是在 Client 连 Server 的时候如果已经没有端口可以使用，并且在 Client 端找到个处在 Time Wait 状态的和 Server 的连接，<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_ipv4.c#L129" target="_blank" rel="external">只要当前时间和该连接最后一次收到数据的时间差值超过 1s</a>，就能立即复用该连接，不用等待连接 Time Wait 结束。</p>
<p>前面说过 Time Wait 有两个作用，一个是避免同一个 TCP Tuple 上前一个连接的数据包错误的被后一个连接接收。因为有了 PAWS 机制，TCP 收到的数据会检查 TSval 是否大于最近一次收到数据的 TSval，所以这种情况不会发生。旧连接的数据包到达接收方后因为 PAWS 检测不通过会直接被丢弃，并更新 LINUX_MIB_PAWSESTABREJECTED 计数。</p>
<p>另一个作用是避免主动断开连接一方最后一个回复的 ACK 丢失而被动断开连接一方一直处在 LAST ACK 状态，超时后会再次发 FIN 到主动断开连接一方。此时如果主动断开连接一方不在 Time Wait 会触发主动断开连接一方发出 RST 让被动连接一方出现一个 Connection reset by peer 的报错。不过这个实际上还好，数据至少都发完了。如果被动断开连接一方还未因超时而重发 FIN 就收到主动断开连接一方因为 tcp_tw_reuse 提前从 TIME WAIT 状态退出而发出的 SYN，被动连接一方会立即重发 FIN，主动连接一方收到 FIN 后回复 RST，之后再重发 SYN 开始正常的 TCP 握手。后一个过程图如下：</p>
<img src="/2017/06/30/tcp-time-wait/tcp_tw_reuse.png" alt="来自：https://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux" title="来自：https://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux">
<p><a href="https://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux" target="_blank" rel="external">这篇文章</a> 说没有开启 TCP Timestamp 时，被动断开连接一方处在 LAST_ACK 状态，收到 SYN 后会回复 RST；开启了 TCP Timestamp 之后，被动连接一方处在 LAST_ACK 状态收到 SYN 会丢弃这个 SYN，在 FIN 超时后再次发 FIN, ACK，这里我有些疑惑。不明白为什么 TCP Timestamp 开启之后处在 LAST ACK 状态的一方就会默认丢弃对方发来的 SYN。PAWS 只有 Timestamp 不和要求时才会丢消息，但同一台机器上没有重启的话 TSval 是逐步递增的，<a href="http://elixir.free-electrons.com/linux/latest/source/net/ipv4/tcp_ipv4.c#L130" target="_blank" rel="external">SEQ 号也是在原来 TIME WAIT 时存下的 SEQ 号基础上加一个偏移值得到</a>，按说没有理由会自动丢弃 SYN 的。</p>
<p>还一个要注意到的是 tcp_tw_reuse 在 reuse 连接的时候<a href="http://elixir.free-electrons.com/linux/latest/source/net/ipv4/tcp_ipv4.c#L133" target="_blank" rel="external">新创建的连接会复用之前连接保存的最近一次收到数据的 Timestamp</a>。这个是与下面要说的 tcp_tw_recycle  的不同点，也是为什么 tcp_tw_reuse 在使用了 NAT 的网络下是安全的，而 tcp_tw_recycle 在使用了 NAT 的网络下是不安全的。因为 tcp_tw_reuse 记录的最近一次收到数据的 Timestamp 是针对 Per-Connection 的，而 tcp_tw_recycle 记录的最近一次收到数据的 Timestamp 是 Per-Host 的，在 NAT 存在的情况下同一个 Host 后面有多少机器就说不清了，每台机器时间不同发出数据包的 TSval 也不同，PAWS 可能错误的将正常的数据包丢弃所以会导致问题，这个在下面描述 tcp_tw_recycle 的时候再继续说。</p>
<p>从上面描述来看 tcp_tw_reuse 还是比较安全的，一般是建议开启。不过该配置对 Server 没有用，因为只有 outgoing 的连接才会使用。每次 reuse 一个连接后会将 TWRecycled 计数加一，通过 netstat -s 能看到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">7212 time wait sockets recycled by time stamp</div></pre></td></tr></table></figure>
<p>虽然叫做 TWRecycled 但实际它指的是 reuse 的连接数，不是下面要说的 recycled 的连接数。其反应的是 LINUX_MIB_TIMEWAITRECYCLED 这个计数，在<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/inet_hashtables.c#L362" target="_blank" rel="external">__inet_check_established 内 reuse TIME WAIT 连接后计数</a></p>
<h4 id="net-ipv4-tcp-tw-recycle"><a href="#net-ipv4-tcp-tw-recycle" class="headerlink" title="net.ipv4.tcp_tw_recycle"></a>net.ipv4.tcp_tw_recycle</h4><p>该配置也是要基于 TCP Timestamp，单独开启 tcp_tw_recycle 没有效果。相对 tcp_tw_reuse 来说 tcp_tw_recycle 是个更激进的参数，这个参数在 Linux 的使用参看：<br><a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_input.c#L6348" target="_blank" rel="external">linux/net/ipv4/tcp_input.c - Elixir - Free Electrons</a><br><a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_minisocks.c#L91" target="_blank" rel="external">linux/net/ipv4/tcp_minisocks.c - Elixir - Free Electrons</a><br><a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_ipv4.c#L144" target="_blank" rel="external">linux/net/ipv4/tcp_ipv4.c - Elixir - Free Electrons</a></p>
<h5 id="理一下-TCP-收消息过程"><a href="#理一下-TCP-收消息过程" class="headerlink" title="理一下 TCP 收消息过程"></a>理一下 TCP 收消息过程</h5><p>为了说清楚 tw_recycle 的使用场景，我准备把接收消息过程理一遍，可能会写的比较啰嗦，看的时候需要静下心来慢慢看。</p>
<p>首先链路层收到 TCP IPv4 的数据后会走到 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_ipv4.c#L1605" target="_blank" rel="external">tcp_v4_rcv</a>  这里，看到参数是 struct sk_buff，以后有机会记录一下，NIC 从链路接收到数据写入 Ring Buffer 后就会构造这个 struct sk_buff，并且之后在数据包的处理过程中一直复用同一个 sk_buff，避免内存拷贝。</p>
<p>tcp_v4_rct 首先是进行各种数据包的校验，根据数据包的 source，dst 信息找到 Socket，是个<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/net/sock.h#L311" target="_blank" rel="external">struct sock 结构</a>。我们这里主要说 TIME WAIT，所以别的东西都先不管。主要是看到下面会调用 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_ipv4.c#L1732" target="_blank" rel="external">tcp_v4_do_rcv</a>。在 tcp_v4_do_rcv 内会开始真的处理 sk_buff 数据内容，如果连接不是 ESTABLSHED，也不是 LISTEN 会走到 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_ipv4.c#L1443" target="_blank" rel="external">tcp_rcv_state_process</a>函数，这个函数是专门处理 TCP 连接各种状态转换的。还是那句话，我们关心的是 TIME WAIT，所以别的也都不看，先看连接的状态转换。我们知道连接先是在 FIN-WAIT-1，收到对方 ACK 后进入 FIN-WAIT-2，再收到对方 FIN 后进入 TIME-WAIT。在 tcp_rcv_state_process 先找到<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_input.c#L6009" target="_blank" rel="external"> FIN-WAIT-1 这个 case</a>，这里先会检查 acceptable 是否置位，表示收到的 sk_buff 内 ACK flag 是置位的，如果没有置位会立即返回 1 表示状态有误，之后会发 RST。即在 FIN-WAIT-1 状态下，连接期待的数据必须设置 ACK Flag，没设置就立即发 RST 重置连接。</p>
<p>如果一切正常，则<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_input.c#L6033" target="_blank" rel="external">将连接状态设置为 FIN-WAIT-2</a>，并读取 sysctl 的 net.ipv4.tcp_fin_timeout 配置。如果 sk_buff 中没有同时设置 FIN 说明对方是先回复了 ACK，让当前连接线进入 FIN-WAIT-2，FIN 在之后的包中发过来。所以此时设置连接状态并 discard 当前 sk_buff。这里有些疑问，此时连接实际是 Half-Open 的，这里没有判断 ACK 内有没有别的数据就把 sk_buff 丢弃了，从 RFC 793 中似乎没看到说要求针对 FIN 的 ACK 内必须不能有数据。接着说，看到用 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_input.c#L6064" target="_blank" rel="external">tcp_time_wait 函数</a>将 tw_substate 设置到 FIN-WAIT-2 ，将连接设置为 TIME WAIT，并设置超时时间是 net.ipv4.tcp_fin_timeout 的值。稍后再继续说 tcp_time_wait 这个函数，还有很多可以挖掘的。</p>
<p>如果 sk_buff 同时设置了 FIN，说明对方是将 FIN 和 ACK 一起发来的，同一个数据包中 FIN 和 ACK 两个 Flag 都置位，<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_input.c#L6055" target="_blank" rel="external">此时并不立即设置 TCP 连接的状态</a>，而是在稍后在专门处理 FIN 的逻辑中处理 TCP 状态变换。如果 FIN 和 ACK 一起设置了，不会 discard 数据包，再往下还有个 case，要注意到有个 switch 的 Fall through，也就是说连接不管在 CLOSE_WAIT, CLOSING, FIN_WAIT1, FIN_WAIT2, ESTALISHED 等最终都会进入 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_input.c#L6112" target="_blank" rel="external">tcp_data_queue</a> 来继续处理这个收到的 sk_buff。在 tcp_data_queue 中我们看到<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_input.c#L4644" target="_blank" rel="external">带着 FIN 的 sk_buff 会交给专门的 tcp_fin</a> 来处理。因为在 tcp_rcv_state_process 内我们刚刚将连接状态设置为 TCP_FIN_WAIT2 所以在 tcp_fin 的 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_input.c#L4065" target="_blank" rel="external">switch 内我们找到 TCP_FIN_WAIT2 的处理逻辑</a>，即回复 ACK 并通过 tcp_time_wait 函数设置 tw_substate 为 TCP_TIME_WAIT ，将连接设置为 TIME WAIT 状态，并设置超时时间是 0。</p>
<p>接下来我们看看 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_minisocks.c#L257" target="_blank" rel="external">tcp_time_wait 函数</a> ，这个函数完成了将连接转换到 TIME WAIT 状态的逻辑。如果 tw_recycle 、TCP Timestamp 开启，<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_minisocks.c#L266" target="_blank" rel="external">会先 Per-Host 的缓存连接最后一次收到数据的对方 TSval</a>。将连接从普通的 struct sock 转换为 TIME WAIT 状态下连接特有的更加精简的 struct tcp_timewait_sock 结构。并 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_minisocks.c#L328" target="_blank" rel="external">设置连接处在 TIME WAIT 的超时时间</a>，能看到 tw_recycle 开启的话 tw_timeout 只有一个 RTO 这么长，能大幅度减少连接处在 TIME WAIT 的时间。而没有开启 tw_recycle 的话超时时间是 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/net/tcp.h#L116" target="_blank" rel="external">TIME_WAIT_LEN</a>，该值是个不可配置的 macro。TIME WAIT 超时后会执行 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/inet_timewait_sock.c#L145" target="_blank" rel="external">tw_timer_handler</a>将连接清理。tw_timer_handler 是在构造 inet_timewait_sock  执行 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/inet_timewait_sock.c#L191" target="_blank" rel="external">inet_twsk_alloc</a> 将一个 TIME WAIT 的 socket 和 tw_timer_handler 关联起来的。</p>
<p>正常的连接使用的是 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/linux/tcp.h#L144" target="_blank" rel="external">struct tcp_sock</a> 内部第一个结构是 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/net/sock.h#L311" target="_blank" rel="external">struct sock</a>，TIME WAIT 的连接使用的是 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/linux/tcp.h#L397" target="_blank" rel="external">struct tcp_timewait_sock</a> 内部第一个结构是 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/net/inet_timewait_sock.h#L39" target="_blank" rel="external">struct inet_timewait_sock</a>。struct sock 和 struct inet_timewait_sock 内部第一个结构都是 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/net/sock.h#L149" target="_blank" rel="external">struct sock_common</a>。struct sock_common 是连接相关最核心的信息，标识一个连接必须要有这个。而为了减少 TIME WAIT 连接对内存空间的占用，所以弄了精简的 struct tcp_timewait_sock 可以看到它相对 strcut tcp_sock 内容要少的多，并且内部 struct inet_timewait_sock 相对于 struct sock 来说内容也少了很多，整个结构很精简。看到 struct sock 和 struct inet_time_wait_sock 第一个结构都是 struct sock_common 所以如果是访问 struct sock_common 的内容，指向这两个 struct 的指针是能够相互转换的。这两个 struct 内部定义了很多宏，用于方便的访问 struct sock_common 的内容。比如 struct sock 内的 sk_state 和 struct inet_timewait_sock 内的 tw_state 实际都访问的是 struct sock_common 的 skc_common。</p>
<p>说 struct socket 等结构主要是为了说明 tcp_time_wait 内是如何将 socket 状态设置为 TIME WAIT的。一般设置 socket 状态使用的是 tcp_set_state 这个函数，比如在 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_input.c#L5969" target="_blank" rel="external">tcp_rcv_state_process</a> 内之前看到的。但 TIME WAIT 这个状态却不是 tcp_set_state 来设置的，而是在从 struct socket 构造 struct inet_timewait_sock 时设置的，构造 struct inet_timewait_sock 会<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/inet_timewait_sock.c#L179" target="_blank" rel="external">默认设置 tw_state 为 TCP_TIME_WAIT</a>。 从前面描述来看，连接在 FIN-WAIT-1 状态时收到 ACK 且 FIN 置位时，会在回复 ACK 后执行 tcp_time_wait，此时连接确实应该进入 TIME WAIT 状态；但在收到 ACK 且 FIN 没有置位的时候，连接实际处在 FIN-WAIT-2 状态却也会执行 tcp_time_wait。tcp_time_wait 内会将连接状态默认的设置为 TCP_TIME_WAIT，这没有实际反映出当前连接的实际状态，所以 struct inet_timewait_sock 内还有个 tw_substate 用以记录这个连接的实际状态。如果连接实际处在 FIN-WAIT-2，收到对方 FIN 后在 tcp_v4_rcv 内根据 sk_buff 找到 Socket，此时的 Socket 虽然使用的是 struct sock 指针，但实际指的是个 struct inet_timewait_sock，访问其 sk_state 实际访问的是 struct sock_common 的 sk_state 字段，也即 struct inet_timewait_sock 的 tw_state 字段。<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_ipv4.c#L1663" target="_blank" rel="external">所以读到的当前状态是 TCP_TIME_WAIT</a>。于是进入 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_minisocks.c#L91" target="_blank" rel="external">tcp_timewait_state_process 函数</a>处理数据包。当连接的 sk_state 处在 TCP_TIME_WAIT 时，所有收到的数据包均交给 tcp_timewait_state_process 处理。在 tcp_timewait_state_process 内可以看到会检查 tw_substate 是不是 TCP_FIN_WAIT2，是的话会将 tw_substate 也设置为 TCP_TIME_WAIT，并会<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_minisocks.c#L155" target="_blank" rel="external">重新设置 TIME WAIT 的 timeout 时间</a>。设置的逻辑跟连接在 FIN-WAIT-1 下收到 ACK 且 FIN 置位时一样，会判断 tw_recycle 是否开启，开启的话 timeout 就是一个 RTO，不是的话就是 TIME_WAIT_LEN。</p>
<p>从上面这么一大段的描述中我们得到这么一些信息：</p>
<ol>
<li>连接在进入 FIN-WAIT-2 后内核维护的 socket 就会改为和 TIME WAIT 状态时一样的精简结构，以减少内存占用</li>
<li>连接进入 FIN-WAIT-2 后为了避免对方不给回复 FIN，所以会设置 <code>net.ipv4.tcp_fin_timeout</code> 这么长的超时时间，超时后会按照清理 TIME WAIT 连接的逻辑清理 FIN-WAIT-2 连接</li>
<li>tcp_tw_recycle 开启后，timeout 只有一个 RTO 这个正常来说是会大大低于 TCP_TIMEWAIT_LEN 的。 这里就说明 tcp_tw_recycle 是对 outgoing 和 incoming 的连接都会产生效果，不管连接是谁先发起创建的，只要是开启 tcp_tw_recycle 的机器先断开连接，其就会进入 TIME WAIT 状态(或 FIN-WAIT-2)，并且会受到 tcp_tw_recycle 的影响，大幅度缩短 TIME WAIT 的时间。</li>
</ol>
<h5 id="tcp-tw-recycle-为什么是不安全的"><a href="#tcp-tw-recycle-为什么是不安全的" class="headerlink" title="tcp_tw_recycle 为什么是不安全的"></a>tcp_tw_recycle 为什么是不安全的</h5><p>跟 tcp_tw_reuse 一样，由于 PAWS 机制的存在，缩短 TIME WAIT 后同一个 TCP Tuple 上前一个连接的数据包不会被后一个连接错误的接收。TIME WAIT 另一个要处理的 LAST ACK 的问题跟 tcp_tw_reuse 也一样，不会产生很大的问题，新的连接依然能正常建立，旧连接的数据也能保证都发到对方，只是旧连接上可能会产生一个 Connection reset by peer 的错误。那 tcp_tw_recycle 是不是就是安全的呢？为什么那么多人都不推荐开启这个机制呢？</p>
<p>原因是前面说过 tcp_tw_reuse 新建立的连接会复用前一次连接保存的 Recent TSval 即最后一次收到数据的 Timestamp 值，这个值是 Per-Connection 的，即使有 NAT 的存在，也不会产生问题。比如当前机器是 A 要和 B C D 三台机器建立连接，假设 B C D 三台机器都在 NAT 之后，对 A 来说 B C D 使用的是相同的 IP 或者称为 Host。B C D 三台机器因为启动时间不同，A 与他们建立连接之后他们发来的 TSval 都不相同，有前有后。因为 A 是以 Per-Connection 的保存 TSval ，不会出现比如因为 C 机器时间比 B 晚，A 收到 B 的一条消息之后再收到 C 的消息而丢弃 C 的数据的情况。因为在 A 上为 B C D 三台机器分别保存了 Recent TSval，他们之间不会混淆。</p>
<p>但是对于 tcp_tw_recycle 来说，TIME WAIT 之后连接信息快速的被回收，Per-Connection 保存的 TSval 记录就被清除了，取而代之的是另一个 Per-Host 的 TSval cache，在这里能看到<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_minisocks.c#L266" target="_blank" rel="external">在 tcp_time_wait</a>当 tcp_tw_recycle 和 TCP Timestamp 都开启后，连接进入 TIME WAIT 之前会将 socket 的 Timestamp 存下来，<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_metrics.c#L638" target="_blank" rel="external">并且存储方法是 Per-Host 的</a>。这样在 NAT 存在的场景下就有问题了，还是上面的例子，B C D 在同一个 NAT 之后，具有相同的 Host，B 先跟 A 建立连接，此时 Per-Host cache 更新为 B 的机器时间，之后 C 来跟 A 建立连接(接收 incoming 连接请求相关逻辑可以看<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_input.c#L6277" target="_blank" rel="external">tcp_conn_request 这个函数</a>)，读取 Per-Host Cache 后发现 C 的 SYN 中 TSval 比 Cache 的 TSval 时间要早，<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_input.c#L6348" target="_blank" rel="external">于是直接默默丢弃 C 的 SYN</a>。这种情况会更新 LINUX_MIB_PAWSPASSIVEREJECTED 从而在 <code>/proc/net/netstat</code> 下的 PAWSPassive 看这个计数。在网上看到好些地方说到这个问题的时候都说会更新 PAWSEstab 计数，这是不对的。</p>
<p>除了上面场景中 A 会丢弃 C 的 SYN 之外，还有别的引起问题的场景。比如 A 跟 B 建立了连接，Per-Host TSval 更新为 B 的时间，之后又要去跟 C 建立连接( IPv4 下创建 outgoing 连接相关逻辑可以看 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_ipv4.c#L144" target="_blank" rel="external">tcp_v4_connect 函数</a>)，<a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_ipv4.c#L208" target="_blank" rel="external">A 发出 SYN 时会更新该连接的 Recent TSval 为缓存的 Per-Host TSval 时间</a>，即 B 的时间。假设 A 这一侧网络环境比较简单，IP 只有 A 一台机器在用，于是 C 校验 A 的 SYN 通过，所以 C 会正常回复 SYN//ACK 给 A，但 SYN//ACK 到达 A 之后，因为 SYN//ACK 带着 C 的 TSval，其时间晚于 B 的时间，导致 A 直接丢弃 C 发来的 SYN/ACK，并更新 LINUX_MIB_PAWSACTIVEREJECTED 计数，该计数能在 <code>/proc/net/netstat</code> 下的  PAWSActive 看到。</p>
<p>所以，tcp_tw_recycle 是不推荐开启的，因为 NAT 在网络上大量的存在，配合 tcp_tw_recycle 会出现问题。</p>
<p>如果有连接由于 tcp_tw_recycle 开启而被清理的话，会更新 <code>/proc/net/netstat</code>的 TWKilled 计数，在 Linux 内是 LINUX_MIB_TIMEWAITKILLED，请注意和 TCPRecycled 区分。这个计数在 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/inet_timewait_sock.c#L145" target="_blank" rel="external">tw_timer_handler 中当 tw-&gt;tw_kill 有值的时候会更新</a>，而 tw_kill 是在连接进入 TIME WAIT，schedule 清理连接任务时候被设置的，在 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/inet_timewait_sock.c#L223" target="_blank" rel="external">__inet_twsk_schedule</a>。</p>
<p>跟 TIME WAIT 相关的还有一个计数叫做 TW，在 Linux 内部叫做 LINUX_MIB_TIMEWAITED。它也是在 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/inet_timewait_sock.c#L145" target="_blank" rel="external">tw_timer_handler 中</a>被更新，可以看到只要不是 tw_kill 都会当做普通的 TW 被计数，我理解只要是正常 TIME WAIT 结束的都会计入这里，被 recycle 的会计入 TWKilled，被 reuse 的会计入 TWRecycled。但 netstat -s 对应 TW 的描述是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">69598943 TCP sockets finished time wait in fast timer</div></pre></td></tr></table></figure></p>
<p>不明白这里 fast timer 是什么意思。</p>
<h2 id="net-ipv4-tcp-max-tw-buckets-是什么"><a href="#net-ipv4-tcp-max-tw-buckets-是什么" class="headerlink" title="net.ipv4.tcp_max_tw_buckets 是什么"></a>net.ipv4.tcp_max_tw_buckets 是什么</h2><p>用于限制系统内处在 TIME WAIT 的最大连接数量，当 TIME WAIT 的连接超过限制之后连接会直接被关闭进入 CLOSED 状态，并会打印日志：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">TCP time wait bucket table overflow</div></pre></td></tr></table></figure></p>
<p>可以看出这个限制有些暴力，得尽量去避免。还是之前的话，TIME WAIT 是有自己的作用的，暴力干掉它对我们并没有好处。</p>
<p>tcp_max_tw_buckets 这个限制主要是控制 TIME WAIT 连接占用的资源数，包括内存、CPU 和端口资源。避免 denial-of-service 攻击。比如端口全部被占用处在 TIME WAIT 状态，可能会出现没有多余的端口来建立新的连接。</p>
<h2 id="SO-REUSEADDR-是什么-SO-REUSEPORT-是什么"><a href="#SO-REUSEADDR-是什么-SO-REUSEPORT-是什么" class="headerlink" title="SO_REUSEADDR 是什么, SO_REUSEPORT 是什么"></a>SO_REUSEADDR 是什么, SO_REUSEPORT 是什么</h2><p>看完上面内容之后可能会和我一样产生这个疑问，因为这两个从名字上看上去跟 tcp_tw_reuse 很像。</p>
<p>关于 SO_REUSEADDR 和 SO_REUSEPORT <a href="https://stackoverflow.com/questions/14388706/socket-options-so-reuseaddr-and-so-reuseport-how-do-they-differ-do-they-mean-t" target="_blank" rel="external">这篇文章</a>介绍的特别好，强烈建议一看。唯一是里面关于 TIME WAIT 内容个人认为是不正确的，想先说明一下，避免以后自己再看这个文章的时候搞混淆。他说：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">That&apos;s why a socket that still has data to send will go into a state called TIME_WAIT when you close it. In that state it will wait until all pending data has been successfully sent or until a timeout is hit, in which case the socket is closed forcefully.</div></pre></td></tr></table></figure></p>
<p>实际上连接进入 TIME WAIT 之后是不可能再有数据发出的，因为能进入 TIME WAIT 一定是已经收到了对方的 FIN，此时对方期待的只有 ACK，发应用层数据会引起对方回复 RST。</p>
<p>还有这里：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">The amount of time the kernel will wait before it closes the socket, regardless if it still has pending send data or not, is called the Linger Time. The Linger Time is globally configurable on most systems and by default rather long (two minutes is a common value you will find on many systems). It is also configurable per socket using the socket option SO_LINGER which can be used to make the timeout shorter or longer, and even to disable it completely.</div></pre></td></tr></table></figure></p>
<p>关于 Linger Time 的描述跟 TIME WAIT 混淆了，Linger Time 并不是全局配置的，最长也不是 2 分钟，这个都是 TIME WAIT 的长度。Linger TIme 确实也有默认长度，但是个非常大的值，<a href="http://elixir.free-electrons.com/linux/latest/source/net/core/sock.c#L787" target="_blank" rel="external">参看这里</a>。所以基本能认为不设置 SO_LINGER 的话，<code>close()</code> 调用后会一直等到 buffer 内数据发完才会开始断开连接流程。</p>
<p>但是瑕不掩瑜，这篇文章把 SO_REUSEADDR 和 SO_REUSEPORT 讲的很清楚。</p>
<p>上面介绍过的 tcp_tw_reuse 和 tcp_tw_recycle 都是内核级参数，使用之后会在整个系统产生作用，所有创建的连接都受到影响。SO_REUSEADDR 和 SO_REUSEPORT 都是单个连接级的参数，使用后只能对单个连接产生影响，不是整个系统级别的。</p>
<h3 id="SO-REUSEADDR"><a href="#SO-REUSEADDR" class="headerlink" title="SO_REUSEADDR"></a>SO_REUSEADDR</h3><p>有两个作用：<br>一个是 <code>bind()</code> socket 时可以绑定 “any address” IPv4 下是 <code>0.0.0.0</code> 或在 IPv6 下是<code>::</code>。SO_REUSEADDR 不开启的话，这个 any address 会和机器具体使用的 IP 冲突，如果绑定的端口一致会报错。比如本地有两个网卡，IP 分别是 192.168.0.1 和 10.0.0.1。如果不开启 SO_REUSEADDR，绑定 0.0.0.0 的某个端口比如 21 之后，再想绑定某个具体的 IP 192.168.0.1 的 21 端口就不允许了。而开启 SO_REUSEADDR 之后除非是 IP 和 Port 都被绑定过才会报错。有个表：</p>
<img src="/2017/06/30/tcp-time-wait/so-reuseaddr.png" alt="来自: https://stackoverflow.com/questions/14388706/socket-options-so-reuseaddr-and-so-reuseport-how-do-they-differ-do-they-mean-t" title="来自: https://stackoverflow.com/questions/14388706/socket-options-so-reuseaddr-and-so-reuseport-how-do-they-differ-do-they-mean-t">
<p>上表中全部默认使用 BSD 系统，并且是 socketA 先绑定，之后再绑定 socketB。ON/OFF表示 SO_REUSEADDR 是否开启不会影响结果。</p>
<p>另一个作用是当连接主动断开后进入 TIME WAIT 状态，不开启 SO_REUSEADDR 的话，TIME WAIT 状态下连接的 IP 和 Port 也是被占用的，同一个 IP 和 Port 不能再次被 bind。但是开启 SO_REUSEADDR ，连接进入 TIME WAIT 后它使用的 IP 和 Port 能再次被应用 bind。bind 时会忽略同一个 IP 和 Port 的连接是否在 TIME WAIT 状态。</p>
<p>需要说明的是上面 SO_REUSEADDR 的行为是 BSD 系统上的，在 Linux 上会有所不同。在 Linux 上上图第六行的绑定是不行的，即先绑定 any address 再绑定 specific address 并且端口相同会被拒绝，反过来也一样，比如先绑定 192.168.1.0:21 再绑定 0.0.0.0:21 是被拒绝的。也就是说在 Linux 上上述 SO_REUSEADDR 第一个作用是没有用的，因为不即使不设置 SO_REUSEADDR 绑定两个不同的 IP 也是允许的。SO_REUSEADDR 在 BSD 上的第二个作用和在 Linux 上相同。除此之外，Linux 上的 SO_REUSEADDR 还有第三个作用，设置后允许同一个 specific addr 和 port 被多个 socket 绑定，行为和下面要说的 SO_REUSEPORT 类似。主要是因为 Linux 3.9 之前没有 SO_REUSEPORT，但又有 SO_REUSEPORT 的使用场景，于是 SO_REUSEADDR 发展出了 SO_REUSEPORT 的能力来替代 SO_REUSEPORT，但 Linux 3.9 之后有了专门的参数 SO_REUSEPORT，SO_REUSEADDR 则保持原状。</p>
<h3 id="SO-REUSEPORT"><a href="#SO-REUSEPORT" class="headerlink" title="SO_REUSEPORT"></a>SO_REUSEPORT</h3><p>BSD 系统上设置后允许同一个 specific address, port 被多个 Socket 绑定，只要这些 Socket 绑定地址的时候都设置了 SO_REUSEPORT。听上去这个 SO_REUSEPORT 干的更像是 reuse addr 的活。如果占用 source addr 和 port 的连接处在 TIME WAIT 状态，并且没有设置 SO_REUSEPORT 那该地址和端口不能被另一个 socket 绑定。事实上 SO_REUSEPORT 和 TIME WAIT 没有什么关系，设置后能不能绑定某个端口和地址完全是看这个端口和地址现在有没有被别的连接使用，如果有则要看这个连接是否开启了 SO_REUSEPORT，跟这个连接被占用时处在 TCP 的什么状态完全无关。</p>
<p>在 Linux 上相对 BSD 还要求地址和端口重用必须是同一个 user 之间，如果地址和端口被某个 Socket 占用，并且这个 Socket 是另一个 user 的，那即使该 Socket 绑定时开启了 SO_REUSEPORT 也不能再次被绑定。另外 Linux 还会做一些 load balancing 的工作，对 UDP 来说一个连接的数据包会被均匀分发到所有绑定同一个 addr 和 port 的 socket 上，对于 TCP 来说是 accept 会被均匀的分发到绑定在同一个 addr 和 port 的连接上。</p>
<h2 id="tcp-fin-timeout"><a href="#tcp-fin-timeout" class="headerlink" title="tcp_fin_timeout"></a>tcp_fin_timeout</h2><p>很多地方写的说 <code>net.ipv4.tcp_fin_timeout</code> 将这个配置减小一些能缩短 TIME WAIT 时间，但是我们从介绍 tcp_tw_recycle 那节看到，如果没设置 tcp_tw_recycle 的话 TIME WAIT 时间是个固定值 TCP_TIMEWAIT_LEN，这个值是个 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/include/net/tcp.h#L116" target="_blank" rel="external">macro</a> :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT                 </div><div class="line">                                  * state, about 60 seconds */</div></pre></td></tr></table></figure>
<p>也就是说它的长度并不是个可配置项。</p>
<p><code>net.ipv4.tcp_fin_timeout</code> 的定义在 <a href="http://elixir.free-electrons.com/linux/v4.11.8/source/net/ipv4/tcp_ipv4.c#L2470" target="_blank" rel="external">struct tcp_prot 中</a> 其默认值定义为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#define TCP_FIN_TIMEOUT TCP_TIMEWAIT_LEN</div><div class="line">                                    /* BSD style FIN_WAIT2 deadlock breaker.</div><div class="line">                                     * It used to be 3min, new value is 60sec,</div><div class="line">                                     * to combine FIN-WAIT-2 timeout with</div><div class="line">                                     * TIME-WAIT timer.</div><div class="line">                                     */</div></pre></td></tr></table></figure>
<p>也就是说这个配置实际是去控制 FIN-WAIT-2 时间的，只是默认值恰好跟 TIME_WAIT 一致。就不贴使用这个配置的地方了，总之该配置是控制 FIN-WAIT-2 的，也就是说主动断开连接一方发出 FIN 也收到 ACK 后等待对方发 FIN 的时间，该配置并不会影响到 TIME WAIT 长度。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li>有很多参考了这篇文章：<a href="https://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux" target="_blank" rel="external">Coping with the TCP TIME-WAIT state on busy Linux servers | Vincent Bernat</a></li>
<li>TCP Timestamp 、PAWS 相关主要参考了 <a href="https://tools.ietf.org/html/rfc1323" target="_blank" rel="external">RFC 1323</a>(已被 RFC 7323 取代)、<a href="https://tools.ietf.org/html/rfc7323" target="_blank" rel="external">RFC 7323</a>、<a href="https://tools.ietf.org/html/rfc6191" target="_blank" rel="external">RFC 6191</a></li>
<li><a href="http://www.serverframework.com/asynchronousevents/2011/01/time-wait-and-its-design-implications-for-protocols-and-scalable-servers.html" target="_blank" rel="external">这个</a>虽然和 1 很多内容重复，但能用来相互印证，至少在未开启 TCP Timestamp 机制时被断开连接一方处在 LAST ACK 状态又没收到 ACK 的处理方式说的不一样 </li>
<li><a href="https://stackoverflow.com/questions/14388706/socket-options-so-reuseaddr-and-so-reuseport-how-do-they-differ-do-they-mean-t" target="_blank" rel="external">linux - Socket options SO_REUSEADDR and SO_REUSEPORT, how do they differ? Do they mean the same across all major operating systems? - Stack Overflow</a></li>
<li><a href="https://stackoverflow.com/questions/410616/increasing-the-maximum-number-of-tcp-ip-connections-in-linux" target="_blank" rel="external">https://stackoverflow.com/questions/410616/increasing-the-maximum-number-of-tcp-ip-connections-in-linux</a></li>
<li><a href="https://stackoverflow.com/questions/8893888/dropping-of-connections-with-tcp-tw-recycle" target="_blank" rel="external">linux - Dropping of connections with tcp_tw_recycle - Stack Overflow</a></li>
<li><a href="http://www.sdnlab.com/17530.html" target="_blank" rel="external">Linux服务器丢包故障的解决思路及引申的TCP/IP协议栈理论 | SDNLAB | 专注网络创新技术</a></li>
<li><a href="http://troy.yort.com/improve-linux-tcp-tw-recycle-man-page-entry/" target="_blank" rel="external">Improve Linux tcp_tw_recycle man page entry - Troy Davis, Seattle</a></li>
<li><a href="https://idea.popcount.org/2014-04-03-bind-before-connect/" target="_blank" rel="external">https://idea.popcount.org/2014-04-03-bind-before-connect/</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TIME-WAIT 是 TCP 挥手过程的一个状态。很多地方都对它有说明，这里只贴两个图唤起记忆。下面是 TCP 完整的状态图：&lt;/p&gt;
&lt;img src=&quot;/2017/06/30/tcp-time-wait/TCP-State.png&quot; alt=&quot;来自：http://ww
    
    </summary>
    
    
      <category term="Network" scheme="http://ylgrgyq.github.io/tags/Network/"/>
    
      <category term="TCP" scheme="http://ylgrgyq.github.io/tags/TCP/"/>
    
  </entry>
  
  <entry>
    <title>TCP Backlog</title>
    <link href="http://ylgrgyq.github.io/2017/05/18/tcp-backlog/"/>
    <id>http://ylgrgyq.github.io/2017/05/18/tcp-backlog/</id>
    <published>2017-05-18T01:01:10.000Z</published>
    <updated>2017-07-02T00:48:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章对 TCP Backlog 的说明写的特别好，<a href="http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html" target="_blank" rel="external">How TCP backlog works in Linux</a> 强烈建议看一看，希望这篇文章的地址不要变化，方便以后回顾。</p>
<p>Linux 上 TCP 握手的时候为每个独立进程维护了两个队列，一个是 Server 在收到 Client 的 SYN 并回复 SYN/ACK 之后，会将连接放入一个 incomplete sockets queue，这个队列的大小受到 net.ipv4.tcp_max_syn_backlog 的限制，超过这个队列长度之后 Server 再收到 SYN 就不会做响应，等 Client 超时之后会重传 SYN 再次尝试建立连接。<strong>需要注意的是在 net.ipv4.tcp_syncookies 置位时 net.ipv4.tcp_max_syn_backlog 参数会失效</strong>。net.ipv4.tcp_syncookies 的相关信息在下面专门说。</p>
<p>Client 收到 Server 的 SYN/ACK 后会回复 ACK 以确认连接建立，之后 Server 收到 Client 的 ACK 时 TCP 连接完成握手工作。Server 将连接从 incomplete sockets queue 中移出来放入另一个进程相关的等待队列称为 completely established sockets queue，在应用调用 <code>accept</code> 系统调用之后移出。completely established sockets queue 队列大小由应用调用 <code>listen</code> 系统调用时传入的 backlog 值决定。如果应用层调用 <code>accept</code> 不及时，新建立的连接会在 completely established sockets queue 堆积，当 completely established queue 满了之后，如果 net.ipv4.tcp_abort_on_overflow 置位，Server 无法将新创建的连接放入 completely established queue，在收到 Client 在 TCP 握手阶段最后一个 ACK 时会直接给 Client 回复 RST 关闭整条连接；如果 net.ipv4.tcp_abort_on_overflow 为 0 (默认为 0)，则队列 completely established queue 满了之后 Server 不做任何事情，直接丢弃 Client 发来的 ACK，连接依然会放在 incomplete socket queue 中。</p>
<p>此时在 Client 看来它并不知道 ACK 被 Server 忽略了所以认为连接已经处在 Established 状态，但是连接实际是 Half-Open 的。<strong>需要注意的是 ACK 并没有重传机制，Client 发完 ACK 到 Server 后不会等着这个 ACK 被 Server ACK，而是会直接尝试发送实际数据给服务端</strong>。服务端收到数据后会将数据丢弃并重发 SYN/ACK。Client 收到 SYN/ACK 会再次回复 ACK 和重发应用层数据，如果 complete established sockets queue 一直是满的，Server 会一直这么丢弃 Client 的 ACK 并在每次收到 Client 数据时候回复 SYN/ACK 直到回复 SYN/ACK 的次数达到 net.ipv4.tcp_synack_retries 上限，接下来 Server 收到 Client 数据就不回复 SYN/ACK 而是直接回复 RST 重置整个连接。</p>
<p>除了 Client 在发实际数据到 Server 时 Server 会重发 SYN/ACK 之外，如果此时 Server 没有开启 net.ipv4.tcp_syncookies，Server 会有个超时时间超时后会重发 SYN/ACK，开启 SYN Cookies 之后 Server不会有这个超时重发，只有在收到 Client 数据的时候才会重发 SYN/ACK。这个下面会再次提及。</p>
<p>在 completely established sockets queue 满了之后，系统还会减慢接收新 Client 发来 SYN 的速度，也就是说即使 incomplete sockets queue 有空间，当 completely established sockets queue 满了之后，系统在收到 SYN 之后也会开始<strong>按一定比率丢弃 SYN</strong> 而不是回复 SYN/ACK 并将连接放入 incomplete sockets queue。也就是说 completely established queue 的长度是会影响到 incomplete sockets queue 的。下文会说 completely established sockets queue 的长度主要由应用调用系统调用 <code>listen</code> 时设置的 backlog 和系统的 net.core.somaxconn 参数决定，有人认为 incompletely socket queue 的长度是 backlog、net.core.somaxconn、net.ipv4.tcp_max_syn_backlog 三个参数中的最小值，实际这个倒不一定，completely established sockets queue 长度只是会影响 incomplete sockets queue 长度，但不是说 incomplete sockets queue 最长就是和 completely established sockets queue 一样。</p>
<p>Linux 上 <code>listen</code> 系统调用上有关于 backlog 的说明：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">The  behavior  of  the backlog argument on TCP sockets changed with Linux 2.2.  Now it specifies the queue length for completely established sockets waiting to be accepted, instead of the number of incomplete connection requests.  The maximum length of the queue for incomplete sockets can be set using /proc/sys/net/ipv4/tcp_max_syn_backlog.  When syncookies are enabled  there  is  no  logical maximum length and this setting is ignored.  See tcp(7) for more information.</div><div class="line"></div><div class="line">If  the  backlog  argument  is greater than the value in /proc/sys/net/core/somaxconn, then it is silently truncated to that value; the default value in this file is 128.  In kernels before 2.4.25, this limit was a hard coded value, SOMAXCONN, with the value 128.</div></pre></td></tr></table></figure>
<p>需要着重关注第二段，listen 传入的 backlog 值会受到系统参数：net.core.somaxconn 的影响，如果 net.core.somaxconn 设置的小，调用 <code>listen</code> 时传入的 backlog 再大都没有用，都会默认使用系统的 net.core.somaxconn 值。大名鼎鼎的 Netty 默认的 backlog 值就是读取的系统 somaxconn 参数，<a href="https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/NetUtil.java#L250" target="_blank" rel="external">参看这里</a>。</p>
<p>从上面描述能看出来这个 backlog 值对会在短时间内建立大量连接的服务很重要，如果 backlog 非常小，服务很可能还没来的急执行 <code>accept</code> 呢 backlog 就满了，于是开始丢弃 Client 发来的 ACK，并且在收到 Client 数据包或者 Server 超时时重发 SYN/ACK。因为 Server 收到 Client 数据包后会直接丢弃数据，所以会给网络带来不必要的开销。另外 Server 超时重发 SYN/ACK 的话等待时间会很长，并且重传是 exponential backoff 的，等待时间会越来越长。</p>
<p>不过这两个开销还不是最主要的，网络开销也就是影响一些带宽，Server 的超时一般不存在因为一般会开启 SYN Cookies，最重要的影响是上面说过 completely established sockets queue 满了之后 Server 会按一定比率丢弃 Client 发来的 SYN，Client 不知道发出去的 SYN 被丢弃了，必须等足一个超时时间之后才会再次发送 SYN，一般这个超时时间是 3s，并且是 exponential backoff 的，第二次发 SYN 如果还被 Server 丢弃就要等足 9s 才会再次重发 SYN，这个时间长度就很讨厌了，在用户那里直接感受就是服务好慢，连半天连不上。<a href="https://vimeo.com/70369211" target="_blank" rel="external">这篇文章里</a>说的 Backlog 小了之后在有大量并发连接的时候 Client 会出现 3s，9s 的延迟就是因为这个原因。 如果连接已经进入 incomplete sockets queue，Client 只要发数据上来服务端就会立即重传 SYN/ACK，所以不会直接产生延迟。</p>
<h2 id="Backlog-修改方法"><a href="#Backlog-修改方法" class="headerlink" title="Backlog 修改方法"></a>Backlog 修改方法</h2><p>如果服务执行能力足够，稍微大一点的 backlog 值是有助于提高系统建立并发连接能力的。但很不幸的是，默认情况下 net.core.somaxconn 的值非常的小，只有 128，在高并发系统下很可能会让 Client 超时，很多人建议是调整到 1024 或者 2048，根据需要和测试情况也有调整到 4096 的。一般建议是将 net.core.somaxconn 设置的稍微大一些，将其作为上限来设置，这样应用可以根据自己需要在执行 listen 的时候将这个值调小。</p>
<p>另外需要说明的是，backlog 最大值是 65535，这个上限是内核规定的但没有文档明确说明。并且 backlog 值并不是越大越好，系统维护的建立连接的两个队列是有资源消耗的，一个是会吃一些内存，一般说是一个 entry 是 64 字节内存；另一个是会消耗 CPU，排在 completely established sockets queue 的连接都是合法的已经完全建立的连接，随时都可能有数据发上来，数据发上来后 Server 就要消耗 CPU 做处理即使这个连接还没被应用层 <code>accept</code>。</p>
<p>除了消耗资源之外，backlog 如果特别大超过应用处理能力，应用要很久才能把 backlog 清空，那这个时间 Client 可能已经超时，甚至认为连接已经失效而执行了 TCP 断开连接的流程，这个时候 Server 处理完 backlog 再想往连接写数据就写不下去了，可能报 Broken pipe。相当于 Server 费半天劲在做无用功，Client 超时后如果还有重试机制会加重 Server 负担恶性循环。</p>
<p>临时修改的话执行:<br>sysctl -w net.core.somaxconn=2048<br>sysctl -w net.ipv4.tcp_syncookies=1<br>sysctl -w net.ipv4.tcp_max_syn_backlog=65535</p>
<p>永久性修改的话需要修改 /etc/sysctl.conf 文件，将上面修改值写在文件中。</p>
<h2 id="怎么看出来服务的-backlog-设置太小了？"><a href="#怎么看出来服务的-backlog-设置太小了？" class="headerlink" title="怎么看出来服务的 backlog 设置太小了？"></a>怎么看出来服务的 backlog 设置太小了？</h2><p>netstat -s 是个神器，搞连接参数相关优化的时候这个能提供不少帮助。它实际读的是 /proc/net/netstat 这个文件，里面记录着系统内各种和网络相关的统计信息。跟 backlog 相关的是两条：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">167480 times the listen queue of a socket overflowed</div><div class="line">258209 SYNs to LISTEN sockets dropped</div></pre></td></tr></table></figure></p>
<p>这两个数据分别对应着内核的 LINUX_MIB_LISTENOVERFLOWS 和 LINUX_MIB_LISTENDROPS 两个统计信息。一个还挺好用的看内核代码的地方是：<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/tcp_input.c#L6306" target="_blank" rel="external">linux/net/ipv4/tcp_input.c - Elixir - Free Electrons</a>，可以搜索一下看看都是什么地方在更新这两个统计信息。</p>
<p>目前来看 LINUX_MIB_LISTENOVERFLOWS 都是因为 completely established sockets queue 满了丢弃 ACK 数据时会记录。LINUX_MIB_LISTENDROPS 被记录的地方太多了，有太多地方会出现 SYN 被丢弃，比如 SYN 本身格式不对，所以 LINUX_MIB_LISTENDROPS 大多数时候是大于 LINUX_MIB_LISTENOVERFLOWS 的。在网上看到很多人分析应用层 Backlog 问题的时候看的是 LINUX_MIB_LISTENDROPS 这个记录，这个是不对的，得看 LINUX_MIB_LISTENOVERFLOWS 这个。 </p>
<p>能确认的跟 backlog 相关，会计入 LINUX_MIB_LISTENDROPS  的是下面这些地方：</p>
<ol>
<li>completely established queue 满了导致 SYN 被丢弃时;</li>
<li>incomplete sockets queue 满了导致 SYN 被丢弃时；</li>
</ol>
<p>所以当 netstat -s 中 XXX the listen queue of a socket overflowed 值比较大的时候就很有可能是 backlog 不合适，或者至少说明应用层没有来得及处理大量的并发连接而导致这些连接 TCP 握手时的 ACK 被丢弃了。</p>
<h2 id="net-ipv4-tcp-syncookies-是干什么的？"><a href="#net-ipv4-tcp-syncookies-是干什么的？" class="headerlink" title="net.ipv4.tcp_syncookies 是干什么的？"></a>net.ipv4.tcp_syncookies 是干什么的？</h2><p>前面说过 incomplete sockets queue 满了之后默认行为是丢弃 Client 发来的 SYN，这就给不法分子提供了一条进行恶意攻击的途径，参看： <a href="https://en.wikipedia.org/wiki/SYN_flood" target="_blank" rel="external">SYN flood - Wikipedia</a>。</p>
<p>SYN Cookie 就是用来应对 SYN flood 的，net.ipv4.tcp_syncookies 是开启 SYN Cookie 的配置，Linux 系统会默认开启这个配置。SYN Cookie 相关内容参看这里：<a href="https://en.wikipedia.org/wiki/SYN_cookies" target="_blank" rel="external">SYN cookies - Wikipedia</a>、<a href="http://www.tcpipguide.com/free/t_TCPMaximumSegmentSizeMSSandRelationshiptoIPDatagra.htm" target="_blank" rel="external">MSS</a>。<strong>这个 wiki 里介绍的针对 SYN Cookie 的攻击挺神奇的，可以看看</strong>。</p>
<p>基本原理就是 Server 收到 SYN 在构造 SYN/ACK 的时候，会将 Client SYN 内的一些本来应该存在 Server 的 incomplete sockets queue 内的信息编码到 SYN/ACK 的 Sequence Number 里，这样 Server 就能完全废弃 incomplete socket queue 而在 Client 回复 ACK 的时候从 ACK 的 Sequence Number 中恢复 Client 在 SYN 中的信息，从而正常建立连接。SYN flood 时候攻击者一般只是发 SYN 而不会在收到 Server 的 SYN/ACK 的时候回复 ACK，所以 SYN Cookie 机制相当于减少了 Server 为每个收到的 SYN 保留信息的开销，并能区分出攻击者和普通用户，从而能解决 SYN flood 问题。</p>
<p>SYN Cookie 的成本在上面 wiki 页面也有详细描述，主要是 SYN 中会有一些 Option 字段，不能完全编码到 Sequence Number 中，一个比较关键的就是 <a href="https://en.wikipedia.org/wiki/Maximum_segment_size" target="_blank" rel="external">Maximum segment size - Wikipedia</a>。Sequence Number 中只给 MSS 留了三个 bit 的位置做编码，所以开启 SYN Cookie 之后 Server 支持的 MSS 最多只有 8 个。内核中有个叫做 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/ipv4/syncookies.c#L167" target="_blank" rel="external">msstab 的表</a>，记录了系统内在开启 SYN Cookie 之后支持的 MSS 值。有人专门做过研究确定下来几个固定值写在 msstab 中，据说基本能覆盖绝大多数情况，所以一般认为虽然 MSS 选择少了很多但能避免 SYN flood 还是很值得的，所以 SYN Cookie 默认为开启。只有对于 MSS 特别大的网络(因为一般内核默认的 MSS 为了覆盖大多数情况设置的值都比较小，基本都在 1500 以下)开启了 SYN Cookie 之后会导致本来网络能发特别大的数据包但因为 MSS 限制而不能发，因为每个数据包都会有 TCP Header、IP Header 等信息使得网络的 overhead 升高。</p>
<p>在前文中说过，当 completely established sockets queue 满了之后，如果进程来不及执行 accept，收到 Client 的 ACK 后连接不会从 incomplete socket queue 中移除，而是会丢弃 ACK。正常来说 Server 回复的 SYN/ACK 是有超时机制的，在丢弃了 Client 发来的 ACK 之后会等 SYN/ACK 超时再次发送 SYN/ACK 给 Client，但是开启 SYN Cookies 之后，incomplete sockets queue 被完全废弃，Server 在收到Client 的 SYN 并回复 SYN/ACK 之后就把这个 Client 完全忘记了，所以 SYN Cookies 开启后是不会有 Server 的 SYN/ACK 超时的，必须等待 Client 主动发数据到 Server 后 Server 才会重发 SYN/ACK。对于有些应用协议如果期待 TCP 握手之后 Server 先发个数据到 Client 的话，会需要额外的超时机制去让 Client 知道自己发的 ACK 丢失了。</p>
<p>对 MSS 再多记录一些东西吧，TCP 的 Option 格式 <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol#TCP_segment_structure" target="_blank" rel="external">在这里有说明</a>，每个 Option 都可能有三个 field，Option-Kind、Option-Length、Option-Data。Option-Kind 就是说这个 Option 是什么 Option，Option-Length 就是这个 Option 总共占了多少字节，Option-Data 就是 Option 的值是什么。拿 MSS 来说，比如抓包得到 TCP Option 如下，Option 全长 20 字节：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">0000  02 04 05 84 04 02 08 0a  6b 73 a5 ca 3b 43 02 f7   ........ ks..;C..</div><div class="line">0010  01 03 03 07                                        ....</div></pre></td></tr></table></figure></p>
<p>02 是 Option-Kind 表示 MSS 这个 Option，04 表示 Option-Length 是四字节，也就是说 MSS 这个 Option 就是上面前四个字节 02 04 05 84，整个 Option 是 4 字节，Option-Kind 和 Option-Length 各占一个字节，05 84 就是 Option-Data 在十进制下值为 1412 。</p>
<h2 id="SYN-Cookies-相关统计"><a href="#SYN-Cookies-相关统计" class="headerlink" title="SYN Cookies 相关统计"></a>SYN Cookies 相关统计</h2><p>netstat -s 中有三个东西跟 SYN Cookies 相关：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">909660620 SYN cookies sent</div><div class="line">867502891 SYN cookies received</div><div class="line">635627953 invalid SYN cookies received</div></pre></td></tr></table></figure></p>
<p>从字面意思也能大概理解这三个东西是干什么的。<a href="https://en.wikipedia.org/wiki/SYN_cookies" target="_blank" rel="external">SYN cookies - Wikipedia</a> 这里介绍过一个通过生成随机的 Sequence Number 伪造 Client 的 ACK 从而连上 Server 未开启的端口，但这个攻击会产生大量的 invalid SYN cookies received 所以如果这个值短时间内大量增加有可能是正在遭受攻击。</p>
<p>监控 SYN cookies sent 和 SYN cookies received 值能提前判断是否遭受 SYN flood 攻击，正常来说 Sent 和 Received 是差别不大的，用户收到 SYN/ACK 之后大部分时间都会回应 ACK，而 SYN flood 攻击时攻击者为了不建立连接从而在本机产生消耗所以不会回复 ACK，从而出现服务端在遭受 SYN flood 时，SYN cookies sent 短时间内大量增加但是 SYN cookies received 变化幅度不大的现象。</p>
<h2 id="一些参考"><a href="#一些参考" class="headerlink" title="一些参考"></a>一些参考</h2><p>强力推荐 <a href="http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html" target="_blank" rel="external">How TCP backlog works in Linux</a><br>强力推荐 <a href="https://vimeo.com/70369211" target="_blank" rel="external">https://vimeo.com/70369211</a><br><a href="https://voipmagazine.wordpress.com/tag/tcp_max_syn_backlog/" target="_blank" rel="external">tcp_max_syn_backlog | VOIP Magazine</a><br><a href="https://serverfault.com/questions/518862/will-increasing-net-core-somaxconn-make-a-difference" target="_blank" rel="external">linux - Will increasing net.core.somaxconn make a difference? - Server Fault</a><br><a href="http://engineering.chartbeat.com/2014/01/02/part-1-lessons-learned-tuning-tcp-and-nginx-in-ec2/" target="_blank" rel="external">Part 1: Lessons learned tuning TCP and Nginx in EC2 «  Chartbeat Engineering Blog</a><br><a href="http://engineering.chartbeat.com/2014/02/12/part-2-lessons-learned-tuning-tcp-and-nginx-in-ec2/" target="_blank" rel="external">Part 2: Lessons learned tuning TCP and Nginx in EC2 «  Chartbeat Engineering Blog</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章对 TCP Backlog 的说明写的特别好，&lt;a href=&quot;http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html&quot; target=&quot;_blank&quot; rel=&quot;externa
    
    </summary>
    
    
      <category term="Network" scheme="http://ylgrgyq.github.io/tags/Network/"/>
    
      <category term="TCP" scheme="http://ylgrgyq.github.io/tags/TCP/"/>
    
  </entry>
  
  <entry>
    <title>一次 TLS SNI 问题</title>
    <link href="http://ylgrgyq.github.io/2017/04/14/tls-sni/"/>
    <id>http://ylgrgyq.github.io/2017/04/14/tls-sni/</id>
    <published>2017-04-14T01:28:46.000Z</published>
    <updated>2017-04-21T05:38:24.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一个遇到的问题"><a href="#一个遇到的问题" class="headerlink" title="一个遇到的问题"></a>一个遇到的问题</h1><p>LeanCloud 的实时通信服务能实现类似客服机器人的功能，用户能自己提供一个 Web Hook 地址，有消息发给机器人的时候实时通信服务会将消息发到这个 Web Hook 上，用户从 Web Hook 收到消息之后能对消息进行解析和处理，构造出客服机器人的回答，再通过 REST API 发还给用户。从而用户能实现客服机器人自动应答功能。</p>
<p>前些天，有个用户反馈说 Web Hook 失效了，一直没有消息发过去。查看之下发现报类似这个样子的错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">javax.net.ssl.SSLException: hostname in certificate didn&apos;t match: &lt;expectedhost&gt; != &lt;defaulthost&gt;</div><div class="line">at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:220)</div></pre></td></tr></table></figure>
<p>看上去就是用户的 SSL 证书限定的 Host Name 和用户填的 Web Hook 域名不符，导致 SSL 握手的时候我们这边对证书校验失败。</p>
<p>于是我去检查用户证书：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">openssl s_client -connect target.host.name:443</div></pre></td></tr></table></figure>
<p>执行下来显示:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">CONNECTED(00000003)</div><div class="line">depth=2 C = US, O = &quot;VeriSign, Inc.&quot;, OU = VeriSign Trust Network, OU = &quot;(c) 2006 VeriSign, Inc. - For authorized use only&quot;, CN = VeriSign Class 3 Public Primary Certification Authority - G5</div><div class="line">verify return:1</div><div class="line">depth=1 C = US, O = Symantec Corporation, OU = Symantec Trust Network, CN = Symantec Class 3 Secure Server CA - G4</div><div class="line">verify return:1</div><div class="line">depth=0 C = CN, ST = guangdong, L = shenzhen, O = Shenzhen Tencent Computer Systems Company Limited, OU = R&amp;D, CN = *.cdn.myqcloud.com</div><div class="line">verify return:1</div><div class="line">---</div><div class="line">.......</div></pre></td></tr></table></figure>
<p>看到这个证书是颁发给一个叫做 cdn.myqcloud.com 的，而不是用户的域名。从这个域名中能看出一个是证书属于腾讯云，另一个就是这是 CDN 的证书。说明这个用户将 CDN 托管给了腾讯云。此时开始怀疑是因为我们在发请求调用用户 Web Hook 的时候 SSL 握手没有带着 <a href="https://en.wikipedia.org/wiki/Server_Name_Indication" target="_blank" rel="external">SNI</a>。于是执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">openssl s_client -connect target.host.name:443 -showcerts -servername target.host.name</div></pre></td></tr></table></figure>
<p>此时看到了用户真正的证书:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">CONNECTED(00000003)</div><div class="line">depth=2 O = Digital Signature Trust Co., CN = DST Root CA X3</div><div class="line">verify return:1</div><div class="line">depth=1 C = US, O = Let&apos;s Encrypt, CN = Let&apos;s Encrypt Authority X3</div><div class="line">verify return:1</div><div class="line">depth=0 CN = target.host.name</div><div class="line">verify return:1</div><div class="line">........</div></pre></td></tr></table></figure>
<p>从而确认是我们在调用用户 Web Hook 时候 SSL 握手一定是没有带着 SNI 导致握手时拿到的不是用户真实的证书，拿的是用户 CDN 的证书，最终导致握手失败。</p>
<h1 id="证实服务确实是不支持-SNI"><a href="#证实服务确实是不支持-SNI" class="headerlink" title="证实服务确实是不支持 SNI"></a>证实服务确实是不支持 SNI</h1><p>首先需要说的是，<a href="http://docs.oracle.com/javase/7/docs/technotes/guides/security/enhancements-7.html" target="_blank" rel="external">从 JDK 是从 1.7 开始才真正支持 SNI</a>，也就是说还在使用 1.6 版本 JDK 的话是无论如何都无法使用 SNI 的。</p>
<p>而 <a href="https://hc.apache.org/httpcomponents-client-ga/" target="_blank" rel="external">HttpClient</a> 是从 4.3.2 开始支持 SNI 的。即使你使用的是 JDK 1.7 或更新版本的 JDK，但还是使用 4.3.2 以前的 HttpClient 的话，也是无法使用 SNI 的。</p>
<p>我们调用 Web Hook 的服务使用的是 clj-http 0.7.8，这个版本的 clj-http 刚好使用的是 4.3.1 的 HttpClient，所以才有了上面说的调用 Web Hook 进行 SSL 握手时没有带着 SNI 导致拿到错误证书。</p>
<p>为了证实 clj-http 0.7.8 在请求 https 服务时，SSL 握手没有带着 SNI ，首先需要添加 JVM 参数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">-Djavax.net.debug=all</div></pre></td></tr></table></figure>
<p>这个参数在调试 SSL 握手相关问题时非常有用，能把完整的握手过程，使用的证书等都打印出来。测试就是随意发了个 POST 请求到 https:leancloud.cn 在打印出来的 ClientHello 阶段有如下信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">*** ClientHello, TLSv1.2</div><div class="line">RandomCookie:  GMT: 1475193456 bytes = &#123; 187, 13, 85, ...... &#125;</div><div class="line">Session ID:  &#123;&#125;</div><div class="line">Cipher Suites: [TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, .......]</div><div class="line">Compression Methods:  &#123; 0 &#125;</div><div class="line">Extension elliptic_curves, curve names: &#123;secp256r1, .......&#125;</div><div class="line">Extension ec_point_formats, formats: [uncompressed]</div><div class="line">Extension signature_algorithms, signature_algorithms: SHA512withECDSA, ......</div></pre></td></tr></table></figure>
<p>上面内容通过 tcpdump 抓包也能得到，但如果能增加 -Djavax.net.debug=all 这个配置的话还是打印出来会更方便一点。主要是看到上面 Extension 只有三行内容，少了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Extension server_name, server_name: [type=host_name (0), value=leancloud.cn]</div></pre></td></tr></table></figure>
<p>如果支持 SNI 的话是一定会打印上面这个 Extension 信息的。从而证实 clj-http 0.7.8 确实是不支持 SNI 的。</p>
<p>那是不是将 clj-http 升级到最新版，HttpClient 也使用最新版就可以了呢？</p>
<p>还不行。目前 HttpClient 对 SNI 的支持并不是向前兼容的，而是提供了一套新的 API 让用户使用。想要使用 SNI 就必须调用新的 HttpClient 的 API。clj-http 从 0.7.8 直到最新的发布版 2.3.0 都还在使用 HttpClient 老版本的 API，只有更新一些的还在开发中的 3.4.1 才真正切换到了新的 API。</p>
<p>这里就有疑问了，为什么 JDK 支持了 SNI，HttpClient 还得靠增加一套 API 来支持 SNI 呢？</p>
<h1 id="JDK-对-SNI-的支持"><a href="#JDK-对-SNI-的支持" class="headerlink" title="JDK 对 SNI 的支持"></a>JDK 对 SNI 的支持</h1><p>为了解开疑问，先来看看 JDK 是怎么支持 SNI 的。JDK 要创建 SSL 的 Socket 需要使用 javax.net.ssl.SSLSocketFactory。SSLSocketFactory 提供了几种构造 Socket 的方式：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function">Socket <span class="title">createSocket</span><span class="params">()</span></span></div><div class="line">Socket <span class="title">createSocket</span><span class="params">(String host, <span class="keyword">int</span> port)</span></div><div class="line">Socket <span class="title">createSocket</span><span class="params">(String host, <span class="keyword">int</span> port, InetAddress localhost, <span class="keyword">int</span> localPort)</span> <span class="keyword">throws</span> IOException, UnknownHostException;</div><div class="line"><span class="function">Socket <span class="title">createSocket</span><span class="params">(InetAddress address, <span class="keyword">int</span> port)</span></span></div><div class="line">Socket <span class="title">createSocket</span><span class="params">(InetAddress address, <span class="keyword">int</span> port, InetAddress remoteHost, <span class="keyword">int</span> remotePort)</span></div><div class="line">Socket <span class="title">createSocket</span><span class="params">(Socket socket, String host, <span class="keyword">int</span> port, <span class="keyword">boolean</span> autoClose)</span></div></pre></td></tr></table></figure>
<p>需要注意：</p>
<ol>
<li>有 host 参数的都会在创建 Socket 的时候自动连接 host</li>
<li>只有直接以 String 传递 host 的方式才会在握手中使用 SNI，以 InetAddress 传递 host 的方式握手时都不会带着 SNI</li>
<li>第 6 行的 createSocket 很特殊，是传入一个 Socket (已连接或未连接)，然后建立一个新的 Socket layered over 原来的 Socket，如果原来的 Socket 没有建立连接，则在创建后会立即连接 host</li>
</ol>
<p>第二条很关键，但在 JDK 文档上竟然完全没有说明。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">javax.net.ssl.SSLSocketFactory socketfactory = (javax.net.ssl.SSLSocketFactory)javax.net.ssl.SSLSocketFactory.getDefault();</div><div class="line">SSLSocket sock;</div><div class="line"></div><div class="line"><span class="comment">// 握手不会带着 SNI</span></div><div class="line">sock = (SSLSocket)socketfactory.createSocket();</div><div class="line">sock.connect(<span class="keyword">new</span> InetSocketAddress(<span class="string">"leancloud.cn"</span>, <span class="number">443</span>));</div><div class="line">sock.startHandshake();</div><div class="line"></div><div class="line"><span class="comment">// 握手不会带着 SNI</span></div><div class="line">sock = (SSLSocket)socketfactory.createSocket(InetAddress.getByName(<span class="string">"leancloud.cn"</span>), <span class="number">443</span>);</div><div class="line">sock.startHandshake();</div><div class="line"></div><div class="line"><span class="comment">// 握手会带着 SNI</span></div><div class="line">sock = (SSLSocket)socketfactory.createSocket(<span class="string">"leancloud.cn"</span>, <span class="number">443</span>);</div><div class="line">sock.startHandshake();</div><div class="line"></div><div class="line"><span class="comment">// 握手会带着 SNI</span></div><div class="line">Socket plainSocket = SocketFactory.getDefault().createSocket();</div><div class="line"><span class="comment">// plainSocket 可以先执行 connect，并且这里可以传递 InetSocketAddress</span></div><div class="line"><span class="comment">// 只要 Layered Socket 创建时用的传 String 的 createSocket 即可</span></div><div class="line">plainSocket.connect(<span class="keyword">new</span> InetSocketAddress(<span class="string">"leancloud.cn"</span>, <span class="number">443</span>), <span class="number">30</span>);</div><div class="line"><span class="comment">// 因为 plainSocket 已经建立连接，所以这里传递 String 的 Host 只是为了将其填入 SNI</span></div><div class="line">sock = socketfactory.createSocket(plainSocket, <span class="string">"leancloud.cn"</span>, <span class="number">443</span>, <span class="keyword">true</span>);</div><div class="line">sock.startHandshake();</div></pre></td></tr></table></figure>
<p>从这里也能看出来是否使用 SNI 创建连接藏的很隐晦。据说 JDK 不允许传递 InetAddress 的 createSocket 创建出来的 SSLSocket 在 SSL 握手时自动使用 SNI，是因为 InetAddress 构造的时候支持 getByName 函数，该函数可以传个 IP 而不是 Host。这种情况下用户真传个 IP 进来再允许开启 SNI 将这个 IP 放入 SNI 中就不符合 SNI 使用条件了，因为 SNI 只能填 Host Name。不过感觉理由还是比较牵强，总之就是这个 API 设计的有些诡异，藏得有点深。</p>
<h1 id="HttpClient-对-SNI-的支持"><a href="#HttpClient-对-SNI-的支持" class="headerlink" title="HttpClient 对 SNI 的支持"></a>HttpClient 对 SNI 的支持</h1><p>为了了解缘由需要看一下这个 <a href="https://issues.apache.org/jira/browse/HTTPCLIENT-1119" target="_blank" rel="external">JIRA 讨论</a>。</p>
<p>注意：以下内容基于:<br>[org.apache.httpcomponents/httpcore “4.4.5”]<br>[org.apache.httpcomponents/httpclient “4.5.2”]<br>来说。以后内部实现可能还会变化。</p>
<p>在 HttpClient 的框架中，所有 Socket 都是先调用 SocketFactory (有新旧两个版本，org.apache.http.conn.scheme.SocketFactory 和 org.apache.http.conn.socket.ConnectionSocketFactory。两个版本都有 createSocket 和 connectSocket) 的 createSocket 方法先创建 Socket，之后对构造出来的 Socket 进行配置，添加比如 SO_TIMEOUT，SO_REUSEADDR，TCP_NODELAY 等，之后再调用 SocketFactory 的 connectSocket 方法去和 remote 地址建立连接。</p>
<p>在老版本的 HttpClient 下，默认都是用 javax.net.ssl.SSLSocketFactory 无参的 createSocket 函数来创建 Socket 的。在完全不改动上层实现的情况下是无法支持 SNI 了，所以新建立了一套 API。</p>
<p>clj-http 0.7.8 翻译为直接使用 HttpClient 的代码如下，这个是不支持 SNI 的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">SchemeRegistry registry = <span class="keyword">new</span> SchemeRegistry();</div><div class="line">registry.register(<span class="keyword">new</span> Scheme(<span class="string">"http"</span>, <span class="number">80</span>, PlainSocketFactory.getSocketFactory()));</div><div class="line">SSLSocketFactory sslFac = SSLSocketFactory.getSocketFactory();</div><div class="line">sslFac.setHostnameVerifier(SSLSocketFactory.STRICT_HOSTNAME_VERIFIER);</div><div class="line">registry.register(<span class="keyword">new</span> Scheme(<span class="string">"https"</span>, <span class="number">443</span>, sslFac));</div><div class="line"></div><div class="line">BasicClientConnectionManager manager = <span class="keyword">new</span> BasicClientConnectionManager(registry);</div><div class="line"></div><div class="line">HttpPost post = <span class="keyword">new</span> HttpPost(<span class="string">"https://leancloud.cn"</span>);</div><div class="line">DefaultHttpClient httpClient = <span class="keyword">new</span> DefaultHttpClient(manager);</div><div class="line"></div><div class="line">httpClient.execute(post);</div></pre></td></tr></table></figure>
<p>clj-http 3.4.1 翻译为直接使用 HttpClient 的代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">Registry&lt;ConnectionSocketFactory&gt; registry = RegistryBuilder.&lt;ConnectionSocketFactory&gt;create()</div><div class="line">                                                            .register(<span class="string">"http"</span>, PlainConnectionSocketFactory.getSocketFactory())</div><div class="line">                                                            .register(<span class="string">"https"</span>, SSLConnectionSocketFactory.getSocketFactory())</div><div class="line">                                                            .build();</div><div class="line"></div><div class="line">BasicHttpClientConnectionManager manager = <span class="keyword">new</span> BasicHttpClientConnectionManager(registry);</div><div class="line">HttpPost post = <span class="keyword">new</span> HttpPost(<span class="string">"https://leancloud.cn"</span>);</div><div class="line"></div><div class="line">HttpClient httpClient = HttpClients.custom()</div><div class="line">                                   .setConnectionManager(manager)</div><div class="line">                                   .build();</div><div class="line"></div><div class="line">httpClient.execute(post);</div></pre></td></tr></table></figure>
<p>最关键的差别在于老的 clj-http 使用的是 SSLSocketFactory 而新的使用的是 SSLConnectionSocketFactory，再就是老版本使用的是 DefaultHttpClient，新版本使用的是 HttpClients 构造出来的 HttpClient 。</p>
<p>DefaultHttpClient 中，处理连接部分的是：org.apache.http.impl.conn.DefaultClientConnectionOperator</p>
<table>
<thead>
<tr>
<th>不同点</th>
<th>新版</th>
<th>老版</th>
</tr>
</thead>
<tbody>
<tr>
<td>创建 SSLSocket 的 Factory 不同</td>
<td>SSLConnectionSocketFactory</td>
<td>SSLSocketFactory</td>
</tr>
<tr>
<td>使用的 HttpClient 不同</td>
<td>HttpClients 构造出来的 HttpClient</td>
<td>DefaultHttpClient </td>
</tr>
<tr>
<td>HttpClient 构建连接的类不同</td>
<td>DefaultHttpClientConnectionOperator</td>
<td>DefaultClientConnectionOperator</td>
</tr>
</tbody>
</table>
<p><a href="https://github.com/apache/httpclient/blob/4.5.2/httpclient/src/main/java-deprecated/org/apache/http/impl/conn/DefaultClientConnectionOperator.java#L147" target="_blank" rel="external">DefaultClientConnectionOperator</a> 使用 SSLSocketFactory 构造 SSLSocket，<a href="https://github.com/apache/httpclient/blob/4.5.2/httpclient/src/main/java-deprecated/org/apache/http/conn/ssl/SSLSocketFactory.java#L522" target="_blank" rel="external">用的是 javax.net.ssl.SSLSocketFactory 的无参的 createSocket 构造 SSLSocket</a>，<a href="https://github.com/apache/httpclient/blob/4.5.2/httpclient/src/main/java-deprecated/org/apache/http/conn/ssl/SSLSocketFactory.java#L542" target="_blank" rel="external">并且在 SSLSocketFactory 内使用创建出来的 SSLSocket 与目标 Host 建立连接时使用的 InetAddress 方式传递目标 Host Name</a>，<a href="https://github.com/apache/httpclient/blob/4.5.2/httpclient/src/main/java-deprecated/org/apache/http/conn/ssl/SSLSocketFactory.java#L553" target="_blank" rel="external">之后再开始握手流程</a>，这就无法使用 SNI 了。</p>
<p><a href="https://github.com/apache/httpclient/blob/4.5.2/httpclient/src/main/java/org/apache/http/impl/conn/DefaultHttpClientConnectionOperator.java#L98" target="_blank" rel="external">DefaultHttpClientConnectionOperator</a> 使用的 <a href="https://github.com/apache/httpclient/blob/4.5.2/httpclient/src/main/java/org/apache/http/conn/ssl/SSLConnectionSocketFactory.java#L313" target="_blank" rel="external">SSLConnectionSocketFactory 先构造出普通的 Socket</a>，<a href="https://github.com/apache/httpclient/blob/4.5.2/httpclient/src/main/java/org/apache/http/conn/ssl/SSLConnectionSocketFactory.java#L337" target="_blank" rel="external">在 SSLConnectionSocketFactory 调用 Socket 的 connect 参数先与目标服务建立连接</a>。注意与 DefaultClientConnectionOperator 的不同，DefaultClientConnectionOperator 在调用 SSLSocketFactory 的 connectSocket 时传入的 Socket 就是 SSLSocket，而 DefaultHttpClientConnectionOperator 在调用 SSLConnectionSocketFactory 的 connectSocket 时传入的 socket 只是普通的 Socket。在这个普通的 Socket 与 remote host 建立连接之后，通过调用  SSLConnectionSocketFactory 内 <a href="https://github.com/apache/httpclient/blob/4.5.2/httpclient/src/main/java/org/apache/http/conn/ssl/SSLConnectionSocketFactory.java#L353" target="_blank" rel="external">createLayeredSocket 在普通 Socket 之上</a>调用 javax.net.ssl.SSLSocketFactory 的传递 Socket 和普通 String 形式 Host Name 的 createSocket 函数构造出 SSLSocket，之后开始握手流程就能使用 SNI 了。</p>
]]></content>
    
    <summary type="html">
    
      工作时遇到的一个涉及 SNI 的问题
    
    </summary>
    
    
      <category term="Bug" scheme="http://ylgrgyq.github.io/tags/Bug/"/>
    
      <category term="Java" scheme="http://ylgrgyq.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>普通队列改造为并发队列</title>
    <link href="http://ylgrgyq.github.io/2017/04/01/basic-concurrent-queue/"/>
    <id>http://ylgrgyq.github.io/2017/04/01/basic-concurrent-queue/</id>
    <published>2017-04-01T13:21:43.000Z</published>
    <updated>2017-04-21T05:39:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>起初是看<a href="http://www.drdobbs.com/parallel/writing-a-generalized-concurrent-queue/211601363" target="_blank" rel="external">这篇文章</a>写的挺好的，介绍了无锁队列的实现。按照它的说法我们来实现一下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">public class ConcurrentListQueue&lt;T&gt; &#123;</div><div class="line">  private Node&lt;T&gt; head, tail;</div><div class="line">  private AtomicInteger producerLock, consumerLock;</div><div class="line">  private static class Node&lt;T&gt; &#123;</div><div class="line">      private T data;</div><div class="line">      private Node&lt;T&gt; next;</div><div class="line"></div><div class="line">      private Node(T d) &#123;</div><div class="line">          this.data = d;</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  public void add(T data) &#123;</div><div class="line">      Node&lt;T&gt; n = new Node&lt;&gt;(data);</div><div class="line"></div><div class="line">      while (!producerLock.compareAndSet(0, 1)) &#123;</div><div class="line">      &#125;</div><div class="line">    </div><div class="line">      tail.next = n;</div><div class="line">      tail = n;</div><div class="line">      producerLock.set(0);</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  public T poll() &#123;</div><div class="line">      T d = null;</div><div class="line">      while (!consumerLock.compareAndSet(0, 1)) &#123;</div><div class="line">      &#125;</div><div class="line">    </div><div class="line">      Node&lt;T&gt; h = head;</div><div class="line">      Node&lt;T&gt; next = h.next;</div><div class="line">      if (next != null) &#123;</div><div class="line">          d = next.data;</div><div class="line">          next.data = null;</div><div class="line">          head = next;</div><div class="line">          h.next = null;</div><div class="line">      &#125;</div><div class="line">    </div><div class="line">      consumerLock.set(0);</div><div class="line">      return d;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里需要注意 Producer 只能访问 tail，而 Consumer 只能访问 head，不然无法做到 Producer 和 Consumer 相互不竞争。很多队列的实现会使用一个固定的哑元做 Head，这个哑元从始至终是不变的，每次出队只是修改哑元的 next 引用，例如单线程版的 poll 可以实现成：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">public void poll() &#123;</div><div class="line">    if (head != tail) &#123;</div><div class="line">        Node&lt;T&gt; next = head.next;</div><div class="line">        head.next = next.next;</div><div class="line">        if (next == tail) &#123;</div><div class="line">            tail = head;</div><div class="line">        &#125;</div><div class="line">        next.data = null;</div><div class="line">        next.next = null;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这种实现下通过加锁改成并发队列，因为当出队后队列为空时由于需要调整 tail 引用指向 Head 哑元，所以在 poll 的时候也访问了 tail，这么一来按前述 Producer 和 Consumer 分别加锁的方式就不成立了，所以需要改成每次出队后，修改 Head 哑元变成刚出队的这个 Node。并且要将 Node 的 data 引用清空，帮助出队的数据 GC。</p>
<h1 id="不那么容易观察到的-False-Sharing"><a href="#不那么容易观察到的-False-Sharing" class="headerlink" title="不那么容易观察到的 False Sharing"></a>不那么容易观察到的 False Sharing</h1><p><a href="http://www.drdobbs.com/parallel/writing-a-generalized-concurrent-queue/211601363" target="_blank" rel="external">这个文章</a>中还提到一个问题就是 <a href="https://en.wikipedia.org/wiki/False_sharing" target="_blank" rel="external">False Sharing</a>，关于这个问题还有好多地方在做解释：比如<a href="https://dzone.com/articles/false-sharing-cache-coherence-and-the-contended-an" target="_blank" rel="external">这个</a>、<a href="https://mechanical-sympathy.blogspot.com/2011/07/false-sharing.html" target="_blank" rel="external">这个</a></p>
<p>作者是通过添加一堆无用的 padding 字段来解决 False Sharing 的，但是在上面 Java 版本实现中该怎么解决 False Sharing 问题呢？</p>
<p>直接在 producerLock 和 consumerLock 前后添加 padding 是没用的，有好两个原因。一是 JVM 会对对象内的 Field 做重新排序和内存对齐，producerLock 和 consumerLock 是引用，他们两之间一定不会放入 long 型的数据（引用类型放在一起，long 类型也会放在一起，但<a href="http://psy-lob-saw.blogspot.com/2013/05/know-thy-java-object-memory-layout.html" target="_blank" rel="external">两种类型不会穿插着放</a>），他们两在一起声明，在内存中很有可能还是放在一起的；二是因为这两个 lock 都是引用，从始至终都不会被并发的修改，并发修改的是他们指向的 AtomicInteger 对象内的 value 字段，所以这两个引用本身就不会产生 False Sharing 问题，为它们增加 padding 完全没有效果，还会导致对象体积变大以及 producerLock 和 consumerLock 引用不能同时放入一个 Cache Line 中导致性能反而下降(可以试一下，加了 @Contended 之后性能反而会下降。因为当线程数超过机器 CPU 核数时，一个核很可能既要执行 producer 逻辑又要执行 consumer 逻辑，只是不是同时执行。如果 producerLock 和 consumerLock 不在一个 cache line 中，那么 CPU 比如从 consumer task 切换到 producer task 的时候就不能沿用之前的 cache line 需要读取主存或下一级 Cache，所以性能就会受到影响)。</p>
<p>AtomicInteger 是 JDK 的库，我们无法修改，不可能给它增加 padding，但<a href="https://mechanical-sympathy.blogspot.com/2011/08/false-sharing-java-7.html" target="_blank" rel="external">我们能够继承它，从而改变其对象的内存布局</a>。AtomicInteger 对象布局如下：</p>
<table>
<thead>
<tr>
<th>OFFSET</th>
<th>SIZE</th>
<th>TYPE</th>
<th>DESCRIPTION</th>
<th>VALUE</th>
</tr>
</thead>
<tbody>
<tr>
<td>      0</td>
<td>4</td>
<td></td>
<td>(object header)</td>
<td>01 00 00 00 (00000001 00000000 00000000 00000000) (1)</td>
</tr>
<tr>
<td>      4</td>
<td>4</td>
<td></td>
<td>(object header)</td>
<td>00 00 00 00 (00000000 00000000 00000000 00000000) (0)</td>
</tr>
<tr>
<td>      8</td>
<td>4</td>
<td></td>
<td>(object header)</td>
<td>f2 35 00 f8 (11110010 00110101 00000000 11111000) (-134203918)</td>
</tr>
<tr>
<td>     12</td>
<td>4</td>
<td>int</td>
<td>AtomicInteger.value</td>
<td>0 </td>
</tr>
</tbody>
</table>
<p>我们继承 AtomicInteger 并添加 padding：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">public static class PaddedAtomicInteger extends AtomicInteger&#123;</div><div class="line">    // 省略构造函数</div><div class="line">    public volatile long p1, p2, p3, p4, p5, p6 = 7L;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>其内存布局如下：</p>
<table>
<thead>
<tr>
<th>OFFSET</th>
<th>SIZE</th>
<th>TYPE</th>
<th>DESCRIPTION</th>
<th>VALUE</th>
</tr>
</thead>
<tbody>
<tr>
<td>      0</td>
<td>4</td>
<td></td>
<td>(object header)</td>
<td>01 00 00 00 (00000001 00000000 00000000 00000000) (1)</td>
</tr>
<tr>
<td>      4</td>
<td>4</td>
<td></td>
<td>(object header)</td>
<td>00 00 00 00 (00000000 00000000 00000000 00000000) (0)</td>
</tr>
<tr>
<td>      8</td>
<td>4</td>
<td></td>
<td>(object header)</td>
<td>05 07 02 f8 (00000101 00000111 00000010 11111000) (-134084859)</td>
</tr>
<tr>
<td>     12</td>
<td>4</td>
<td>int</td>
<td>AtomicInteger.value</td>
<td>0</td>
</tr>
<tr>
<td>     16</td>
<td>8</td>
<td>long</td>
<td>PaddedAtomicInteger.p1</td>
<td>0</td>
</tr>
<tr>
<td>     24</td>
<td>8</td>
<td>long</td>
<td>PaddedAtomicInteger.p2</td>
<td>0</td>
</tr>
<tr>
<td>     32</td>
<td>8</td>
<td>long</td>
<td>PaddedAtomicInteger.p3</td>
<td>0</td>
</tr>
<tr>
<td>     40</td>
<td>8</td>
<td>long</td>
<td>PaddedAtomicInteger.p4</td>
<td>0</td>
</tr>
<tr>
<td>     48</td>
<td>8</td>
<td>long</td>
<td>PaddedAtomicInteger.p5</td>
<td>0</td>
</tr>
<tr>
<td>     56</td>
<td>8</td>
<td>long</td>
<td>PaddedAtomicInteger.p6</td>
<td>7</td>
</tr>
</tbody>
</table>
<p>由于 PaddedAtomicInteger 是 AtomicInteger 的子类，其对象内存布局是在 AtomicInteger 的基础上进行的，父类对象的 Field 一定在子类对象之前，所以不会受到子类对象内存布局重排序的影响。</p>
<p>使用 PaddedAtomicInteger 之后平均性能确实比使用 AtomicInteger 好一些，但不是特别明显，平均下来只快了大概 20% 左右。但有意思的是他俩最快时间基本相同，最慢时间 AtomicInteger 要高的多，并且使用 AtomicInteger 的波动更大，慢的时候是快的时候的两倍，而 PaddedAtomicInteger 波动较小。猜想原因是 consumerLock 和 producerLock 指向的对象都在 Heap 上，我测试的时候每一轮测试都会重新构造队列对象，从而重新构造 consumerLock 和 producerLock，这两个对象虽然是连续分配的但是否一定相邻，能刚好放入一个 Cache Line 并不能说的清楚，Java 也缺乏工具去查看一个对象的内存地址。如果他俩没有分配在一个 Cache Line 上，那么使用 AtomicIntger 和使用 PaddedAtomicInteger 效果一样，所以性能结果也差不多，但是当他俩刚好分配在同一个 Cache Line 上时，AtomicInteger 性能要比 PaddedAtomicInteger 性能差一倍。巧的是我使用<a href="https://mechanical-sympathy.blogspot.com/2011/08/false-sharing-java-7.html" target="_blank" rel="external">这篇文章</a>给的例子在相同机器做测试，使用 PaddedAtomicInteger 的性能也是比使用 AtomicInteger 好一倍。</p>
<p>不过从平均性能上看这两者差别较小，并且在正常使用中由于 GC 的影响也许会让 PaddedAtomicInteger 的优势更不容易发现。</p>
<h1 id="非常容易观察到的-False-Sharing"><a href="#非常容易观察到的-False-Sharing" class="headerlink" title="非常容易观察到的 False Sharing"></a>非常容易观察到的 False Sharing</h1><p>从上面叙述也能看出来，上面 False Sharing 问题不明显的原因就是 consumerLock 和 producerLock 都是引用类型，引用的对象在 Heap，所以是否会出现 False Sharing 得看对象是否刚好分配在同一个 Cache Line 上。如果我们想进一步观察到 False Sharing，我们可以将引用对象改成基本类型，使用 sun.misc.Unsafe 的 CAS 操作来实现 Atomic 库的 CAS 操作。</p>
<p>Unsafe 的获取能参考<a href="https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/internal/PlatformDependent0.java" target="_blank" rel="external">Netty 上的代码</a>。在有了 Unsafe 之后我们能够重新声明 consumerLock 和 producerLock 将其改成 volatile 的基本类型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">private static final sun.misc.Unsafe UNSAFE;</div><div class="line">private static final long producerLockOffset;</div><div class="line">private static final long consumerLockOffset;</div><div class="line">static &#123;</div><div class="line">  UNSAFE = getUnsafe();</div><div class="line">  try &#123;</div><div class="line">      producerLockOffset = UNSAFE.objectFieldOffset</div><div class="line">              (ConcurrentListQueue.class.getDeclaredField(&quot;producerLock&quot;));</div><div class="line">      consumerLockOffset = UNSAFE.objectFieldOffset</div><div class="line">              (ConcurrentListQueue.class.getDeclaredField(&quot;consumerLock&quot;));</div><div class="line">  &#125; catch (Exception ex) &#123;</div><div class="line">      throw new Error(ex);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">private volatile int producerLock;</div><div class="line">private volatile int consumerLock;</div></pre></td></tr></table></figure>
<p>自旋锁上锁要稍微修改一下，例如 Producer 的锁加锁逻辑改为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">while (!UNSAFE.compareAndSwapInt(this, producerLockOffset, 0, 1)) &#123;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>解锁时候直接设置 producerLock 为 0 即可。</p>
<p>在上述实现下，性能非常差，比使用 PaddedAtomicInteger 时慢不止一倍。基本能确认是由 False Sharing 引起的。Java 8 之后不需要再通过 padding 的方式解决 False Sharing 问题，而是通过 <a href="https://blogs.oracle.com/dave/entry/java_contented_annotation_to_help" target="_blank" rel="external">@Contended 注解</a>解决，但该注解目前还是默认不启用的，需要主动增加配置 -XX:-RestrictContended 才会产生效果。给 producerLock 和 consumerLock 增加 @Contended 注释之后，队列性能就变得跟使用 PaddedAtomicInteger 差不多了。</p>
<h1 id="JOL-的使用"><a href="#JOL-的使用" class="headerlink" title="JOL 的使用"></a>JOL 的使用</h1><p>全名 <a href="http://openjdk.java.net/projects/code-tools/jol/" target="_blank" rel="external">Java Object Layout</a> 探索对象内存布局时非常有用。下载下来 Jar 包后，比如要看自己的某个类对象的布局：<br>java -XX:-RestrictContended -jar jol-cli-0.8-full.jar internals my.ConcurrentListQueue -cp ~/Projects/mine/my/target/classes</p>
<p>如果是看内部类对象的布局，比如看 ConcurrentListQueue 下 Node 类对象的布局因为是命令行上使用，需要对 $ 转义：<br>java -XX:-RestrictContended -jar jol-cli-0.8-full.jar internals my.ConcurrentListQueue\$Node -cp ~/Projects/mine/my/target/classes</p>
<p>当然它不止是用来看内存布局奥，还有很多别的功能，非常推荐。</p>
<h1 id="更进一步"><a href="#更进一步" class="headerlink" title="更进一步"></a>更进一步</h1><p>实际上这里使用自旋锁在竞争激烈的时候并不适合，大量的线程资源消耗在竞争上而实际任务处理时间则花费的很少。一个典型的现象就是将 Producer 或 Consumer 并发线程数降低能显著增加性能。自旋锁上也能进行一些改进，可以参看<a href="http://blog.fnil.net/blog/1df8c71d5019f4ca48c19b1707174897/" target="_blank" rel="external">这篇文章</a>。实际这里将自旋锁改成 ReentrantLock 性能能比使用自旋锁高三四倍。</p>
<p>不过无论怎么对锁进行修改，锁的存在都是对性能影响很大的，可以参看 JUC 的 ConcurrentLinkedQueue 的实现，其性能比使用 ReentrantLock 的队列还要强两到三倍。</p>
<p>所有参考都在文章链接中，不一一列出了，感谢前辈的分享。</p>
]]></content>
    
    <summary type="html">
    
      涉及并发队列实现、自旋锁、False Sharing 等问题
    
    </summary>
    
    
      <category term="Java" scheme="http://ylgrgyq.github.io/tags/Java/"/>
    
      <category term="Algorithm" scheme="http://ylgrgyq.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>不可变队列的实现</title>
    <link href="http://ylgrgyq.github.io/2017/03/27/persistent-queue/"/>
    <id>http://ylgrgyq.github.io/2017/03/27/persistent-queue/</id>
    <published>2017-03-27T05:30:52.000Z</published>
    <updated>2017-04-21T05:40:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>命令式语言中一般实现 Queue 就是用一个链表或数组做存储，之后有两个引用 Head 和 Tail 分别指向队首和队尾。一般为了方便判断队列是否为空还会引入一个哑元，Head 引用 不直接指向队列内的第一个元素而是指向这个哑元。具体细节可以参看各种数据结构相关的介绍。</p>
<p>不过在函数式语言下，各种数据结构都是不可变的，不会有个 Head、Tail 引用去指向队首队尾，并在每次有元素入队出队的时候被更新。在这种情况下该怎么实现 Queue 呢？</p>
<h1 id="两个-Stack-来实现-Queue"><a href="#两个-Stack-来实现-Queue" class="headerlink" title="两个 Stack 来实现 Queue"></a>两个 Stack 来实现 Queue</h1><p>以前遇到过这么一个面试题：怎么用两个 stack 去实现一个 queue？当时在做这个题的时候并没有想太多，只是把它当做一个思考题来做，但这个问题实际是对怎么在函数式语言下实现不可变队列有帮助的。</p>
<p>不可变队列实现的难点就是因为没有 Head、Tail 引用，用单个链表或数组在不考虑扩容的情况下，都是只能在队列的一端以 O(1) 的时间增加一个元素，但要从队列另一端取元素需要遍历整个队列导致时间复杂度变为 O(n)。这个时间复杂度并不符合我们的期待。而使用两个用链表实现的 Stack 去实现队列就能做到出入队平均时间复杂度都为 O(1)。注意是平均时间复杂度。</p>
<img src="/2017/03/27/persistent-queue/double-list.png" alt="两个 Stack 实现 Queue" title="两个 Stack 实现 Queue">
<p>如上图所示，用两个链表分别称为 Front 和 Rear。每次入队时，都将新元素添加到 Rear 链表中，这样时间为 O(1)。每次出队时，都从 Front 队列中取元素出队。如果 Front 非空，那么出队的时间复杂度为 O(1)，如果 Front 为空，则将 Rear 链表反转一下作为新的 Front 队列。之后再将新 Front 队列的队首元素出队，这个过程的时间复杂度为 O(n)。假设 Front 为空时 Rear 链表元素数量为 m，那么将 Rear 反转的时间是 O(m) 但之后 m 个元素出队时时间都是 O(1)，于是能将反转 Rear 队列的时间平摊给之后 m 次出队时间，所以平均下来出队操作的时间还是 O(1)。</p>
<p>这个过程用 Clojure 实现如下：<br><figure class="highlight clojure"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">(<span class="name"><span class="builtin-name">defrecord</span></span> Queue [front rear cnt])</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> make-queue</div><div class="line">  ([] (<span class="name">Queue.</span> '() '() <span class="number">0</span>))</div><div class="line">  ([front rear cnt]</div><div class="line">    (<span class="name">Queue.</span> front rear cnt)))</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn-</span></span> maintain [queue]</div><div class="line">  (<span class="name"><span class="builtin-name">if</span></span> (<span class="name"><span class="builtin-name">and</span></span> (<span class="name"><span class="builtin-name">empty?</span></span> (<span class="name">.-front</span> queue)) (<span class="name">not-empty</span> (<span class="name">.-rear</span> queue)))</div><div class="line">    (<span class="name">make-queue</span> (<span class="name"><span class="builtin-name">reverse</span></span> (<span class="name">.-rear</span> queue)) '() (<span class="name">.-cnt</span> queue))</div><div class="line">    queue))</div><div class="line"></div><div class="line"><span class="comment">;; maintain after every enqueue to insure front list </span></div><div class="line"><span class="comment">;; must have at least one element when queue is not empty</span></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> enqueue [queue x]</div><div class="line">  (<span class="name">maintain</span> (<span class="name">make-queue</span> (<span class="name">.-front</span> queue) (<span class="name"><span class="builtin-name">conj</span></span> (<span class="name">.-rear</span> queue) x) (<span class="name"><span class="builtin-name">inc</span></span> (<span class="name">.-cnt</span> queue)))))</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> empty-queue? [queue]</div><div class="line">  (<span class="name"><span class="builtin-name">zero?</span></span> (<span class="name">.-cnt</span> queue)))</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> peek [queue]</div><div class="line">  (<span class="name"><span class="builtin-name">when-not</span></span> (<span class="name">empty-queue?</span> queue)</div><div class="line">    (<span class="name"><span class="builtin-name">first</span></span> (<span class="name">.-front</span> queue))))</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> dequeue [queue]</div><div class="line">  (<span class="name"><span class="builtin-name">if-not</span></span> (<span class="name">empty-queue?</span> queue)</div><div class="line">    (<span class="name">maintain</span> (<span class="name">make-queue</span> (<span class="name"><span class="builtin-name">rest</span></span> (<span class="name">.-front</span> queue)) (<span class="name">.-rear</span> queue) (<span class="name"><span class="builtin-name">dec</span></span> (<span class="name">.-cnt</span> queue))))</div><div class="line">    queue))</div><div class="line"></div></pre></td></tr></table></figure></p>
<p>maintain 函数的作用主要是为了让队列非空时，Front 链表至少有一个元素存在。不然队列的 peek 操作在 Front 为空时需要遍历 Rear 链表才能找到下一个出队元素，并且由于 peek 的返回值只有下一个出队的元素，我们不能将新的队列结构返回(即反转 Rear 链表并将其作为 Front)，于是在连续执行 peek 操作时，每次 peek 都要遍历整个 Rear 表。这个是不合理的，所以我们需要保证队列非空时，Front 链表至少有一个元素存在，以供 peek 使用。</p>
<h1 id="Lazy-来实现真正-O-1-的队列"><a href="#Lazy-来实现真正-O-1-的队列" class="headerlink" title="Lazy 来实现真正 O(1) 的队列"></a>Lazy 来实现真正 O(1) 的队列</h1><p>上面实现的缺陷是显而易见的，虽然平均时间是 O(1) 但毕竟会存在某次出队的耗时为 O(n)，导致使用时每次出队操作时间有颠簸。而如果我们能借助函数式编程语言普遍支持的 Lazy 特性去实现队列，就能避免这种颠簸。</p>
<p>还是延续上面双链表的实现，最耗时的操作是反转 Rear 链表，每次执行这个操作的时间点是在 Front 队列为空的时候。如果我们不想只在这一个时间点进行 Rear 反转，想将反转过程平摊到每次出入队操作中，那么我们就需要在 Front 还未变为空的时候就执行 Rear 反转，并且每次都执行一小步，比如反转一个元素，从而保证每一个出入队操作的时间都是 O(1)。</p>
<p>但是 Front 链表非空时我们就反转 Rear 的话，反转完 Rear 时如果队列没有元素出队，那么 Front 链表是没有变动过的，那不可能像之前一样将反转后的 Rear 链表直接作为 Front 来使用，而是还需要将 Front 和反转后的 Rear 连接起来。Front 是链表，头部是队首，如下图所示，如果直接拼接的话还是要遍历 Front 链表找到 Front 链表的尾巴，再将尾巴指向反转后的 Rear 链表。</p>
<img src="/2017/03/27/persistent-queue/m-double-list.png" alt="Front 和 Rear 的拼接" title="Front 和 Rear 的拼接">
<p>如果 Rear 长度是 n，Front 长度是 m，那么反转 Rear 的操作要 n 步，连接 Front 的操作要 m 步。以连续入队操作来看，也就是说如果每次入队都执行一个 Rear 反转或者连接 Front 工作的话，连接完毕后 Rear 又会有 m + n 个元素了，我们为了每次入队都只执行一次额外操作，所以每一个入队操作都不能浪费，也就是说连接完毕之后再入队一个元素时我们又要开始下一轮反转 Rear 和连接 Front 的工作，此时 Front 长度为 m + n，Rear 长度为 m + n + 1，从而得到 Rear 反转和连接 Front 的工作每次是 Rear 比 Front 多一个元素的时候开始。</p>
<p>我们将反转 Rear 和连接 Front 两个工作称为 rotate，并且约定 rotate 执行时间为 Rear 比 Front 多一个元素时。我们得到 rotate 函数如下，注意 acc 表示的是反转 Rear 的结果，即 rotate 的反转 Rear 和连接 Front 操作最终都是构建在 acc 这个 List 上。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">// rotate 执行时必须保证 rear 比 front 多一个元素</div><div class="line">(defn rotate [front rear acc]</div><div class="line">  (lazy-seq</div><div class="line">    (if (empty? front)</div><div class="line">      // 如果 front 为空，那么 rear 只有一个元素，所以直接放入 acc</div><div class="line">      (cons (first rear) acc)</div><div class="line">      // 当 front 非空，下面解释</div><div class="line">      (cons (first front)</div><div class="line">            (rotate (rest front) (rest rear) (cons (first rear) acc))))))</div></pre></td></tr></table></figure>
<p>front 非空的情形直接这么看比较难理解，不过通过下面公式就好理解了：<br>rotate(X, Y, A) = X ++ reverse(Y) ++ A<br>                = x1 ++ (X’ ++ reverse(Y) ++ A)<br>                = x1 ++ X’ ++ reverse(Y’) ++ y1 ++ A<br>                = x1 ++ rotate(X’, Y’, y1 ++ A)</p>
<p>其中 x1 是 X 中的第一个元素，y1 是 Y 中的第一个元素。A 是用来存放 Rear 反转结果的。</p>
<p>rotate 的结果实际就是 Front，我们希望出队入队都能执行 Rear 反转或连接 Front，所以我们每次入队出队都要求 rotate 结果下一个值。所以单独拿一个叫做 rots 的 var 来存放 rotate 的结果，rots 和 front 共同指向 rotate 的结果，只是 rots 是用来每次出队入队时候求值的，front 仅仅在出队时使用，相当于 front 保存着队首元素，rots 只是为了求出整个队列，所以不需要保存队首元素。</p>
<p>使用 Clojure 实现如下：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">(<span class="name"><span class="builtin-name">defrecord</span></span> LazyQueue [front rear rots cnt])</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> make-queue</div><div class="line">  ([] (<span class="name">LazyQueue.</span> '() '() '() <span class="number">0</span>))</div><div class="line">  ([front rear rots cnt] (<span class="name">LazyQueue.</span> front rear rots cnt)))</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> rotate [front rear acc]</div><div class="line">  (<span class="name"><span class="builtin-name">lazy-seq</span></span> (<span class="name"><span class="builtin-name">if</span></span> (<span class="name"><span class="builtin-name">empty?</span></span> front)</div><div class="line">              (<span class="name"><span class="builtin-name">cons</span></span> (<span class="name"><span class="builtin-name">first</span></span> rear) acc)</div><div class="line">              (<span class="name"><span class="builtin-name">cons</span></span> (<span class="name"><span class="builtin-name">first</span></span> front)</div><div class="line">                    (<span class="name">rotate</span> (<span class="name"><span class="builtin-name">rest</span></span> front) (<span class="name"><span class="builtin-name">rest</span></span> rear) (<span class="name"><span class="builtin-name">cons</span></span> (<span class="name"><span class="builtin-name">first</span></span> rear) acc))))))</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> maintain [queue]</div><div class="line">  (<span class="name"><span class="builtin-name">if</span></span> (<span class="name"><span class="builtin-name">empty?</span></span> (<span class="name">.-rots</span> queue))</div><div class="line">    (<span class="name"><span class="builtin-name">let</span></span> [rots (<span class="name">rotate</span> (<span class="name">.-front</span> queue) (<span class="name">.-rear</span> queue) '())]</div><div class="line">      (<span class="name">make-queue</span> rots '() rots (<span class="name">.-cnt</span> queue)))</div><div class="line">    (<span class="name">make-queue</span> (<span class="name">.-front</span> queue) (<span class="name">.-rear</span> queue) (<span class="name"><span class="builtin-name">rest</span></span> (<span class="name">.-rots</span> queue)) (<span class="name">.-cnt</span> queue))))</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> enqueue [queue x]</div><div class="line">  (<span class="name">maintain</span> (<span class="name">make-queue</span> (<span class="name">.-front</span> queue) (<span class="name"><span class="builtin-name">conj</span></span> (<span class="name">.-rear</span> queue) x) (<span class="name">.-rots</span> queue) (<span class="name"><span class="builtin-name">inc</span></span> (<span class="name">.-cnt</span> queue)))))</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> empty-queue? [queue]</div><div class="line">  (<span class="name"><span class="builtin-name">zero?</span></span> (<span class="name">.-cnt</span> queue)))</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> peek-queue [queue]</div><div class="line">  (<span class="name"><span class="builtin-name">when-not</span></span> (<span class="name">empty-queue?</span> queue)</div><div class="line">    (<span class="name"><span class="builtin-name">first</span></span> (<span class="name">.-front</span> queue))))</div><div class="line"></div><div class="line">(<span class="name"><span class="builtin-name">defn</span></span> dequeue [queue]</div><div class="line">  (<span class="name">maintain</span> (<span class="name">make-queue</span> (<span class="name"><span class="builtin-name">rest</span></span> (<span class="name">.-front</span> queue)) (<span class="name">.-rear</span> queue) (<span class="name">.-rots</span> queue) (<span class="name"><span class="builtin-name">dec</span></span> (<span class="name">.-cnt</span> queue)))))</div></pre></td></tr></table></figure>
<p>那么 Clojure 中的 Queue 是怎么实现的呢？</p>
<h1 id="Clojure-中的-queue"><a href="#Clojure-中的-queue" class="headerlink" title="Clojure 中的 queue"></a>Clojure 中的 queue</h1><p>Clojure 中的 queue 一般是指 clojure.lang 中的 PersistentQueue，它使用起来可能会让人觉得有点怪，比如像这样：<br><figure class="highlight clojure"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">(<span class="name"><span class="builtin-name">let</span></span> [queue (<span class="name"><span class="builtin-name">-&gt;</span></span> (<span class="name">PersistentQueue/EMPTY</span>)</div><div class="line">                (<span class="name"><span class="builtin-name">conj</span></span> <span class="number">1</span>)</div><div class="line">                (<span class="name"><span class="builtin-name">conj</span></span> <span class="number">2</span>)</div><div class="line">                (<span class="name"><span class="builtin-name">conj</span></span> <span class="number">3</span>))]</div><div class="line">  (<span class="name"><span class="builtin-name">loop</span></span> [q queue]</div><div class="line">    (<span class="name"><span class="builtin-name">when-let</span></span> [x (<span class="name"><span class="builtin-name">peek</span></span> q)]</div><div class="line">      (<span class="name">println</span> x)</div><div class="line">      (<span class="name"><span class="builtin-name">recur</span></span> (<span class="name"><span class="builtin-name">pop</span></span> q)))))</div></pre></td></tr></table></figure><br>它出队用的 peek 和 pop，让人感觉像个 stack 一样。用 first 和 rest 在一些场合下也行，first 使用起来跟 peek 效果相同，但 rest 会将队列转换为 list，从而失去 queue 的语义，转换成 list 之后再使用 conj 会将新元素加入到 list 首部。</p>
<p>PersistentQueue 实现方式跟前面说的两个 Stack 实现 Queue 有点类似，但里面 Front 是 List，Rear 是 PersistentVector。入队函数实现如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">public PersistentQueue cons(Object o)&#123;</div><div class="line">    if(f == null)     //empty</div><div class="line">        return new PersistentQueue(meta(), cnt + 1, RT.list(o), null);</div><div class="line">    else</div><div class="line">        return new PersistentQueue(meta(), cnt + 1, f, (r != null ? r : PersistentVector.EMPTY).cons(o));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>看到入队的时候也是直接放入 Rear，并且也是只要 Queue 不空则至少有一个元素再 Front 中。</p>
<p>出队函数如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">public PersistentQueue pop()&#123;</div><div class="line">    if(f == null)  //hmmm... pop of empty queue -&gt; empty queue?</div><div class="line">        return this;</div><div class="line">    ISeq f1 = f.next();</div><div class="line">    PersistentVector r1 = r;</div><div class="line">    if(f1 == null)</div><div class="line">        &#123;</div><div class="line">        f1 = RT.seq(r);</div><div class="line">        r1 = null;</div><div class="line">        &#125;</div><div class="line">    return new PersistentQueue(meta(), cnt - 1, f1, r1);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>出队时从 Front 出队，如果出队完 Front 为空，则需要将 Rear 导入 Front。由于 Rear 是由 PersistentVector 实现的，每次添加元素是以接近 O(1) 的时间添加在 vector 末尾，所以其元素顺序和 Front 顺序相同，可以直接将 Rear 通过 seq 操作转化为 List 后作为 Front 即可，无需进行反转操作。因为 PersistentVector 能够以 O(1) 的时间访问任意一个元素，所以它只需要实现 ISeq 接口就能变成一个 List，不需要任何拷贝操作。</p>
<p>不需要反转，也不需要连接 Front (因为每次 Rear 变成 Front 时 Front 都是空的)，所以 PersistentQueue 不会出现“颠簸”，出队和入队基本都是 O(1) 的时间。</p>
<p>如此神奇的 PersistentVector 的实现可以看下<a href="http://hypirion.com/musings/understanding-persistent-vector-pt-1" target="_blank" rel="external">这篇文章</a>。</p>
]]></content>
    
    <summary type="html">
    
      函数式预言下不可变队列实现
    
    </summary>
    
    
      <category term="Clojure" scheme="http://ylgrgyq.github.io/tags/Clojure/"/>
    
  </entry>
  
  <entry>
    <title>追踪 Netty 异常占用堆外内存的经验分享</title>
    <link href="http://ylgrgyq.github.io/2016/07/17/found-a-bug-in-netty/"/>
    <id>http://ylgrgyq.github.io/2016/07/17/found-a-bug-in-netty/</id>
    <published>2016-07-17T00:21:30.000Z</published>
    <updated>2017-04-21T05:39:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文记述了定位 Netty 的一处漏洞的全过程。事情的起因是我们一个使用了 <a href="http://netty.io" target="_blank" rel="external">Netty</a> 的服务，随着运行时间的增长，其占用的堆外内存会逐步攀升，大概持续运行三四天左右，堆外内存会被全部占满，然后只能重启来解决问题。好在服务是冗余配置的，并且可以自动进行 Load Balance，所以每次重启不会带来什么损失。</p>
<p>从现象上分析，我们能确定一定是服务内部有地方出现了内存泄露。在这个出问题的服务上有大量的网络 IO 操作，为了优化性能，我们使用了 PooledByteBufAllocator 来分配 PooledDirectByteBuf。因为是堆外内存泄露，所以第一种可能就是我们在某个地方分配了内存但忘记了释放。我们仔细检查了与业务相关的 ChannelHandler 但并未发现问题，于是又将 Netty 的 io.netty.leakDetectionLevel 设置到 Advanced 级别，放在 Beta 环境上进行测试。在服务连续运行了几天并即将因内存不足再次重启之前，我们从日志中也没有发现任何由 Netty 打出来的内存泄露的报警信息。随后我们又将 io.netty.leakDetectionLevel 设置到 Paranoid 来重新测试，但依然没有发现有关 Netty 内存泄露的相关日志。</p>
<p>在排查过程中，我们也发现虽然引起服务重启的原因是堆外内存不足，但实际堆内内存也有小幅度攀升。起初我们以为这是正常现象，因为有使用 PooledByteBufAllocator，这种 Allocator 为了减少堆外内存的重复分配，会在服务内部建立一个堆外内存池，每次分配内存优先从内存池分配，只有内存池没有足够内存时候，才会去堆外分配新内存。内存池上的内存虽然在堆外，但维护内存池的数据结构却是在堆上。随着堆外内存分配的增多，内部维护内存池的数据结构也会相应增大，堆内内存也会有所升高。为了验证这个猜想，我们将 io.netty.allocator.type 设置为 unpooled 再去测试，几天后发现堆内内存依旧会小幅度攀升，从而判定内存泄露并不是由内存池而导致。</p>
<h2 id="顺藤摸瓜"><a href="#顺藤摸瓜" class="headerlink" title="顺藤摸瓜"></a>顺藤摸瓜</h2><p>不是内存池出现泄露，而且堆内堆外一起泄露，能同时占用堆内堆外内存的对象一般不多，不过一时也想不出到底有哪些，于是随手 dump 了一份堆内存快照开始分析，果不其然从中还真看出了些端倪。一般通过 dump 排查内存泄露都使用 <a href="http://www.eclipse.org/mat/" target="_blank" rel="external">Eclipse Memory Analyzer Tool</a>（简称 MAT）去检查 dominator tree，从中找出哪个类的对象不正常地占用了大量内存。但这次的 dominator tree 看不出有什么问题。因为出现泄露的对象在堆上占用的总内存并不是很多，它在 dominator tree 上根本排不到前列，很难被关注到，但是在 Histogram（如下图）中就有它的身影了。</p>
<p><img src="https://cloud.githubusercontent.com/assets/1115061/16897985/53d4ae34-4bf8-11e6-8170-acc9eee1a42c.png" alt="2016-07-17 7 40 25"></p>
<p>出现泄露的就是上图画红框圈起来的 OpenSslClientContext。</p>
<p>从 OpenSslClientContext 的使用也能看出来，这个出问题的服务是作为 client 一端，使用 OpenSsl 去连接另一个服务。一般正常使用的情况下，一个 SSL 证书会只对应一个 OpenSslClientContext。对于大多数场景来说，整个服务可能只会使用一种证书，所以只会有一个 OpenSslClientContext 保留在内存中。但我们这个服务有些特殊，会使用很多不同的证书去建立 SSL 连接，只是服务在内部做了限制，将同一时刻不同证书建立的 SSL 连接数量控制在几十个左右，并且在一个 SSL 证书使用完毕之后，指向该 SSL 证书对应 OpenSslClientContext 的引用会被清理掉。之后按正常逻辑来说 OpenSslClientContext 会被 GC 掉，不会在内存中长久停留。但是上图显示同一时间并存的 OpenSslClientContext 有 27472 个之多，远远超过了原本服务内部在同一时间允许并存的 OpenSslClientContext 的数量限制，这就意味着这个 OpenSslClientContext 发生了泄露。</p>
<p>从 dump 中我们还发现，维护 OpenSslClientContext 的业务对象没有产生泄露，并被正常 GC。这说明我们的业务代码可以正确清理指向 OpenSslClientContext 对象的引用。那这个 OpenSslClientContext 是怎么被 GC Root 引用到的呢？</p>
<h2 id="水落石出"><a href="#水落石出" class="headerlink" title="水落石出"></a>水落石出</h2><p>依然是使用 MAT，分析指向泄露的 OpenSslClientContext 对象的引用路径后得到如下图：</p>
<p><img src="https://cloud.githubusercontent.com/assets/1115061/16897983/472ae7fc-4bf8-11e6-8a69-a311827348b0.png" alt="2016-07-13 7 27 37"></p>
<p>可以看出 OpenSslClientContext 对象指向了两个引用，一个是 Finalizer 上的引用，一个是 Native Stack 上的引用，这表明我们的业务对象已经正确地释放了对 OpenSslClientContext 的引用。<br>Finalizer 引用的存在是因为 finalize method 被 OpenSslClientContext  所 overide 了（实际是 OpenSslClientContext 的父类 OpenSslContext 来进行 overide），这样 JVM 会为这类对象自动加上 Finalizer 引用，从而在该对象被 GC 的时候调用对象的 finalize method。但这个 Finalizer 引用不会阻碍对象被 GC，所以内存泄露与它没有直接的关系。</p>
<p>而 Native Stack 就是 GC Root，被其引用的对象是不能被 GC 的，这也就是 OpenSslClientContext 泄露的源头。从这个 Native Stack 指向的对象的类名 OpenSslClientContext$1 能看出，这是一个 OpenSslClientContext 上的匿名类。</p>
<p>查看这个匿名类的对象的属性：</p>
<p><img src="https://cloud.githubusercontent.com/assets/1115061/16897981/426fd038-4bf8-11e6-95d7-6b657213c973.png" alt="2016-07-13 9 57 15"></p>
<p>一方面它包含有指向外部 OpenSslClientContext 的引用 this$0，还包含一个叫做 val$extendedManager 的引用指向了对象 sun.security.ssl.X509TrustManagerImpl。这时候去翻看 Netty 4.1.1-Final 的 OpenSslClientContext 第 240 ~ 268 行代码如下（注意现在的 Netty 4.1 分支已经将这个 bug 修复，所以不能直接看到下面的代码了）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">try &#123;</div><div class="line">    if (trustCertCollection != null) &#123;</div><div class="line">        trustManagerFactory = buildTrustManagerFactory(trustCertCollection, trustManagerFactory);</div><div class="line">    &#125; else if (trustManagerFactory == null) &#123;</div><div class="line">        trustManagerFactory = TrustManagerFactory.getInstance(</div><div class="line">                TrustManagerFactory.getDefaultAlgorithm());</div><div class="line">        trustManagerFactory.init((KeyStore) null);</div><div class="line">    &#125;</div><div class="line">    final X509TrustManager manager = chooseTrustManager(trustManagerFactory.getTrustManagers());</div><div class="line"></div><div class="line">    // Use this to prevent an error when running on java &lt; 7</div><div class="line">    if (useExtendedTrustManager(manager)) &#123;</div><div class="line">        final X509ExtendedTrustManager extendedManager = (X509ExtendedTrustManager) manager;</div><div class="line">        SSLContext.setCertVerifyCallback(ctx, new AbstractCertificateVerifier() &#123;</div><div class="line">            @Override</div><div class="line">            void verify(OpenSslEngine engine, X509Certificate[] peerCerts, String auth)</div><div class="line">                    throws Exception &#123;</div><div class="line">                extendedManager.checkServerTrusted(peerCerts, auth, engine);</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">    &#125; else &#123;</div><div class="line">        SSLContext.setCertVerifyCallback(ctx, new AbstractCertificateVerifier() &#123;</div><div class="line">            @Override</div><div class="line">            void verify(OpenSslEngine engine, X509Certificate[] peerCerts, String auth)</div><div class="line">                    throws Exception &#123;</div><div class="line">                manager.checkServerTrusted(peerCerts, auth);</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">    &#125;</div><div class="line">&#125; catch (Exception e) &#123;</div><div class="line">    throw new SSLException(&quot;unable to setup trustmanager&quot;, e);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对比上面 MAT 中看到的 val$extendedManager 引用信息我们会知道，上述代码 14 ~ 20 行设置的这个 callback 就是之前说的出现泄露的匿名类。匿名类有指向外部对象 OpenSslClientContext 的引用，也有个指向外部 extendedManager 的引用。这段逻辑是在 OpenSslClientContext 的构造函数中的，而且 12 ~ 29 行的这个 if 语句无论走哪个分支，都会设置一个匿名的 verifier 到 SSLContext.setCertVerifyCallback，也就是说只要 new 一个 OpenSslClientContext 对象，就一定会设置一个 verifier 到 Native Stack 上。</p>
<p>找到 SSLContext.setCertVerifyCallback 的代码。在我们使用的 netty-tcnative-1.1.33.Fork17 中，SSLContext.setCertVerifyCallback 函数声明如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line"> * Allow to hook &#123;@link CertificateVerifier&#125; into the handshake processing.</div><div class="line"> * This will call &#123;@code SSL_CTX_set_cert_verify_callback&#125; and so replace the default verification</div><div class="line"> * callback used by openssl</div><div class="line"> * @param ctx Server or Client context to use.</div><div class="line"> * @param verifier the verifier to call during handshake.</div><div class="line"> */</div><div class="line">public static native void setCertVerifyCallback(long ctx, CertificateVerifier verifier);</div></pre></td></tr></table></figure>
<p>从注释上能看出来，这个函数是用来让用户自定义证书检查函数，好在 SSL Handshake 过程中来使用去校验证书。</p>
<p>函数声明上的「native」关键字也表明它是通过调用本地 C 代码实现的。结合之前的分析，能推理出一定是这个 Native 代码将 verifier callback 存入了 Native Stack，并且在 OpenSslClientContext 没有其他引用指向时没能将这个 callback 正确清理，从而让 OpenSslClientContext 对象有了从 GC Root 过来的引用指向，所以不能被 GC 掉，造成了泄露。</p>
<p>有了指导路线，我们继续追踪问题。在 netty-tcnative-1.1.33.Fork17 的 sslcontext.c 文件下找到 setCertVerifyCallback 函数对应的 Native 代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">TCN_IMPLEMENT_CALL(void, SSLContext, setCertVerifyCallback)(TCN_STDARGS, jlong ctx, jobject verifier)</div><div class="line">&#123;</div><div class="line">    tcn_ssl_ctxt_t *c = J2P(ctx, tcn_ssl_ctxt_t *);</div><div class="line"></div><div class="line">    UNREFERENCED(o);</div><div class="line">    TCN_ASSERT(ctx != 0);</div><div class="line"></div><div class="line">    if (verifier == NULL) &#123;</div><div class="line">        SSL_CTX_set_cert_verify_callback(c-&gt;ctx, NULL, NULL);</div><div class="line">    &#125; else &#123;</div><div class="line">        jclass verifier_class = (*e)-&gt;GetObjectClass(e, verifier);</div><div class="line">        jmethodID method = (*e)-&gt;GetMethodID(e, verifier_class, &quot;verify&quot;, &quot;(J[[BLjava/lang/String;)I&quot;);</div><div class="line"></div><div class="line">        if (method == NULL) &#123;</div><div class="line">            return;</div><div class="line">        &#125;    </div><div class="line">        // Delete the reference to the previous specified verifier if needed.</div><div class="line">        if (c-&gt;verifier != NULL) &#123;</div><div class="line">            (*e)-&gt;DeleteLocalRef(e, c-&gt;verifier);</div><div class="line">        &#125;    </div><div class="line">        c-&gt;verifier = (*e)-&gt;NewGlobalRef(e, verifier);</div><div class="line">        c-&gt;verifier_method = method;</div><div class="line"></div><div class="line">        SSL_CTX_set_cert_verify_callback(c-&gt;ctx, SSL_cert_verify, NULL);</div><div class="line">    &#125;    </div><div class="line">&#125;       </div></pre></td></tr></table></figure>
<p>这里函数的 verifier 参数就对应着 SSLContext.setCertVerifyCallback 上传入的 verifier。这里也不需要完全理解上面代码的含义，主要是看到第 21 行，创建了个引用从 *e 指向了 verifier。这个 *e 是个 JNIEnv struct，NewGlobalRef(e, verifier) 相当于将 verifier 保存在一个全局的变量当中，必须通过对应的 DeleteGlobalRef 才能销毁。</p>
<p>在搜索 sslcontext.c 的代码后发现在正常的逻辑下，要对 verifier 调用 DeleteGlobalRef 将其清理，必须调用 SSLContext.free 函数才能实现。SSLContext.free 声明如下：</p>
<figure class="highlight plain"><figcaption><span>java</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line"> * Free the resources used by the Context</div><div class="line"> * @param ctx Server or Client context to free.</div><div class="line"> * @return APR Status code.</div><div class="line"> */</div><div class="line">public static native int free(long ctx);</div></pre></td></tr></table></figure>
<p>它还有个对应的 make 函数，合并起来用于负责 OpenSslClientContext 分配和回收一些 Native 的资源。OpenSslClientContext 在构造函数中必须调用一次 SSLContext.make，在对象被销毁时需要调用 SSLContext.free。「在对象被销毁时调用」听上去有点析构函数的意思，但 Java 中没有析构函数的概念，看上去 Netty 也没有好的方法来实现这种类似析构函数的功能，虽然所有讲到 finalize 的地方都在谆谆告诫开发者只是知晓它的存在就好但永远不要去使用，Netty 还是「被逼无奈」地将用于资源回收的 SSLContext.free 调用放在了 OpenSslClientContext 的 finalize（继承自 OpenSslContext）函数中。</p>
<p>分析到这里基本就能得到 OpenSslClientContext 泄露的原因了。因为 OpenSslClientContext 在构造时会将一个匿名的 AbstractCertificateVerifier 子类对象作为证书的校验函数（简称为 verifier），通过调用 SSLContext.setCertVerifyCallback 存储到 Native Stack 上，必须在 OpenSslClientContext 销毁时主动调用 SSLContext.free 才能将这个 verifier 从 Native Stack 清除。而 SSLContext.free 是在 OpenSslClientContext 的 finalize 内，必须等到 OpenSslClientContext 被 GC 掉之后才会被调用。由于 verifier 是个匿名类，它含有隐含的指向了其所属 OpenSslClientContext 的引用，导致当 verifier 不被销毁时，其所在 OpenSslClientContext 也无法销毁，从而产生依赖环，verifier 的清理依赖 OpenSslClientContext 的清理，OpenSslClientContext 的清理又依赖 verifier 的清理。这种依赖环如果都是在堆内，JVM GC 的时候会自动检测依赖环，并将相互依赖的两个对象全部 GC 掉。但这里 verifier 比较特殊，它是直接存储在 Native Stack 上的，JVM GC 拿它没有办法。JVM GC 的管辖范围只有堆，Native Stack 可以理解为是它的上级，它无权过问。</p>
<p>另外补充一点，上述问题虽然是在 OpenSslClientContext 中发现，但 OpenSslServerContext 中也有相同问题。</p>
<h1 id="修复"><a href="#修复" class="headerlink" title="修复"></a>修复</h1><p>Bug 找到了，具体的 PR 请参考<a href="https://github.com/netty/netty/pull/5380" target="_blank" rel="external">这里</a>。修复办法就是将导致泄露的匿名 AbstractCertificateVerifier 子类对象修改为 static 的内部类，这样它不会包含指向其所在外部类的引用（即 OpenSslClientContext），从而不会阻碍外部类的 GC，也就避免了泄露的发生。</p>
<p>问题是解决了，但究其根本原因是不是可以归结到 finalize 函数的使用呢？如果 OpenSslClientContext 没有使用 finalize，而是暴露一个类似 close 的接口，要求 OpenSslClientContext 的使用者主动调用 close，finalize 内只是打印日志，提醒使用者没有调用 close，这个问题是不是从一开始就不会存在了呢？<br>一般来说 finalize 出现的问题主要有以下几类：</p>
<ol>
<li>使用 finalize 的对象在创建和销毁时性能比正常的对象差；</li>
<li>finalize 执行时间不确定。可能出现 heap 内虽然有很多拥有 finalize 函数的类对象，且这些对象都已死掉（从 GC Root 无法访问），如果遇到 GC 压力比较大等原因，这些对象的 finalize 还没有被触发，就会导致这些本来该被 GC 但没有被 GC 的对象大量存在于 Heap 中。</li>
</ol>
<p>猜想 Netty 这里使用 finalize 而不是明确提供一个 close 函数，主要是为了使用方便，毕竟 OpenSslContext 在大多数场景下在一个服务中只存在一两个对象，需要将其销毁的情况也许也不是很多。以上猜想<a href="https://github.com/netty/netty/issues/4958" target="_blank" rel="external">在这个 issue </a>中也得到了一定程度的印证，<a href="https://github.com/netty/netty/pull/5547" target="_blank" rel="external">并且 Netty 已经在修改这个问题</a>，让 OpenSslContext 实现 ReferenceCount 接口，在 finalize 之外又提供了 release 函数专门用于清理 Native 资源。</p>
<p>所以分享这些经验来让大家引以为戒，finalize 要尽量少用，看着以为使用 finalize 很合理的地方还是有可能出现问题。</p>
]]></content>
    
    <summary type="html">
    
      追踪一个 Netty 里的 bug
    
    </summary>
    
    
      <category term="Bug" scheme="http://ylgrgyq.github.io/tags/Bug/"/>
    
      <category term="Java" scheme="http://ylgrgyq.github.io/tags/Java/"/>
    
      <category term="Netty" scheme="http://ylgrgyq.github.io/tags/Netty/"/>
    
  </entry>
  
  <entry>
    <title>Garbage First Collector 理解</title>
    <link href="http://ylgrgyq.github.io/2016/07/03/garbage-first-collector-understanding/"/>
    <id>http://ylgrgyq.github.io/2016/07/03/garbage-first-collector-understanding/</id>
    <published>2016-07-03T07:39:30.000Z</published>
    <updated>2017-04-21T05:40:01.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h1><p>缩写约定：</p>
<p>YGC: Young Generation GC</p>
<p>OGC: 针对 Old Generation 的 GC，对 G1 来说指 Mixed GC</p>
<p>FGC: 针对整个 Heap 的 Full GC</p>
<p>STW: Stop-The-World</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ol>
<li>适合大堆，因为不像 CMS 和 Parallel GC 在对老代进行收集的时候需要将整个老代全部收集，G1 收集老代一次只收集老代的一部分 Region</li>
<li>G1 的 Heap 划分为多个 Region，Young Generation 和 Old Generation 都只是逻辑概念，不是物理上隔离又连续的空间</li>
<li>G1 的新老代划分不是固定的，一个新代的 Region 在被回收之后可以作为老代 Region 使用，Young Generation 和 Old Generation 大小也会随着系统运行而调整</li>
<li>G1 的新生代收集和 Parallel、CMS GC 一样是并发的 STW 收集，且每次 YGC 会将整个 Young Generation 收集</li>
<li>G1 的 Old Generation 收集每次只收集一部分 Old Region，且这部分 Old Region 是和 YGC 一起进行的，所以称为 Mixed GC</li>
<li>和 CMS 一样，G1 也有 fail-safe 的 FGC，单线程且会做 compaction</li>
<li>G1 的 Old Generation GC (Mixed GC) 也是自带 compaction 的</li>
<li>G1 没有永久代的概念</li>
</ol>
<h1 id="Young-Generation"><a href="#Young-Generation" class="headerlink" title="Young Generation"></a>Young Generation</h1><p>G1 的 Young Generation 逻辑上也划分为 Eden 和 Survivor。新生 Object 都是在属于 Eden 的一个 Region 上进行分配，Region 满了之后会从 available region 中再取一个新 Region 标记为 Eden 并继续将新生 object 放在里面。直到标记为 Eden 的 Region 数达到上限。到达上限后，触发 YGC。</p>
<h2 id="TLAB"><a href="#TLAB" class="headerlink" title="TLAB"></a>TLAB</h2><p>既然 “新生对象都分配在 Eden”，而 Eden 是个全局的概念，应用内会申请分配内存创建新生对象的业务线程有很多，如果分配内存操作全部由这些业务线程直接去操作 Eden 就一定会产生竞争，因为属于 Eden 的 Region 是一个一个分配的，一个 Region 占满了才会去分配新的 Region。而竞争的存在就导致要用锁去保护 Eden ，才能保证多线程并发的从 Eden 分配内存不出问题。由于分配内存这个操作会非常频繁，只是用锁去保护 Eden 会出现大量的线程去抢占这个保护 Eden 的锁。所以有了 TLAB，Thread Local Allocation Buffer， 这么个优化，去减少业务进程对保护 Eden 的锁的竞争。</p>
<p>Eden 中有一部分内存会划拨出来专门给 TLAB 使用，每个线程都有自己的 TLAB，这块内存是线程自己独占的，为的就是线程在分配内存的时候可以直接从 TLAB 上分配， 不用加锁。只有要分配的内存较大，超出了 TLAB 范围时才需要从 Eden 中以加锁的方式获取内存，或者如果特别大超过了 Region 的 50%，会作为 Humongous Object 专门划拨 Region 存放。</p>
<h2 id="YGC"><a href="#YGC" class="headerlink" title="YGC"></a>YGC</h2><p>随着 Eden 内 Object 越来越多，越来越多的 available region 现在被标记为 Eden 并被占满，当标记为 Eden 的 Region 数达到上限时，会触发 YGC。</p>
<p>每次 YGC 时 G1 从 available region 取一个新 Region 标记为 Survivor，将当前整个 Eden 和老 Survivor 中的 live object 找出来，并根据 live object 熬过的 YGC 次数判断是将其拷贝到这个新的 Survivor 还是拷贝(晋升)到 Old。</p>
<p>Young Generation Object 每熬过一次 GC，age 就增长一岁。G1 会维护一个 age -&gt; object 的 hash 表，将 age 达到目标值的 object，晋升到 Old Generation。</p>
<p>这个目标值一般称为 Tenuring Threshold，是根据 -XX:TargetSurvivorRatio 和 -XX:MaxTenuringThreshold 来动态计算得到的。</p>
<h2 id="PLAB"><a href="#PLAB" class="headerlink" title="PLAB"></a>PLAB</h2><p>除了 TLAB 之外，还有个叫做 PLAB 的东西。YGC 时，live object 需要被拷贝到 Survivor Region 或者晋升到一个 Old Region。拷贝过程是并发的，会有多个 GC 线程一同处理，而目标 Survivor Region 和 Old Region 也是一个 Region 写满之后再分配另一个 available region 继续写。所以这些 GC 线程之间也存在竞争。所以 GC 过程中，会类似 TLAB 一样，从当前正在操作的 Region 上给这些 GC 线程都各自分配一块 Thread Local Buffer，拷贝 live object 时每个 GC 线程都是将 live object 优先拷贝到分配给自己的 Thread Local Buffer 上，这个 Thread Local Buffer 就叫做 PLAB，Promotion Lab，以避免加锁，减少竞争。</p>
<h2 id="Young-Generation-大小"><a href="#Young-Generation-大小" class="headerlink" title="Young Generation 大小"></a>Young Generation 大小</h2><p>上面看到 YGC 触发时机是在 Eden 被占满时，而 Eden 在 Young Generation 中占比最大，也就是说 Young Generation 的大小会影响到 YGC 触发时间和频率。</p>
<p>有三个量会影响到 Young Generation 大小：</p>
<ul>
<li>-XX:G1NewSizePercent 初始 Young Generation 大小，也是 Young Generation 的最小大小，默认 5%</li>
<li>-XX:G1MaxNewSizePercent Young Generation 最大大小，默认 60%</li>
<li>-XX:MaxGCPauseMillis GC 最大停顿时间，默认 200ms</li>
</ul>
<p>Young Generation 的大小只能在 -XX:G1NewSizePercent 和 -XX:G1MaxNewSizePercent 规定的范围内变化。</p>
<p>MaxGCPauseMillis 会影响到 Young Generation 大小是因为 MaxGCPauseMillis 越小，留给 GC 的 STW 的时间越少，则趋向于减少 Young Generation 大小以减少 YGC STW 时间。每次 YGC 完毕，都会根据上面三个量和 G1 内部的一些统计量去计算 Young Generation 大小，然后实现 Young Generation 扩展或收缩。</p>
<p>MaxGCPauseMillis 也不是越小越好。MaxGCPauseMillis 越小，Young Generation 也越小，从而有更多本来是 short-live 的 object 被过早晋升到 Old Generation。而 Old Generation GC 起来比较麻烦，标记清理过程比 Young Generation GC 要复杂的多，整体效率也低，就导致虽然 GC 停滞时间下降了，但 GC 次数可能增多，整体吞吐量下降的情况。并且 GC 次数增多也会导致对 CPU 占用增加，跟业务线程一起抢 CPU。</p>
<p>Young Generation 的扩展或收缩在 GC 日志当中会体现:<br><img src="https://cloud.githubusercontent.com/assets/1115061/17271377/80db9c1c-56ac-11e6-9ba2-eb69502aff42.png" alt="image"></p>
<p>上图看到 Eden 从 8008M 降低到 7936M，同样 Survivor 也有类似变化。而总 Heap 大小因为 -Xmx 和 -Xms 参数都调的 14G 所以 YGC 前后不会出现变化。</p>
<p><strong>注意：</strong>如果 Young Generation 大小被明确规定，比如用 -Xmn 或者 -XX:NewRatio 限制，则 Young Generation 大小就不能根据 GC 实际的 Pause Time 而动态调节了，所以不要使用这类参数。上面 G1NewSizePercent 和 G1MaxNewSizePercent 规定的只是 Young Generation 范围，而不是固定的某个值。</p>
<h1 id="RSet"><a href="#RSet" class="headerlink" title="RSet"></a>RSet</h1><p>G1 也属于分代收集器，G1 是从逻辑上划分 Young Generation 和 Old Generation，没有从物理存储空间上将不同代隔离开 ( Region 可以在 Old 和 Young Generation 之间切换)。分代收集的好处就是将 long-live object 和 short-live object 分开收集，从而不用每次 GC 都扫描整个 Heap，降低 GC 时间。那么 YGC 的时候，放入 CSet (一次 GC 中参与收集的所有 Region 组成的集合叫做 CSet)的只有 Young Generation Region，所有 Old Generation Region 都不会参与 YGC。于是就需要有机制去保证 Young Generation 上的 Object 在被 Old Generation Region 上某个 Object 引用时，这个 Young Generation 上的 Object 不能在 YGC 的时候被 GC 掉。所以需要有个地方能记录每个 Object 都被哪些引用指向，这些引用来自哪个 Region。</p>
<p>另一方面，YGC 和 OGC 在执行完后都会有 live object 被搬迁到新的 Free Region 上，那么指向这些 live object 的引用就会发生变化，需要更新引用让其重新指向这个 live object 的新地址。所以也需要上述这个记录每个 Object 被哪些引用指向的机制，从而在 GC 后去更新引用。</p>
<p>G1 中每个 Region 都会维护一个 Remember Sets，也叫 RSet，用于记录当前 Region 之外，有哪些 Region 有指向当前 Region 的引用。没有这个 RSet 的话，单拿 YGC 来说，每一次 YGC 在扫描完 Root 之后，都要再扫描一遍当前所有 Old Generation Region 以找出从 Old Generation 指向 Young Generation 的引用。</p>
<p><strong>注意：</strong>看到 RSet 只会记录别的 Region 对本 Region 的引用，自己 Region 内部的引用无需 RSet 参与记录。</p>
<h2 id="RSet-内引用构建"><a href="#RSet-内引用构建" class="headerlink" title="RSet 内引用构建"></a>RSet 内引用构建</h2><p>既然 RSet 是必须要有的，接下来就看看 RSet 内是怎么对引用关系进行记录的。</p>
<p>因为每次 YGC 都会将整个 Young Generation 都放入 CSet，不存在哪个属于 Young Generation 的 Region 不参与 YGC 的情况。所以对 Heap 上的所有 Region 来说，被 Young Generation 内 Object 的引用指向是不需要记录到 RSet 中的。于是，RSet 内需要维护的引用只有两种：</p>
<ul>
<li>Old-to-young refernence</li>
<li>Old-to-old refernence. </li>
</ul>
<p><img src="https://cloud.githubusercontent.com/assets/1115061/17271397/db5d8952-56ac-11e6-8fae-3fd7c772aead.png" alt="image"><br>(图片来自参考文献[1])</p>
<p>看到上面图中，x Region 是 Young Region，y、z 是 Old Region。每个 Region 都有个配套的 RSet，x 的 RSet 有个指向 z 的记录，因为 z 是 Old  且有指向 x 的引用。z 虽然被 x 和 y 两个 Region 上的引用指向，但因为 x 是 Young Region，所以 z 的 RSet 中只有指向 y 的记录。同样的方式分析，y 的 RSet 没有任何记录，因为 y 只有被 x 指向的引用。</p>
<h2 id="RSet-记录"><a href="#RSet-记录" class="headerlink" title="RSet 记录"></a>RSet 记录</h2><p>Region 和 Region 之间的 popular 程度是不同的，有的 Region 有更多的引用指向，有的则会少一些。如果一个 Region 特别 popular，有大量的引用指向这个 Region，该 Region 的 RSet 所要记录的引用也更多，GC 时扫描 RSet 的时间也更长。</p>
<p>为了减少这种特别 popular 的 Region 的 RSet 处理时间(这里不光是能减少 GC 时间，还能减少各 GC 线程之间处理 RSet 时的不均匀性，越均匀越能发会多线程 GC 性能)，RSet 根据所属 Region “popular” 程度的不同，一共分为三种等级，sparse、fine 和 coarse。每个等级都有个 per-region-table (PRT) 用于存储引用信息。</p>
<p>每个 Region 实际又能被细分为最小单个 512 字节的 heap chunk，称为 card。每个 card 都有个根据它地址构造出来的全局唯一 id ，这个唯一 id 不仅是在一个 Region 中唯一，在整个 Heap 中都是唯一的，并且能根据这个 id 立即找到对应的 card。说了半天的 RSet 记录指向 RSet 所属 Region 的引用，实际就是在 RSet 中记录指向这个 Region 引用所在 card 的唯一 id。</p>
<p>当 RSet 处在 sparse 级别，PRT 中每个 entry 直接存引用当前 Region 所在 card 的 id，这种粒度下 RSet 扫描效率最高。当 Region popular 程度上升，指向该 Region 的引用越来越多，直接存 card id 会导致 PRT 过大。当 sparse PRT 内存储引用到达限制后，升级为 fine 级别的 PRT。</p>
<p>fine 级别的 PRT 中每个 entry 不再直接存储 card id，具体存储内容拿下图来说。B 有个指向 A 的引用，当 A 的 RSet 升级到 fine 级别时，A 为 B 单独创建一个 Bitmap，将指向这个 Bitmap 的引用和指向 B Region 的引用一起存入 A 的 PRT 的 entry 中。</p>
<p><img src="https://cloud.githubusercontent.com/assets/1115061/16544439/b42716f2-4138-11e6-8c8b-aa80b00ba319.png" alt="2016-07-01 7 10 10"></p>
<p>并且在 B 对应的这个 Bitmap 会标识出 B 中哪个 card 有指向 A 的引用。从这里描述能看出来 fine 级别的 PRT 对引用的记录更间接一些，所以扫描的时候相对更慢一些。</p>
<p>当 Region popular 程度继续升高，还是按上图说的，B 指向 A 的引用越来越多，B 对应的 Bitmap 达到上限之后，A fine 级别的 PRT 内 B 相关的 entry 会被删除，取而代之的是使用 coarse 级别的 PRT 来记录 B 指向 A 的引用。</p>
<p>coarse 级别的 PRT 实际就是个 Bitmap，该 Bitmap 上每个 bit 代表当前 Heap 的一个 Region。拿上面例子来说就是将 A 的 coarse PRT 的 Bitmap 中代表 B 的 bit 置位，并且不再记录 B 中到底是哪个 card 含有指向 A 的引用。这也能看出来 coarse 级别 PRT 扫描起来耗时最大，必须扫描整个 B region 才能找到所有指向 A 的引用。</p>
<p>除了上面结构之外，跟 RSet 相关的还有个全局的 card table，也是个基于 Bitmap 的结构，用于在 GC 时扫描 RSet 阶段记录已经扫过的 card，避免重复扫已经扫过的 card。每轮 GC 后这个 card table 会被删除。</p>
<p>下面是一次真实 GC 记录，其中 Update RS, Scan RS 就是处理 RSet 的时间，Clear CT 是清理上面说的全局 card table 的时间。</p>
<p><img src="https://cloud.githubusercontent.com/assets/1115061/16544449/f9a589ca-4138-11e6-9954-68f989881d04.png" alt="2016-07-01 7 34 34"></p>
<h2 id="RSet-的更新"><a href="#RSet-的更新" class="headerlink" title="RSet 的更新"></a>RSet 的更新</h2><p>为了为每个 Region 维护 RSet，就一定涉及到 Region 内有引用被更新的时候，去更新这个 Region 对应的 RSet。</p>
<p>RSet 在 Parallel Old 和 CMS GC 中也有使用，他们是通过 write barrier 来在 Region 内引用有更新的时候去对应的维护 RSet 的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">object.field = some_other_object</div></pre></td></tr></table></figure>
<p>在执行例如上面语句的时候去更新 intergenerational reference。</p>
<p>G1 是引入了两个 barrier，一个 pre-write barrier 和一个 post-write barrier。其中 pre-write barrier 会在后面叙述 G1 concurrent marking 的时候描述，这里只叙述 post-write barrier 功能和 G1 如何使用这个 barrier 去更新 RSet 。</p>
<p>post-write barrier 在每次写入一个 reference 的时候被调用，应用内修改 reference 的地方肯定很多，所以这个 barrier 性能非常关键，执行的慢了会影响整个系统的运行。所以 G1 的这个 post-write barrier 只做很少的事情：</p>
<ol>
<li>判断这次 reference 写入是不是个 cross-region 的写入，reference 是否符合 old-to-old 或 old-to-young 的 RSet 修改条件；</li>
<li>如果是 cross-region 的写入，则说明需要更新 RSet，于是将引用所在 card 和被引用的 Region 等信息存入一个叫做 update log buffer 或者 dirty card queue 的地方</li>
<li>如果 update log buffer 写满了，就再申请一个新的 buffer 继续写，写满的 buffer 会放在全局的 list 中</li>
</ol>
<p>之后，由 concurrent refinement threads 去消费这个 update log buffer，拿到 buffer 后这个 GC refinement thread 会根据 buffer 内的信息，实际完成 RSet 更新工作，包括将 reference 记录在 RSet 中以及 RSet 粒度升级等工作。 </p>
<p>concurrent refinement threads 是持续运行的，并且会随着 update log buffer 积累的数量而动态调节。有三个配置项 -XX:G1ConcRefinementGreenZone, -XX:G1ConcRefinementYellowZone, -XX:G1ConcRefinementRedZone 去控制在有多少积压的 buffer 时，使用多少 refinement threads。目的就是为了保证 refinement threads 一定要尽可能的跟上 update log buffer 产生的步伐。但是这个 refinement threads 不是无限增加的，有个 -XX:G1ConcRefinementThreads 能控制 refinement 线程数上限。</p>
<p>如果一旦出现 refinement threads 跟不上 update log buffer 产生的速度，update log buffer 开始出现积压，mutator threads 即上面修改 reference 的线程就会协助 refinement 线程执行 RSet 的更新工作。这个 mutator threads 实际就是应用业务线程，当业务线程去参与 RSet 修改时，系统性能一定会受到影响，所以需要尽力去避免这种状况。</p>
<p>个人理解这里 mutator threads 去帮助 refinement 线程更新 RSet，不是说 mutator thread 在修改 reference 的时候直接同步的更新 RSet，而还是采用上面异步的方式，只是每次写入一个 job 到 update log buffer，就从 update log buffer 中消费一个 job，从而保证 RSet 更新顺序。</p>
<p>除了 Mutator Thread 和 Concurrent Refinement Thread 之外，GC 时真正处理清理工作的 Worker Thread 也会参与消费 update log buffer。可以看上面那张 YGC 实际日志的图，有个 Update RS 过程，这个过程就是在消费 GC 时 Concurrent Refinement Thread 没有处理完的 Update log buffer。</p>
<p><img src="https://cloud.githubusercontent.com/assets/1115061/16544448/f72dab3c-4138-11e6-9e6d-40e212ccc196.png" alt="2016-07-03 10 21 08"></p>
<p>看到下图是一个线程快照，能看到有很多 Concurrent Refinement Thread 处在运行中。</p>
<p><img src="https://cloud.githubusercontent.com/assets/1115061/16544440/b7d7aff0-4138-11e6-86ef-a2eb93d1092f.png" alt="2016-07-01 10 16 17"></p>
<h1 id="YGC-日志"><a href="#YGC-日志" class="headerlink" title="YGC 日志"></a>YGC 日志</h1><p><img src="https://cloud.githubusercontent.com/assets/1115061/19113472/54ef4e9a-8b3c-11e6-8265-165ae585e56f.png" alt="image"></p>
<p>[GC pause (G1 Evacuation Pause) (young), 0.2604517 secs]</p>
<p>第一行是 GC 开始时间和 GC 用时。下面分部分说明 GC 日志内容。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[Parallel Time: 236.3 ms, GC Workers: 18]</div><div class="line">[GC Worker Start (ms): Min: 7181358.2, Avg: 7181358.5, Max: 7181359.2, Diff: 0.9]</div><div class="line">[Ext Root Scanning (ms): Min: 5.7, Avg: 20.8, Max: 47.3, Diff: 41.7, Sum: 374.2]</div><div class="line">[Update RS (ms): Min: 46.1, Avg: 72.0, Max: 87.1, Diff: 40.9, Sum: 1296.3]</div><div class="line">   [Processed Buffers: Min: 72, Avg: 118.2, Max: 179, Diff: 107, Sum: 2128]</div><div class="line">[Scan RS (ms): Min: 0.1, Avg: 0.3, Max: 0.5, Diff: 0.4, Sum: 6.0]</div><div class="line">[Code Root Scanning (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]</div><div class="line">[Object Copy (ms): Min: 141.9, Avg: 142.2, Max: 142.6, Diff: 0.7, Sum: 2559.4]</div><div class="line">[Termination (ms): Min: 0.1, Avg: 0.4, Max: 0.6, Diff: 0.5, Sum: 6.8]</div><div class="line">   [Termination Attempts: Min: 1, Avg: 209.6, Max: 313, Diff: 312, Sum: 3773]</div><div class="line">[GC Worker Other (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 2.0]</div><div class="line">[GC Worker Total (ms): Min: 235.1, Avg: 235.8, Max: 236.1, Diff: 1.0, Sum: 4244.7]</div><div class="line">[GC Worker End (ms): Min: 7181594.3, Avg: 7181594.3, Max: 7181594.4, Diff: 0.1]</div></pre></td></tr></table></figure>
<p>这个是 YGC 中并发执行的过程。</p>
<p>[GC Worker Start (ms): Min: 7181358.2, Avg: 7181358.5, Max: 7181359.2, Diff: 0.9]<br>这个是 GC 线程启动时间，Min, Max 是最早启动的线程和最晚启动线程的时间点，Avg 是 GC 线程启动平均时间点，Diff 是这些 GC 线程启动时间的偏差。一般来说 Diff 不会很大，如果很大，可能是 某个 GC 线程在执行什么任务，耽搁住了。</p>
<p><strong>注意</strong> 这里强烈怀疑 GC worker 就是 Concurrent Refinement Thread。GC 开始后，Refinement Thread 开始变成 GC worker 专心处理 GC 事务。因为这个过程中是 STW 的，refinement thread 也没有工作要执行。GC worker start 的时间不统一，有可能是 refinement thread 在处理 RSet 更新的时候有的 RSet 更新时间长，有的短，如果某个 RSet 更新时间较长，把 refinement thread 占用时间长了，这里 GC Worker Start 的 Diff 就偏差较大。</p>
<p>[Ext Root Scanning (ms): Min: 5.7, Avg: 20.8, Max: 47.3, Diff: 41.7, Sum: 374.2]<br>扫描 off heap 上指向参与当前 GC 的 CSet 内 Region 的引用。off heap 主要是 JVM system dictionary，VM data structur，JNI thread handles，hardware register，global variable，thread stack root 等。</p>
<p>[Update RS (ms): Min: 46.1, Avg: 72.0, Max: 87.1, Diff: 40.9, Sum: 1296.3]<br>[Processed Buffers: Min: 72, Avg: 118.2, Max: 179, Diff: 107, Sum: 2128]<br>消费 update log buffer 队列，取出 buffer 后更新 RSet。</p>
<p>[Scan RS (ms): Min: 0.1, Avg: 0.3, Max: 0.5, Diff: 0.4, Sum: 6.0]<br>扫描 RSet，RSet 上引用的 object 都是 Old Generation 上指过来的引用，被引用的对象都标记为 live。</p>
<p>[Code Root Scanning (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]<br>G1 没有永久代，所有 code cache 还是会放在内存中，也会参与 GC。这里会扫描所有 Code Root。</p>
<p>[Object Copy (ms): Min: 141.9, Avg: 142.2, Max: 142.6, Diff: 0.7, Sum: 2559.4]<br>真正的清理工作执行时间。将 live object 拷贝到空的地方。</p>
<p>[Termination (ms): Min: 0.1, Avg: 0.4, Max: 0.6, Diff: 0.5, Sum: 6.8]<br>[Termination Attempts: Min: 1, Avg: 209.6, Max: 313, Diff: 312, Sum: 3773]<br>GC 线程的工作都会放入队列，之后被 GC 线程从队列消费后执行工作内容。当队列消费完毕之后，会取别的线程的任务去执行。如果别的线程队列都是空的，他就开始进入 termination 环节，等待所有线程全部执行完毕之后，结束 GC 过程。GC 线程在 termination 环节停留的时间就是这里的 Termination 时间。Termination Attempts 是线程尝试 Terminate 的次数，如果尝试 Terminate 时发现别的线程还有工作要做，就放弃 Terminate，完成工作之后再重新 Terminate。</p>
<p>[GC Worker Other (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 2.0]<br>除了上述主要过程之外，其它过程消耗的时间。</p>
<p>[GC Worker Total (ms): Min: 235.1, Avg: 235.8, Max: 236.1, Diff: 1.0, Sum: 4244.7]<br>GC Parallel 过程总时间</p>
<p>[GC Worker End (ms): Min: 7181594.3, Avg: 7181594.3, Max: 7181594.4, Diff: 0.1]<br>GC Parallel 过程结束时间，跟 GC Worker Start 对应。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"> [Code Root Fixup: 0.5 ms]</div><div class="line">  [Code Root Purge: 0.0 ms]</div><div class="line">  [Clear CT: 1.7 ms]</div><div class="line">  [Other: 22.0 ms]</div><div class="line">     [Choose CSet: 0.0 ms]</div><div class="line">     [Ref Proc: 12.8 ms]</div><div class="line">     [Ref Enq: 0.6 ms]</div><div class="line">     [Redirty Cards: 1.2 ms]</div><div class="line">     [Humongous Register: 1.4 ms]</div><div class="line">     [Humongous Reclaim: 0.1 ms]</div><div class="line">     [Free CSet: 2.6 ms]</div><div class="line">  [Eden: 5556.0M(5556.0M)-&gt;0.0B(228.0M) Survivors: 444.0M-&gt;488.0M Heap: 11.6G(14.0G)-&gt;6338.3M(14.0G)]</div><div class="line">[Times: user=3.47 sys=0.26, real=0.26 secs]</div></pre></td></tr></table></figure>
<p>以上这些都是单个 GC 线程执行的了，不再是并发过程。</p>
<p>[Code Root Fixup: 0.5 ms]<br>因为 GC 的原因，有一些 object 的位置会出现变更，如果这个 object 是被 code root 引用，这里更新 code root 的引用。</p>
<p>[Code Root Purge: 0.0 ms]<br>对 Code Root 进行清理。</p>
<p>[Clear CT: 1.7 ms]<br>清理 Card Table。这个 table 用于记录扫描过的 RSet，避免重复扫描 RSet。</p>
<p>[Choose CSet: 0.0 ms]<br>如果是 YGC，这个时间永远是 0，因为 YGC 时 CSet 就是整个 Young Generation。只有 Mixed GC 才会需要从 Old Generation 选出一部分 Region 放入 CSet 所以会消耗时间。</p>
<p>[Ref Proc: 12.8 ms]<br>处理 soft、weak、phantom、final、JNI 等等引用的时间。</p>
<p>[Ref Enq: 0.6 ms]<br>soft, weak, phantom 等引用在 GC 掉之后都会将通知信息放在 ReferenceQueue 上。</p>
<p>[Redirty Cards: 1.2 ms]<br>在执行将 reference enqueue 时，可能有 RSet 被更新了，这时候要标记这些 RSet 为 dirty，处理一下。</p>
<p>[Humongous Register: 1.4 ms]<br>不知道干嘛的。</p>
<p>[Humongous Reclaim: 0.1 ms]<br>reclaim 是清理 Humongous object。</p>
<p>[Free CSet: 2.6 ms]<br>清理 CSet。</p>
<h1 id="Old-Generation"><a href="#Old-Generation" class="headerlink" title="Old Generation"></a>Old Generation</h1><p>接下来再看看 Old Generation 相关内容。熬过一定次数 YGC 的 live object 会被晋升到 Old Generation，于是 Old Generation 内存占用会越来越大，并且晋升到 Old 之后之前本来 live 的 object 可能随着使用也变成 dead object 了，也需要去 GC。</p>
<p>当 Old Generation 空间占用整个 Heap 比例超过目标值(-XX:InitiatingHeapOccupancyPercent, IHOP)后，开始 OGC 过程。</p>
<p><strong>注意：</strong>CMS 是 Garbage 占到整个 Old Generation 比例超过某个值后开始 OGC。而这里 G1 是 Old Generation Garbage 占整个 Heap 的比例。</p>
<p>G1 的 OGC 也是分为 marking 和 sweeping 两个过程。marking 阶段找到当前 Old Generation Heap 中所有 live 的 object，sweeping 过程将 live object 拷贝到新的 available region 从而留下 Garbage 在老的 Region ，之后直接清理掉这些老的 Region。live object 拷贝到 available region 时 live object 是紧挨着排列的，所以没有碎片。清理过程自带 compat 效果。</p>
<p>G1 OGC 最大的特色就是不是一口气将整个 Old Generation 全部清理，从而减小 Old Generation 大小对清理 Old Generation 时间的影响。类似 CMS 或 Parallel 因为每次针对 Old Generation 的清理都要一口气将 Old Generation 全部清理干净，于是 Old Generation 越大，清理的时间越长，所以在大堆上容易产生超长 GC。</p>
<h2 id="G1-OGC-Marking"><a href="#G1-OGC-Marking" class="headerlink" title="G1 OGC Marking"></a>G1 OGC Marking</h2><p>当 Old Generation 空间占用整个 Heap 比例超过 IHOP 后，下一次 YGC 时就会开始 initial-mark，STW 且并发的标记所有 Root Object。跟着 YGC 一起是因为 YGC 本就需要标记一次所有 Root object。也正因为 initial-mark 是在 YGC 中进行的，所以 concurrent marking 开始的时候只用标记 Old Generation Region 就行了，Young Generation 的 Eden 此时都被清理完了，Survivor 是算作 live object 存在的。</p>
<p>initial-mark 结束后开始 concurrent root scanning. 因为 initial-mark 就是一次 YGC。YGC 后 live Object 都放在 Survivor Region 中。这个过程就是标记所有 Survivor 内 Object 引用的对象。这个过程跟它名字指示的一样是并发的，唯一限制是必须在下一次 YGC 之前完成，因为下一次 YGC 就会产生新的 Survivor ，很有可能跟当前 Survivor 完全不同。</p>
<p>之后是 concurrent marking。大部分 mark 工作都在这里完成。后面会详细再说。这个过程是并发的，对业务的影响主要是降低业务的 throughput.</p>
<p>concurrent marking 结束后开始 STW 的 remark. 标记所有因为 concurrent marking 阶段 marking 线程和业务线程并发运行而导致的没有标记到的 live object.</p>
<p>remark 完毕后，开始 clean up. 如果 mark 阶段找到没有任何 object 存活的 region，该 region 在该阶段直接被放入 available regions.</p>
<h3 id="G1-Concurrent-Marking"><a href="#G1-Concurrent-Marking" class="headerlink" title="G1 Concurrent Marking"></a>G1 Concurrent Marking</h3><h4 id="Marking-算法"><a href="#Marking-算法" class="headerlink" title="Marking 算法"></a>Marking 算法</h4><p>G1 的这套 Marking 算法借鉴了 Taiichi Yuasa 的 Snapshot-at-the-beginning (SATB) 算法，并进行了一些改进。Marking 的最基本目标就是在 Heap 耗尽之前，完成对整个 Heap 的 marking 工作，从而能够在 Heap 耗尽前开始清理。</p>
<p>SATB 内部会对 Heap 维护一个 Snapshot，标记工作也是在这个 Snapshot 上进行。SATB 保证：</p>
<ol>
<li>所有在 concurrent marking 阶段开始时 live 的 object ，一定会被 marked and traced；</li>
<li>所有在 concurrent marking 过程中产生<strong>或死掉</strong>的 object 都一定被标记为 live 并且不被 traced ;</li>
</ol>
<p>SATB 会维护两个 bitmap，preivous 和 next。previous bitmap 存的是上一次完成的 marking 信息，当前 marking 阶段会创建并更新 next bitmap。随着 marking 阶段的进行，next bitmap 会逐渐被完善，当 next bitmap 拥有整个 Heap 的 marking 信息后，next bitmap 会替代 previous bitmap。</p>
<p>在 G1 Region 上，有两个 top-at-mark-start (TAMS) 标记位，一个是标记上一次 marking 阶段使用的 TAMS，也称为 PrevTAMS；另一个用来标记本次 marking 阶段，也称为 NextTAMS。</p>
<p>Marking 过程如下图：<br><img src="https://cloud.githubusercontent.com/assets/1115061/16544442/d1748c76-4138-11e6-902d-7c37b6bb0843.png" alt="2016-07-03 12 23 19"><br>(该图片引自参考文献 [2])</p>
<p>上面图是连续两次 mark 的过程。下面对每一步进行解释：<br>A: PrevBitmap 和 NextBitmap 都是空的，说明这是个新 Region，没有经历过 marking 阶段。Bottom 和 Top 之间是 Region 当前已经分配的空间。因为没有经历过 marking，PrevTAMS 指向 Bottom。NextTAMS 不管 Region 之前是否经历过 marking，initial marking 的时候都会指向 Top。</p>
<p>B: 在 Remark 阶段结束之后，来到了图中 B 指示的阶段。看到 NextBitmap 上已经被标记了哪些是 live object，没被标记的就是 dead object。NextTAMS 到 Top 之间的 object 是 concurrent marking 阶段，因为业务线程跟 marking 线程并发运行而新产生的 object。按照 SATB 之前说的，这部分 object 全部认为是 live 的。也正是因为这个原因，本次 marking 只针对 PrevTAMS 到 NextTAMS 之间的区域进行标记。</p>
<p>C: Cleanup 阶段，NextBitmap 替换 PrevBitmap，因为 marking 工作已经完成，NextBitmap 已经有了整个 Heap 的信息。<strong>注意：</strong>NextBitmap 和 PrevBitmap 实际都是全局的一个 Bitmap，是标识整个 Heap 的。上图中 Bitmap 看上去跟 Region 绑定只是为了方便看，便于理解。</p>
<p>D: 能到 D 这个阶段，这个 Region 经历两次 initial marking，说明在上一次 marking 后，这个 Region 并没有被收集。后面会说，G1 的 Mixed GC 不是一定要在整个 Heap 上所有 dead object 都被收集干净了才停止，而是只要根据 marking 提供的 dead object 占用的空间在整个 Heap 中占比小于一定值后，就停止收集。所以完全是有可能存在能经历两次甚至更多次 marking 的 Region。从 D 能看到 Top 相对于 C 增长了一些，说明上次 marking 结束后，这个 Region 上又有新晋升上来的 object。跟 A 一样，PrevBitmap、PrevTAMS 保持不变，创建 NextBitmap，NextTAMS 指向 Top。并且看到 PrevBitmap 没有变化，因为不经历 makring 这个 bitmap 是不可能变化的。</p>
<p>E: 跟 B 一样，Remark 结束，Bottom 到 NextTAMS 之间所有 live object 都被标识出来，NextTAMS 到 Top 之间是本轮 concurrent marking 阶段新晋升的 object，直接被标记为 live。<strong>注意：</strong>看到第二次 marking 的时候 mark 的还是 Bottom 到 NextTAMS，上一轮已经被 mark 过的 Bottom 到 PrevTAMS 的还是会参与 marking。</p>
<p>F: 跟 C 一样，NextBitmap 替换 PrevBitmap。NextBitmap 被清理。</p>
<p>从上面看到 Marking 阶段实际就是为了维护 PrevBitmap，有了这个 Bitmap，就能知道一个 Region 上有多少 live object，从而能够根据 dead object 空间占比来排序，找出 GC 效率最高的 Region 来 GC。</p>
<h4 id="marking-过程"><a href="#marking-过程" class="headerlink" title="marking 过程"></a>marking 过程</h4><p>在了解了 Marking 算法过程之后，再回过头来看一遍并发标记的所有阶段。</p>
<h5 id="Initial-Marking"><a href="#Initial-Marking" class="headerlink" title="Initial Marking"></a>Initial Marking</h5><p>之前说过了，就是 STW 的标记所有 roots 直接指向 object 。</p>
<p>root object 指的就是能被 heap 之外引用的对象，比如 native stack objects，JNI local 或 global object 等。</p>
<p>因为 YGC 的时候也是要 STW 的扫描 roots ，所以 initial-mark 都是 piggybacking 到一个 YGC 上进行的，并且会将每个 Region 的 NextTAMS 设置为各自 Region 的 Top 指向的值。</p>
<h5 id="Root-Region-Scanning"><a href="#Root-Region-Scanning" class="headerlink" title="Root Region Scanning"></a>Root Region Scanning</h5><p>设置完每个 Region 的 NextTAMS 之后，STW 阶段结束，业务线程被重启运行。initial-marking 阶段拷贝到 Survivor Region 的 object 都被认为是 marking roots，需要在本阶段被 scan 。所有被 Survivor Region 内 Object 引用的 Object 都需要被 mark，认为是 live 的。</p>
<p>Root Region Scanning 必须在下一次 YGC 之前完成，不然 Survivor 又被更新了，标记过程就算是失败了，会重新触发标记。</p>
<h5 id="Concurrent-Marking"><a href="#Concurrent-Marking" class="headerlink" title="Concurrent Marking"></a>Concurrent Marking</h5><p>这个阶段就是并发的标记所有 live object，参与 concurrent marking 的线程数由 -XX:ConcGCThreads 规定，如果没有设置默认就是 -XX:ParallelGCThreads 的四分之一。</p>
<p>之前在描述 RSet 功能的时候说过，G1 引入了两个 write barrier，一个 post-barrier 在 RSet 那里用来每次修改引用的时候维护 RSet，还有一个 pre-write barrier 是在 concurrent marking 这个阶段使用的。</p>
<p>之前说过，SATB 的保证是，在 marking 阶段开始时所有的 live object 都会在 marking 结束的时候被标记出来，所有 marking 过程中新生或死亡的 object 都被认为是 live object。新生的对象因为都会在 NextTAMS 到 Top 之间，所以没有什么需要特殊处理的。但 marking 过程中 dead 的 object 需要特殊处理，这里 pre-write 就是干这个特殊处理的。</p>
<p>比如在 concurrent marking 过程中，业务线程执行如下语句：</p>
<figure class="highlight plain"><figcaption><span>java</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"> </div><div class="line">x.f = y</div></pre></td></tr></table></figure>
<p>也就是说修改了 x 这个 object 中 f 这个引用，另其指向了 y 。那么 x.f 原本指向的 object 可能死亡了也可能还活着，根据 SATB 的要求，需要将其标记为 live。pre-write 的代码逻辑类似：</p>
<figure class="highlight plain"><figcaption><span>java</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"> </div><div class="line">if (is-marking-active) &#123;</div><div class="line">  prev = x.f;</div><div class="line">  if (prev != Null) &#123;</div><div class="line">    satb_enqueue(prev);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure> 
<p>也就是说如果在 marking 过程中，x.f 的引用发生改变，需要将 x.f 原本指向的 object 放入 satb_enqueuey 以异步的方式将 x.f 原本指向的 Object 标记为 live。</p>
<p>satb_enqueue() 是将 prev 放入一个 thread local buffer，也称为 SATB buffer。SATB buffer 有个初始大小，每个业务线程都会有这么个 buffer。当业务线程的 SATB buffer 被占满后，JVM 会再分配一个新的空 SATB buffer 给这个线程使用，写满的那个 SATB buffer 就放在一个全局的 list 中。</p>
<p>执行 concurrent marking 的线程在 scan object 和 mark live object 过程中，会定时的过来查看这个 global list，从中读出 SATB buffer 然后将对应的 object 标记为 live ，被这个 object 指向的所有 object 最终也会被标记为 live。</p>
<p>concurrent marking thread 在 marking 过程中还会计算 live object 数量。从而为之后的清理过程提供参考数据。</p>
<p>问题：</p>
<ol>
<li>假若一开始有 x.f = z，之后执行了 x.f = y。为何必须标记 z 为 live，不这么标记可以吗？如果执行 x.f = y 的时候，z 就是死掉的，那么就没必要再标记 z，本轮 GC 就能将 z GC 掉。如果 z 还活着，说明 z 能够不依赖 x 这个对象而存在，说明 z 还被别的 live 的对象所指向，所以 z 也是不需要被标记的。</li>
</ol>
<p>答：不这么补充标记 z 是不行的。这里原因我认为是由于业务线程和 GC 线程并发执行，无法确定在 x.f = y 执行之后，z 是否可能被 “救活”。所以必须将 z 标记为 live，<strong>宁可错误标记都不能漏标记</strong>。</p>
<p>比如业务线程需要执行如下伪代码：</p>
<figure class="highlight plain"><figcaption><span>java</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x.f = z      // 假如一开始 x.f 是指向 z 的</div><div class="line"></div><div class="line">m = z        // 将 z 找个中间变量记录下来</div><div class="line">x.f = y      // 修改 x.f 的引用，此时 z 可能确实是死掉了</div><div class="line"></div><div class="line">w.f = z      // 将 z 救活</div></pre></td></tr></table></figure>
<p>上述执行过程一定是在某个线程的 Thread Stack 上，但 Thread Stack 在 Concurrent Marking 过程中已经被扫描过了，不会再次扫描，所以 z 交给 m 之后，m 虽然确实在 Thread Stack 上，但并非被 GC Root 引用，必须有其它 live 的对象引用才能被标记为 live。m 是当前 Stack 上的本地变量，是新生成的，在之前 initial-mark 时根本不存在，所以不可能参与标记。</p>
<p>假若 pre-write barrier 不存在。在上述伪码第 1 行，w 对象因为被别的 live 对象引用，已经标记为 live。此时 w.f 还未指向 z 所以 z 不会被标记。</p>
<p>执行到第 4 行，x.f = y 执行时如果不用 pre-write barrier 去补充标记 z，z 此时此刻确实是 Dead Object，没有任何从 GC Root 能够到达它的路径。</p>
<p>执行到第 6 行，w.f = z，上面说了此时 w 已经被标记过是 live，w.f = z 执行后 z 也重新有了从 GC Root 指向的引用链，但如果没有 write barrier 的协助，z 就漏标记了。</p>
<p>还是之前说的宁可错误标记，将 Dead Object 标记为 live，也不能漏标记。错误标记会在下一次 GC 的时候得到修正，但漏标记就没有修正机会了。</p>
<ol>
<li>为什么非要用 prev-write barrier，为什么不用 post-write barrier? 还是拿上面的例子来说，x.f = y 执行的时候需要将 x.f 之前指向的对象 z 标记为 live。但 x.f = y 执行完之后再去执行这个补标记 z 的过程为何不行，非要在 x.f = y 执行之前就去补标记 z ？</li>
</ol>
<p>答：为了说清楚问题，可能得先尝试一下使用 post-write barrier 是什么样子的。比如如果用 post-write barrier 去补充标记 z，伪码大概是这个样子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">x.f = z       // 这里还是认为 x.f = z 是一开始就存在的</div><div class="line"></div><div class="line">if (is-marking-active) &#123;</div><div class="line">  prev = z      // 先把 z 找个地方存放一下</div><div class="line">&#125;</div><div class="line"></div><div class="line">x.f = y       // 业务代码</div><div class="line"></div><div class="line">if (is-marking-active) &#123;</div><div class="line">  if (prev != Null) &#123;</div><div class="line">    satb_enqueue(prev);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>也就是说一方面，如果使用 post-write barrier 还是需要用到 pre-write barrier，因为需要在 x.f = y 执行之前记录 x.f 之前指向的 z 对象。看到 is-marking-active 需要判断两次，因为只有在 marking 过程中，记录这个 prev 才是有意义的，不在 marking 中是不需要记录这个 prev 的。这个样子还不如使用一个 pre-write barrier 来的简单。</p>
<p>另一方面，以上 barrier 还是业务线程执行的代码，业务线程和 GC 线程之间的顺序和时间是无法确定的。不能确认业务线程在执行了第 7 行之后，是否能在 GC 标记过程结束前将 prev 成功存入 satb queue。如果没来得及将 z 放入 satb_enqueue 去补充标记 z 时，GC 线程并发标记过程就结束了，会造成 z 对象漏标记。</p>
<ol>
<li>为什么说 concurrent marking 过程不会去参照 PrevBitmap 呢？Marking 过程中，参照前一次 Marking 的结果是不是能让本轮 Marking 执行的更快一些呢？</li>
</ol>
<p>答：这个的结论是 Marking 过程不会参考前一次留下来的 PrevBitmap。原因是，标记过程都是在标记 live object，真正关心的是 Heap 上当前有哪些 live 的 object 存在。而 PrevBitmap 上标记出来的 live object 是上一次 GC 时 live 的 object，现在这些 object 可能已经死掉了。</p>
<p>那遍历一遍 PrevBitmap 只去查看上一次 live 的 object 是否还 live 不是会更快一些吗？因为可以跳过已经确认死掉的 Object。</p>
<p>一个 object 是否 live 是看他有没有被 GC Root 指向。而 initial mark，concurrent root scanning，concurrent mark，Remark 等等都是在从 GC Root 追踪引用，追踪完了才能判断出来一个 object 是否是 live。等这些过程完成之后，PrevBitmap 实际已经没用了。在 PrevBitmap 上留下来的信息也随之可以丢弃。</p>
<h5 id="Remark"><a href="#Remark" class="headerlink" title="Remark"></a>Remark</h5><p>该过程是个并发的 STW 过程。GC 线程会将 SATB buffer 消费干净，并将 buffer 中指定 object 标记为 mark ，也将被这些 object 指向的 object 标记为 live。这也是为什么 Remark 必须是 STW 的，因为如果业务线程持续运行，GC 线程就不可能将 SATB buffer 消费干净。</p>
<h5 id="Cleanup"><a href="#Cleanup" class="headerlink" title="Cleanup"></a>Cleanup</h5><p>这个阶段之前说过，也是并发执行的，且不会 STW。在这个阶段 Next marking bitmap 会替代之前的 Previous marking bitmap，将 PrevTAMS 设置为 NextTAMS 的值。</p>
<p>这个阶段三个最耗时的操作是：</p>
<ol>
<li>根据每个 Region 的 garbage 占用情况，和 RSet popular 程度评估每个 Region 的 GC 效率，并根据 GC 效率将 Region 排序。</li>
<li>发现没有 live object 的 Region 时直接将其清理。</li>
<li>对每个 Region 的 RSet 进行清理，比如发现一个 card 中指向当前 Region 的 Object 都 dead 了，就直接清理这个 RSet 内的记录。因为之后无论是 YGC 还是 Mixed GC 都会扫描这个 RSet，将其清理一下有助于提升之后清理过程中 RSet 扫描效率。</li>
</ol>
<h3 id="Marking-日志"><a href="#Marking-日志" class="headerlink" title="Marking 日志"></a>Marking 日志</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">2016-07-12T18:04:15.484+0800: 6436.051: [GC concurrent-root-region-scan-start]</div><div class="line">2016-07-12T18:04:15.733+0800: 6436.301: [GC concurrent-root-region-scan-end, 0.2494835 secs]</div><div class="line">2016-07-12T18:04:15.733+0800: 6436.301: [GC concurrent-mark-start]</div><div class="line">2016-07-12T18:04:16.665+0800: 6437.233: [GC concurrent-mark-end, 0.9322241 secs]</div><div class="line">2016-07-12T18:04:16.671+0800: 6437.238: [GC remark 2016-07-12T18:04:16.671+0800: 6437.238: [Finalize Marking, 0.0020818 secs] 2016-07-12T18:04:16.673+0800: 6437.240: [GC ref-proc, 0.1690421 secs] 2016-07-12T18:04:16.842+0800: 6437.409: [Unloading, 0.0219765 secs], 0.2116673 secs]</div><div class="line"> [Times: user=1.55 sys=0.20, real=0.21 secs] </div><div class="line">2016-07-12T18:04:16.887+0800: 6437.455: [GC cleanup 7275M-&gt;6699M(14G), 0.0357972 secs]</div><div class="line"> [Times: user=0.38 sys=0.02, real=0.04 secs] </div><div class="line">2016-07-12T18:04:16.923+0800: 6437.491: [GC concurrent-cleanup-start]</div><div class="line">2016-07-12T18:04:16.924+0800: 6437.492: [GC concurrent-cleanup-end, 0.0007694 secs]</div></pre></td></tr></table></figure>
<p>Marking 阶段的日志相对 YGC 时候的日志来说要简单很多，没有什么特别的地方，都是 Start、End。</p>
<p>GC ref-proc 和 YGC 时候的 Ref proc 是类似的，也是在处理 soft、weak、phantom 引用。以前遇到过一个使用 Netty 的服务这个 ref-proc 时间特别长的例子，因为 Netty 内使用 phantom 引用比较多，ref-proc 又是单线程的，所以执行的时间特别长，能停滞十几秒这种。之后开启了 -XX:+ParallelRefProcEnabled，时间立即降低到 0.X 秒，非常神奇。这个参数是开启并发的处理 ref-proc。</p>
<h2 id="G1-OGC-Sweeping"><a href="#G1-OGC-Sweeping" class="headerlink" title="G1 OGC Sweeping"></a>G1 OGC Sweeping</h2><p>remark 阶段完毕后，G1 就完成了对整个 heap 的标记，能知道整个 heap 中有哪些 object 是 live 的。在接下来的几次 YGC 中，会从待收集的所有 Region 中依次选出 GC 效率最高的 Region 组成本次回收的 CSet，来执行 GC，也即 Mixed GC. “GC 效率最高” 一般是有两个指标，一个是 Region 内 live object 多少，live object 占空间最少的 Region，GC 效率越高。即 Garbage 越多的 Region，GC 效率越高。这也是 Garbage First 的由来。另一个是 Region 的 “popular” 程度，越 “popular“ 的 Region 就有越多的 Region 含有引用指向这个 Region，其 RSet 扫描和更新操作耗时也越长。</p>
<p>但也不是只要是有 dead object 就会被放入 CSet，而是有个参数去控制放入 CSet 的 Region 的选择。-XX:G1MixedGCLiveThresholdPercent，默认是 85% 即当一个 Region 内 live object 空间占比小于 85% 时，就会被放入 CSet。</p>
<p>YGC 时 CSet 内全部是 Young Generation Region。OGC 时，CSet 内有一部分 Young Generation Region 也有一部分 O 区 Region。</p>
<p>G1 的收集无论是 Old Generation 还是 Young Generation ，都是 live object 拷贝到一个 available region，拷贝过去后在这个新的 region 上每个 object 都是紧挨着排列的，所以没有 fragment.</p>
<p>需要注意的是，Mixed GC 是 G1 最主要的清理内存的阶段，但 mixed GC 要求 marking 阶段必须结束，从而能知道 heap 中有哪些 live object，才能开始清理。如果 marking 阶段没结束 heap 就满了，G1 会先尝试扩大 heap，如果无法扩大，则使用 fail-safe GC 收集内存。</p>
<p>Sweeping 阶段具体由几轮 Mixed GC 组成，每次 Mixed GC 需要收集多少个 Region，需要由两个参数决定：-XX:G1MixedGCCountTgarget 和 -XX:G1HeapWastePercent。</p>
<p>-XX:G1MixedGCCountTarget 限定 Sweeping 阶段连续的 Mixed GC 最大次数。所有 Mixed GC 阶段待收集的 Region 总数，除以 G1MixedGCCountTarget 就是每轮 Mixed GC 最少需要清理的 Region 数。</p>
<p>这里计算的是单次 Mixed GC 最少需要清理的 Region 数，还有一个 -XX:G1OldCSetRegionThresholdPercent 用于控制单次 Mixed GC 最多能收集多少 old regions。G1OldCSetRegionThresholdPercent 表示的是单次 mixed GC 能收集的 old region 占 Heap 的百分比。如果 Heap 大小不变，则这个值不会变化。</p>
<p>-XX:G1HeapWastePercent 限定 Mixed GC 在 Garbage 占 Heap 空间的百分之多少的时候停止 Mixed GC。也就是说每次 Mixed GC 结束都会计算当前 dead object 占总 Heap 空间比例，当这个比例小于 G1HeapWastePercent 后，就停止 Mixed GC 即使现在还有 dead object 没有收集完也停止收集了。因为剩下的 Region 内可能剩余的 dead object 比例不是很多，收集起来效率很低。</p>
<p>G1MixedGCCountTarget 决定 Mixed GC 最大次数，G1HeapWastePercent 决定 Mixed GC 实际次数。</p>
<h3 id="Evacuation-Failures-And-FGC"><a href="#Evacuation-Failures-And-FGC" class="headerlink" title="Evacuation Failures And FGC"></a>Evacuation Failures And FGC</h3><p>Evacuation 过程中有三种情况会导致 G1 降级为 FGC 去收集内存：</p>
<ol>
<li>YGC 时，无法找到 available region 去存放 survive 的 object；</li>
<li>Mixed GC 时，无法找到 available region 去存放 live object；</li>
<li>无法找到足够大的连续的 Region 去存放 Humongous Object</li>
</ol>
<p>Humongous Object 后续会说。</p>
<p>FGC 是单线程的，完成 mark 、 sweep、compation 工作。单线程的 FGC 效率最低，但也是最安全的。 </p>
<p>如果真出现 FGC 了，日志会非常简单：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">2016-07-20T15:56:37.515+0800: 96764.890: [Full GC (Heap Dump Initiated GC)  8442M-&gt;2040M(14G), 7.0207811 secs]</div><div class="line">   [Eden: 2824.0M(6552.0M)-&gt;0.0B(8600.0M) Survivors: 532.0M-&gt;0.0B Heap: 8442.5M(14.0G)-&gt;2040.7M(14.0G)], [Metaspace: 85076K-&gt;85076K(1120256K)]</div><div class="line"> [Times: user=10.41 sys=0.54, real=7.02 secs]</div></pre></td></tr></table></figure>
<h1 id="Humongous-Object"><a href="#Humongous-Object" class="headerlink" title="Humongous Object"></a>Humongous Object</h1><p>正因为无论是 Young Generation 还是 Old Generation，在 GC 的时候都会有 object 拷贝。Young Generation 一方面是将 object 从 Eden 拷贝到 Survivor ，另一方面是拷贝晋升的 object 到 Old 区。这种拷贝过程对特别大的 object 来说就很不经济。</p>
<p>G1 中 Region 大小最小是 1MB，最大是 32MB。具体多大会根据 Heap 大小做设置，它是尽力去保证整个 Heap 被划分为大约 2048 个 Region。比如如果 Heap 有 16G，算下来 16G / 2048 = 8MB 即一个 Region 大概是 8MB。当然 2048 个 Region 也不是绝对的，如果 Heap 特别大或者特别小，Region 总数是可以超过或小于 2048。Region 总数也能通过参数精确设置 -XX:G1HeapRegionSize=n。</p>
<p>回到 Humongous Object，G1 中内存占用超过当前单个 Region 50% 的 Object 就叫 Humongous Object，G1 对他们有单独的处理。</p>
<p>Humongous Object 分配时会根据这个 object 大小，在 available regions 中找足够放下这个 object 的连续的数个 region，专门分配给这个 Humongous object 使用。如果找不到这么个连续的 region，G1 会直接使用 fail-safe 的 FGC 来清理并 compact heap。</p>
<p>理解这里不先进行 YGC 或 OGC 的原因是 YGC 和 OGC 很多过程都是 concurrent 的，这个时候 Humongous Object 无法分配内存，无法让应用线程继续运行，必须执行完全的 STW 收集一次内存才行。<br><img src="https://cloud.githubusercontent.com/assets/1115061/16544443/d4082fe2-4138-11e6-99e0-ea98ae50d94c.png" alt="2016-07-03 12 16 36"></p>
<p>更细的看上面存放 Humongous Object 的连续的 Region：<br><img src="https://cloud.githubusercontent.com/assets/1115061/16544448/f72dab3c-4138-11e6-9e6d-40e212ccc196.png" alt="2016-07-03 10 21 08"><br>看到连续的 Region 是由 StartsHumongous 和 ContinuesHumongous Region 组成的。</p>
<p>开辟单独的区域存放 Humongous Object 是为了避免 long-live 的大对象在 GC 过程中的拷贝。开辟连续的 Region 只存放一个 Humongous Object 是为了让 G1 对 Humongous Object 更激进的进行收集，只要发现这个 object dead，就能将其所占用的 Regions 全部收集，不用去判断 Region 还有没有别的 object 使用，别的 object 是否还 live。比如在 marking 的 clean up 阶段、 YGC 和 FGC 时，发现 humongous object 没有任何引用，就会立即被收集。</p>
<p>但正因为 Humongous Object 这种独特的分配机制，使得其无法享受到 TLAB 和 PLAB 带来的便利。</p>
<h1 id="Heap-Size-调节"><a href="#Heap-Size-调节" class="headerlink" title="Heap Size 调节"></a>Heap Size 调节</h1><p>G1 heap 大小可以在 -Xms -Xmx 之间变化。</p>
<p>G1 增大 heap 的时机有：</p>
<ol>
<li>FGC 时会根据应用行为计算预期 heap size，增大 heap；</li>
<li>YGC 或 OGC 触发时，G1 计算 GC 花费的时间相对应用运行时间的比例，如果 GC 耗费时间比例过大(可通过 -XX:GCTimeRatio 调节，G1 默认是 9，其它 GC 是 99 也就是说其它 GC会更激进的扩展 heap size)，Heap size 会增加从而减少 GC 发生次数，增大单次 GC 所能收集的内存比例. </li>
<li>分配内存失败时，在进行 fail-safe GC 之前，会先尝试扩展一下内存;</li>
<li>无法为 Humongous object 找到足够大的连续 region 时，先尝试扩展内存，再 fail-safe;</li>
<li>G1 GC 时将 live object 拷贝到一个 available region，如果找不到这么个 available region，会先尝试扩展内存，无法扩展则执行 fail-safe</li>
</ol>
<p>参考文献：</p>
<p>[1] Charlie Hunt,Monica Beckwith,Poonam Parhar,Bengt Rutisson. Java Performance Companion. Addison-Wesley. ISBN-13: 978-0-13-379682-7</p>
<p>[2] David Detlefs, Christine Flood, Steve Heller, Tony Printezis. Garbage-First Garbage Collection.ISMM’04, October 24–25, 2004, Vancouver, British Columbia, Canada. ACM 1-58113-945-4/04/0010.</p>
<p>[3] Darko Stefanovic,Matthew Hertz, Stephen M. Blackburn,Kathryn S. McKinley,J. Eliot B. Moss†. Older-first Garbage Collection in Practice: Evaluation in a Java Virtual Machine.</p>
<p>[4] Taiichi Yuasa. Real-Time Garbage Collection on General Purpose Machines. Journal of Systems and Software, Volume 11, Issue 3, March 1990, pp. 181-98. Elsevier Science, Inc., New York.</p>
<p>[5] Tony Printezis and David Detlefs. A Generational Mostyly-Concurrent Garbage Collector. Proceedings of the 2nd Internaltional Symposium on Memory Management. ACM, New York, 2000, pp. 143-54. ISBN 1-58113-263-8</p>
]]></content>
    
    <summary type="html">
    
      研究一下 G1
    
    </summary>
    
    
      <category term="Java" scheme="http://ylgrgyq.github.io/tags/Java/"/>
    
      <category term="JVM" scheme="http://ylgrgyq.github.io/tags/JVM/"/>
    
  </entry>
  
</feed>
